{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gan-Bert-3-level-Sentiment-DataFrame-Colab",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riyadhctg/ganbert_notebook/blob/main/Gan_Bert_3_level_Sentiment_DataFrame_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEJpUwL1CsGp"
      },
      "source": [
        "#NOTICE\n",
        "This is an attempt to run Gan-BERT as a Colab Notebook with three level sentiment with DataFrame data from CSV loaded from Google Drive\n",
        "\n",
        "**Original Gan-Bert Code** - https://github.com/crux82/ganbert\n",
        "\n",
        "**Gan-Bert Paper:**\n",
        "Croce, D., Castellucci, G., & Basili, R. (2020, July). Gan-bert: Generative adversarial learning for robust text classification with a bunch of labeled examples. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (pp. 2114-2119).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbHfo5PnC0Qy"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "\n",
        "*   Store your data in Google drive in csv format. For this example, \"text\" column needs to have a heading named \"text\", and \"sentiment\" column needs to have a heading named \"sentiment\"\n",
        "*   Define Google drive file path and modify the data distribution across labeled, test, and unlabeled data [here](https://colab.research.google.com/drive/1M4IZ5aAFB6_n5YDucAzvGPrmrGL1QUkm?authuser=2#scrollTo=iI6lUxt8gHgL&line=1&uniqifier=1)\n",
        "*   Define your labels [here](https://colab.research.google.com/drive/1M4IZ5aAFB6_n5YDucAzvGPrmrGL1QUkm?authuser=2#scrollTo=eRi84S7BgNKN&line=6&uniqifier=1) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx8VG9kA_WpG"
      },
      "source": [
        "# Download BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H2JGbqE8IWA",
        "outputId": "f383b450-1e39-4e6a-f1a1-6a382d9eefe5"
      },
      "source": [
        "!if [ -d \"cased_L-12_H-768_A-12\" ]; then echo \"BERT folder already exists\"; else wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip && unzip cased_L-12_H-768_A-12.zip; fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-04 05:30:08--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.112.128, 172.217.214.128, 108.177.121.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.112.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 404261442 (386M) [application/zip]\n",
            "Saving to: ‘cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "cased_L-12_H-768_A- 100%[===================>] 385.53M   148MB/s    in 2.6s    \n",
            "\n",
            "2021-04-04 05:30:10 (148 MB/s) - ‘cased_L-12_H-768_A-12.zip’ saved [404261442/404261442]\n",
            "\n",
            "Archive:  cased_L-12_H-768_A-12.zip\n",
            "   creating: cased_L-12_H-768_A-12/\n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCgLMDDTrVxA"
      },
      "source": [
        "#Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkNUyDSwuplS"
      },
      "source": [
        "!pip install git+https://github.com/guillaumegenthial/tf_metrics.git \n",
        "!pip install gast==0.2.2 \n",
        "!pip install tensorflow==1.15\n",
        "!pip install 'tensorflow-estimator<1.15.0rc0,>=1.14.0rc0' --force-reinstall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-snHc60eq-8h"
      },
      "source": [
        "#tokenization.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqj9XC0Lq94y"
      },
      "source": [
        "# This file comes originally from https://github.com/google-research/bert/blob/master/tokenization.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import unicodedata\n",
        "import six\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n",
        "  \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n",
        "\n",
        "  # The casing has to be passed in by the user and there is no explicit check\n",
        "  # as to whether it matches the checkpoint. The casing information probably\n",
        "  # should have been stored in the bert_config.json file, but it's not, so\n",
        "  # we have to heuristically detect it to validate.\n",
        "\n",
        "  if not init_checkpoint:\n",
        "    return\n",
        "\n",
        "  m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n",
        "  if m is None:\n",
        "    return\n",
        "\n",
        "  model_name = m.group(1)\n",
        "\n",
        "  lower_models = [\n",
        "      \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n",
        "      \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  cased_models = [\n",
        "      \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n",
        "      \"multi_cased_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  is_bad_config = False\n",
        "  if model_name in lower_models and not do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"False\"\n",
        "    case_name = \"lowercased\"\n",
        "    opposite_flag = \"True\"\n",
        "\n",
        "  if model_name in cased_models and do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"True\"\n",
        "    case_name = \"cased\"\n",
        "    opposite_flag = \"False\"\n",
        "\n",
        "  if is_bad_config:\n",
        "    raise ValueError(\n",
        "        \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n",
        "        \"However, `%s` seems to be a %s model, so you \"\n",
        "        \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n",
        "        \"how the model was pre-training. If this error is wrong, please \"\n",
        "        \"just comment out this check.\" % (actual_flag, init_checkpoint,\n",
        "                                          model_name, case_name, opposite_flag))\n",
        "\n",
        "\n",
        "def convert_to_unicode(text):\n",
        "  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    elif isinstance(text, unicode):\n",
        "      return text\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def printable_text(text):\n",
        "  \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
        "\n",
        "  # These functions want `str` for both Python2 and Python3, but in one case\n",
        "  # it's a Unicode string and in the other it's a byte string.\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, unicode):\n",
        "      return text.encode(\"utf-8\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "  vocab = collections.OrderedDict()\n",
        "  index = 0\n",
        "  with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
        "    while True:\n",
        "      token = convert_to_unicode(reader.readline())\n",
        "      if not token:\n",
        "        break\n",
        "      token = token.strip()\n",
        "      vocab[token] = index\n",
        "      index += 1\n",
        "  return vocab\n",
        "\n",
        "\n",
        "def convert_by_vocab(vocab, items):\n",
        "  \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
        "  output = []\n",
        "  for item in items:\n",
        "    output.append(vocab[item])\n",
        "  return output\n",
        "\n",
        "\n",
        "def convert_tokens_to_ids(vocab, tokens):\n",
        "  return convert_by_vocab(vocab, tokens)\n",
        "\n",
        "\n",
        "def convert_ids_to_tokens(inv_vocab, ids):\n",
        "  return convert_by_vocab(inv_vocab, ids)\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "  text = text.strip()\n",
        "  if not text:\n",
        "    return []\n",
        "  tokens = text.split()\n",
        "  return tokens\n",
        "\n",
        "\n",
        "class FullTokenizer(object):\n",
        "  \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_file, do_lower_case=True):\n",
        "    self.vocab = load_vocab(vocab_file)\n",
        "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    split_tokens = []\n",
        "    for token in self.basic_tokenizer.tokenize(text):\n",
        "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "        split_tokens.append(sub_token)\n",
        "\n",
        "    return split_tokens\n",
        "\n",
        "  def convert_tokens_to_ids(self, tokens):\n",
        "    return convert_by_vocab(self.vocab, tokens)\n",
        "\n",
        "  def convert_ids_to_tokens(self, ids):\n",
        "    return convert_by_vocab(self.inv_vocab, ids)\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "  \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "  def __init__(self, do_lower_case=True):\n",
        "    \"\"\"Constructs a BasicTokenizer.\n",
        "\n",
        "    Args:\n",
        "      do_lower_case: Whether to lower case the input.\n",
        "    \"\"\"\n",
        "    self.do_lower_case = do_lower_case\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "    text = convert_to_unicode(text)\n",
        "    text = self._clean_text(text)\n",
        "\n",
        "    # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "    # models. This is also applied to the English models now, but it doesn't\n",
        "    # matter since the English models were not trained on any Chinese data\n",
        "    # and generally don't have any Chinese data in them (there are Chinese\n",
        "    # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "    # words in the English Wikipedia.).\n",
        "    text = self._tokenize_chinese_chars(text)\n",
        "\n",
        "    orig_tokens = whitespace_tokenize(text)\n",
        "    split_tokens = []\n",
        "    for token in orig_tokens:\n",
        "      if self.do_lower_case:\n",
        "        token = token.lower()\n",
        "        token = self._run_strip_accents(token)\n",
        "      split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "    return output_tokens\n",
        "\n",
        "  def _run_strip_accents(self, text):\n",
        "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "    text = unicodedata.normalize(\"NFD\", text)\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cat = unicodedata.category(char)\n",
        "      if cat == \"Mn\":\n",
        "        continue\n",
        "      output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _run_split_on_punc(self, text):\n",
        "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "    chars = list(text)\n",
        "    i = 0\n",
        "    start_new_word = True\n",
        "    output = []\n",
        "    while i < len(chars):\n",
        "      char = chars[i]\n",
        "      if _is_punctuation(char):\n",
        "        output.append([char])\n",
        "        start_new_word = True\n",
        "      else:\n",
        "        if start_new_word:\n",
        "          output.append([])\n",
        "        start_new_word = False\n",
        "        output[-1].append(char)\n",
        "      i += 1\n",
        "\n",
        "    return [\"\".join(x) for x in output]\n",
        "\n",
        "  def _tokenize_chinese_chars(self, text):\n",
        "    \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if self._is_chinese_char(cp):\n",
        "        output.append(\" \")\n",
        "        output.append(char)\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _is_chinese_char(self, cp):\n",
        "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "    #\n",
        "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "    # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "    # space-separated words, so they are not treated specially and handled\n",
        "    # like the all of the other languages.\n",
        "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "  def _clean_text(self, text):\n",
        "    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "        continue\n",
        "      if _is_whitespace(char):\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "  \"\"\"Runs WordPiece tokenziation.\"\"\"\n",
        "  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
        "    self.vocab = vocab\n",
        "    self.unk_token = unk_token\n",
        "    self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "\n",
        "    This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "    using the given vocabulary.\n",
        "\n",
        "    For example:\n",
        "      input = \"unaffable\"\n",
        "      output = [\"un\", \"##aff\", \"##able\"]\n",
        "\n",
        "    Args:\n",
        "      text: A single token or whitespace separated tokens. This should have\n",
        "        already been passed through `BasicTokenizer.\n",
        "\n",
        "    Returns:\n",
        "      A list of wordpiece tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    text = convert_to_unicode(text)\n",
        "\n",
        "    output_tokens = []\n",
        "    for token in whitespace_tokenize(text):\n",
        "      chars = list(token)\n",
        "      if len(chars) > self.max_input_chars_per_word:\n",
        "        output_tokens.append(self.unk_token)\n",
        "        continue\n",
        "\n",
        "      is_bad = False\n",
        "      start = 0\n",
        "      sub_tokens = []\n",
        "      while start < len(chars):\n",
        "        end = len(chars)\n",
        "        cur_substr = None\n",
        "        while start < end:\n",
        "          substr = \"\".join(chars[start:end])\n",
        "          if start > 0:\n",
        "            substr = \"##\" + substr\n",
        "          if substr in self.vocab:\n",
        "            cur_substr = substr\n",
        "            break\n",
        "          end -= 1\n",
        "        if cur_substr is None:\n",
        "          is_bad = True\n",
        "          break\n",
        "        sub_tokens.append(cur_substr)\n",
        "        start = end\n",
        "\n",
        "      if is_bad:\n",
        "        output_tokens.append(self.unk_token)\n",
        "      else:\n",
        "        output_tokens.extend(sub_tokens)\n",
        "    return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "  \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "  # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "  # as whitespace since they are generally considered as such.\n",
        "  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat == \"Zs\":\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "  \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "  # These are technically control characters but we count them as whitespace\n",
        "  # characters.\n",
        "  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return False\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat in (\"Cc\", \"Cf\"):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "  \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "  cp = ord(char)\n",
        "  # We treat all non-letter/number ASCII as punctuation.\n",
        "  # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "  # Punctuation class but we treat them as punctuation anyways, for\n",
        "  # consistency.\n",
        "  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat.startswith(\"P\"):\n",
        "    return True\n",
        "  return False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7848BuX_q3u5"
      },
      "source": [
        "#optimization.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-SVl_Smq23x"
      },
      "source": [
        "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
        "# Copyright Tor Vergata, University of Rome. All Rights Reserved.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "#\n",
        "# Modification of the https://github.com/google-research/bert/blob/master/optimization.py\n",
        "# script for GAN-BERT.\n",
        "\n",
        "\"\"\"Functions and classes related to optimization (weight updates) for GAN-BERT.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def create_optimizer(prefix_name, tvars, loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
        "  \"\"\"Creates an optimizer training op.\"\"\"\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "  #global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
        "\n",
        "  # Implements linear decay of the learning rate.\n",
        "  learning_rate = tf.train.polynomial_decay(\n",
        "      learning_rate,\n",
        "      global_step,\n",
        "      num_train_steps,\n",
        "      end_learning_rate=0.0,\n",
        "      power=1.0,\n",
        "      cycle=False)\n",
        "\n",
        "  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
        "  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
        "  if num_warmup_steps:\n",
        "    global_steps_int = tf.cast(global_step, tf.int32)\n",
        "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
        "\n",
        "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
        "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
        "\n",
        "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
        "    warmup_learning_rate = init_lr * warmup_percent_done\n",
        "\n",
        "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
        "    learning_rate = (\n",
        "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
        "\n",
        "  # It is recommended that you use this optimizer for fine tuning, since this\n",
        "  # is how the model was trained (note that the Adam m/v variables are NOT\n",
        "  # loaded from init_checkpoint.)\n",
        "  optimizer = AdamWeightDecayOptimizer(\n",
        "      learning_rate=learning_rate,\n",
        "      weight_decay_rate=0.01,\n",
        "      beta_1=0.9,\n",
        "      beta_2=0.999,\n",
        "      epsilon=1e-6,\n",
        "      prefix_name = prefix_name,\n",
        "      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
        "\n",
        "  if use_tpu:\n",
        "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "  #tvars = tf.trainable_variables()\n",
        "  grads = tf.gradients(loss, tvars)\n",
        "\n",
        "  # This is how the model was pre-trained.\n",
        "  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
        "\n",
        "  train_op = optimizer.apply_gradients(\n",
        "      zip(grads, tvars), global_step=global_step)\n",
        "\n",
        "  # Normally the global step update is done inside of `apply_gradients`.\n",
        "  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
        "  # a different optimizer, you should probably take this line out.\n",
        "  new_global_step = global_step + 1\n",
        "  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
        "  return train_op\n",
        "\n",
        "\n",
        "class AdamWeightDecayOptimizer(tf.compat.v1.train.AdamOptimizer):\n",
        "  \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate,\n",
        "               weight_decay_rate=0.0,\n",
        "               beta_1=0.9,\n",
        "               beta_2=0.999,\n",
        "               epsilon=1e-6,\n",
        "               prefix_name=\"3\",\n",
        "               exclude_from_weight_decay=None,\n",
        "               name=\"AdamWeightDecayOptimizer\"):\n",
        "    \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n",
        "    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weight_decay_rate = weight_decay_rate\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.epsilon = epsilon\n",
        "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "    self.prefix_name=prefix_name\n",
        "\n",
        "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    assignments = []\n",
        "    for (grad, param) in grads_and_vars:\n",
        "      if grad is None or param is None:\n",
        "        continue\n",
        "\n",
        "      param_name = self._get_variable_name(param.name)\n",
        "\n",
        "      m = tf.get_variable(\n",
        "          name=param_name + \"/adam_m\" + self.prefix_name,\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "      v = tf.get_variable(\n",
        "          name=param_name + \"/adam_v\" + self.prefix_name,\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "\n",
        "      # Standard Adam update.\n",
        "      next_m = (\n",
        "          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
        "      next_v = (\n",
        "          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
        "                                                    tf.square(grad)))\n",
        "\n",
        "      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
        "\n",
        "      # Just adding the square of the weights to the loss function is *not*\n",
        "      # the correct way of using L2 regularization/weight decay with Adam,\n",
        "      # since that will interact with the m and v parameters in strange ways.\n",
        "      #\n",
        "      # Instead we want ot decay the weights in a manner that doesn't interact\n",
        "      # with the m/v parameters. This is equivalent to adding the square\n",
        "      # of the weights to the loss with plain (non-momentum) SGD.\n",
        "      if self._do_use_weight_decay(param_name):\n",
        "        update += self.weight_decay_rate * param\n",
        "\n",
        "      update_with_lr = self.learning_rate * update\n",
        "\n",
        "      next_param = param - update_with_lr\n",
        "\n",
        "      assignments.extend(\n",
        "          [param.assign(next_param),\n",
        "           m.assign(next_m),\n",
        "           v.assign(next_v)])\n",
        "    return tf.group(*assignments, name=name)\n",
        "\n",
        "  def _do_use_weight_decay(self, param_name):\n",
        "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "    if not self.weight_decay_rate:\n",
        "      return False\n",
        "    if self.exclude_from_weight_decay:\n",
        "      for r in self.exclude_from_weight_decay:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def _get_variable_name(self, param_name):\n",
        "    \"\"\"Get the variable name from the tensor name.\"\"\"\n",
        "    m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
        "    if m is not None:\n",
        "      param_name = m.group(1)\n",
        "    return param_name\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQnuNLt-qyU1"
      },
      "source": [
        "#modelling.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnuxaOPWqw-C"
      },
      "source": [
        "# Original BERT model from https://github.com/google-research/bert/blob/master/modeling.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import numpy as np\n",
        "import six\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class BertConfig(object):\n",
        "  \"\"\"Configuration for `BertModel`.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               hidden_size=768,\n",
        "               num_hidden_layers=12,\n",
        "               num_attention_heads=12,\n",
        "               intermediate_size=3072,\n",
        "               hidden_act=\"gelu\",\n",
        "               hidden_dropout_prob=0.1,\n",
        "               attention_probs_dropout_prob=0.1,\n",
        "               max_position_embeddings=512,\n",
        "               type_vocab_size=16,\n",
        "               initializer_range=0.02):\n",
        "    \"\"\"Constructs BertConfig.\n",
        "\n",
        "    Args:\n",
        "      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n",
        "      hidden_size: Size of the encoder layers and the pooler layer.\n",
        "      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
        "      num_attention_heads: Number of attention heads for each attention layer in\n",
        "        the Transformer encoder.\n",
        "      intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
        "        layer in the Transformer encoder.\n",
        "      hidden_act: The non-linear activation function (function or string) in the\n",
        "        encoder and pooler.\n",
        "      hidden_dropout_prob: The dropout probability for all fully connected\n",
        "        layers in the embeddings, encoder, and pooler.\n",
        "      attention_probs_dropout_prob: The dropout ratio for the attention\n",
        "        probabilities.\n",
        "      max_position_embeddings: The maximum sequence length that this model might\n",
        "        ever be used with. Typically set this to something large just in case\n",
        "        (e.g., 512 or 1024 or 2048).\n",
        "      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
        "        `BertModel`.\n",
        "      initializer_range: The stdev of the truncated_normal_initializer for\n",
        "        initializing all weight matrices.\n",
        "    \"\"\"\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.hidden_act = hidden_act\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "    self.max_position_embeddings = max_position_embeddings\n",
        "    self.type_vocab_size = type_vocab_size\n",
        "    self.initializer_range = initializer_range\n",
        "\n",
        "  @classmethod\n",
        "  def from_dict(cls, json_object):\n",
        "    \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
        "    config = BertConfig(vocab_size=None)\n",
        "    for (key, value) in six.iteritems(json_object):\n",
        "      config.__dict__[key] = value\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_json_file(cls, json_file):\n",
        "    \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "    with tf.gfile.GFile(json_file, \"r\") as reader:\n",
        "      text = reader.read()\n",
        "    return cls.from_dict(json.loads(text))\n",
        "\n",
        "  def to_dict(self):\n",
        "    \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "    output = copy.deepcopy(self.__dict__)\n",
        "    return output\n",
        "\n",
        "  def to_json_string(self):\n",
        "    \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class BertModel(object):\n",
        "  \"\"\"BERT model (\"Bidirectional Encoder Representations from Transformers\").\n",
        "\n",
        "  Example usage:\n",
        "\n",
        "  ```python\n",
        "  # Already been converted into WordPiece token ids\n",
        "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
        "  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
        "  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
        "    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "  model = modeling.BertModel(config=config, is_training=True,\n",
        "    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "  label_embeddings = tf.get_variable(...)\n",
        "  pooled_output = model.get_pooled_output()\n",
        "  logits = tf.matmul(pooled_output, label_embeddings)\n",
        "  ...\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               config,\n",
        "               is_training,\n",
        "               input_ids,\n",
        "               input_mask=None,\n",
        "               token_type_ids=None,\n",
        "               use_one_hot_embeddings=False,\n",
        "               scope=None):\n",
        "    \"\"\"Constructor for BertModel.\n",
        "\n",
        "    Args:\n",
        "      config: `BertConfig` instance.\n",
        "      is_training: bool. true for training model, false for eval model. Controls\n",
        "        whether dropout will be applied.\n",
        "      input_ids: int32 Tensor of shape [batch_size, seq_length].\n",
        "      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n",
        "        embeddings or tf.embedding_lookup() for the word embeddings.\n",
        "      scope: (optional) variable scope. Defaults to \"bert\".\n",
        "\n",
        "    Raises:\n",
        "      ValueError: The config is invalid or one of the input tensor shapes\n",
        "        is invalid.\n",
        "    \"\"\"\n",
        "    config = copy.deepcopy(config)\n",
        "    if not is_training:\n",
        "      config.hidden_dropout_prob = 0.0\n",
        "      config.attention_probs_dropout_prob = 0.0\n",
        "\n",
        "    input_shape = get_shape_list(input_ids, expected_rank=2)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_length = input_shape[1]\n",
        "\n",
        "    if input_mask is None:\n",
        "      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n",
        "\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n",
        "\n",
        "    with tf.variable_scope(scope, default_name=\"bert\"):\n",
        "      with tf.variable_scope(\"embeddings\"):\n",
        "        # Perform embedding lookup on the word ids.\n",
        "        (self.embedding_output, self.embedding_table) = embedding_lookup(\n",
        "            input_ids=input_ids,\n",
        "            vocab_size=config.vocab_size,\n",
        "            embedding_size=config.hidden_size,\n",
        "            initializer_range=config.initializer_range,\n",
        "            word_embedding_name=\"word_embeddings\",\n",
        "            use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "        # Add positional embeddings and token type embeddings, then layer\n",
        "        # normalize and perform dropout.\n",
        "        self.embedding_output = embedding_postprocessor(\n",
        "            input_tensor=self.embedding_output,\n",
        "            use_token_type=True,\n",
        "            token_type_ids=token_type_ids,\n",
        "            token_type_vocab_size=config.type_vocab_size,\n",
        "            token_type_embedding_name=\"token_type_embeddings\",\n",
        "            use_position_embeddings=True,\n",
        "            position_embedding_name=\"position_embeddings\",\n",
        "            initializer_range=config.initializer_range,\n",
        "            max_position_embeddings=config.max_position_embeddings,\n",
        "            dropout_prob=config.hidden_dropout_prob)\n",
        "\n",
        "      with tf.variable_scope(\"encoder\"):\n",
        "        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n",
        "        # mask of shape [batch_size, seq_length, seq_length] which is used\n",
        "        # for the attention scores.\n",
        "        attention_mask = create_attention_mask_from_input_mask(\n",
        "            input_ids, input_mask)\n",
        "\n",
        "        # Run the stacked transformer.\n",
        "        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n",
        "        self.all_encoder_layers = transformer_model(\n",
        "            input_tensor=self.embedding_output,\n",
        "            attention_mask=attention_mask,\n",
        "            hidden_size=config.hidden_size,\n",
        "            num_hidden_layers=config.num_hidden_layers,\n",
        "            num_attention_heads=config.num_attention_heads,\n",
        "            intermediate_size=config.intermediate_size,\n",
        "            intermediate_act_fn=get_activation(config.hidden_act),\n",
        "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
        "            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
        "            initializer_range=config.initializer_range,\n",
        "            do_return_all_layers=True)\n",
        "\n",
        "      self.sequence_output = self.all_encoder_layers[-1]\n",
        "      # The \"pooler\" converts the encoded sequence tensor of shape\n",
        "      # [batch_size, seq_length, hidden_size] to a tensor of shape\n",
        "      # [batch_size, hidden_size]. This is necessary for segment-level\n",
        "      # (or segment-pair-level) classification tasks where we need a fixed\n",
        "      # dimensional representation of the segment.\n",
        "      with tf.variable_scope(\"pooler\"):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token. We assume that this has been pre-trained\n",
        "        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
        "        self.pooled_output = tf.layers.dense(\n",
        "            first_token_tensor,\n",
        "            config.hidden_size,\n",
        "            activation=tf.tanh,\n",
        "            kernel_initializer=create_initializer(config.initializer_range))\n",
        "\n",
        "  def get_pooled_output(self):\n",
        "    return self.pooled_output\n",
        "\n",
        "  def get_sequence_output(self):\n",
        "    \"\"\"Gets final hidden layer of encoder.\n",
        "\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
        "      to the final hidden of the transformer encoder.\n",
        "    \"\"\"\n",
        "    return self.sequence_output\n",
        "\n",
        "  def get_all_encoder_layers(self):\n",
        "    return self.all_encoder_layers\n",
        "\n",
        "  def get_embedding_output(self):\n",
        "    \"\"\"Gets output of the embedding lookup (i.e., input to the transformer).\n",
        "\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
        "      to the output of the embedding layer, after summing the word\n",
        "      embeddings with the positional embeddings and the token type embeddings,\n",
        "      then performing layer normalization. This is the input to the transformer.\n",
        "    \"\"\"\n",
        "    return self.embedding_output\n",
        "\n",
        "  def get_embedding_table(self):\n",
        "    return self.embedding_table\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "  \"\"\"Gaussian Error Linear Unit.\n",
        "\n",
        "  This is a smoother version of the RELU.\n",
        "  Original paper: https://arxiv.org/abs/1606.08415\n",
        "  Args:\n",
        "    x: float Tensor to perform activation.\n",
        "\n",
        "  Returns:\n",
        "    `x` with the GELU activation applied.\n",
        "  \"\"\"\n",
        "  cdf = 0.5 * (1.0 + tf.tanh(\n",
        "      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "  return x * cdf\n",
        "\n",
        "\n",
        "def get_activation(activation_string):\n",
        "  \"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n",
        "\n",
        "  Args:\n",
        "    activation_string: String name of the activation function.\n",
        "\n",
        "  Returns:\n",
        "    A Python function corresponding to the activation function. If\n",
        "    `activation_string` is None, empty, or \"linear\", this will return None.\n",
        "    If `activation_string` is not a string, it will return `activation_string`.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: The `activation_string` does not correspond to a known\n",
        "      activation.\n",
        "  \"\"\"\n",
        "\n",
        "  # We assume that anything that\"s not a string is already an activation\n",
        "  # function, so we just return it.\n",
        "  if not isinstance(activation_string, six.string_types):\n",
        "    return activation_string\n",
        "\n",
        "  if not activation_string:\n",
        "    return None\n",
        "\n",
        "  act = activation_string.lower()\n",
        "  if act == \"linear\":\n",
        "    return None\n",
        "  elif act == \"relu\":\n",
        "    return tf.nn.relu\n",
        "  elif act == \"gelu\":\n",
        "    return gelu\n",
        "  elif act == \"tanh\":\n",
        "    return tf.tanh\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported activation: %s\" % act)\n",
        "\n",
        "\n",
        "def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
        "  \"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"\n",
        "  assignment_map = {}\n",
        "  initialized_variable_names = {}\n",
        "\n",
        "  name_to_variable = collections.OrderedDict()\n",
        "  for var in tvars:\n",
        "    name = var.name\n",
        "    m = re.match(\"^(.*):\\\\d+$\", name)\n",
        "    if m is not None:\n",
        "      name = m.group(1)\n",
        "    name_to_variable[name] = var\n",
        "\n",
        "  init_vars = tf.train.list_variables(init_checkpoint)\n",
        "\n",
        "  assignment_map = collections.OrderedDict()\n",
        "  for x in init_vars:\n",
        "    (name, var) = (x[0], x[1])\n",
        "    if name not in name_to_variable:\n",
        "      continue\n",
        "    assignment_map[name] = name\n",
        "    initialized_variable_names[name] = 1\n",
        "    initialized_variable_names[name + \":0\"] = 1\n",
        "\n",
        "  return (assignment_map, initialized_variable_names)\n",
        "\n",
        "\n",
        "def dropout(input_tensor, dropout_prob):\n",
        "  \"\"\"Perform dropout.\n",
        "\n",
        "  Args:\n",
        "    input_tensor: float Tensor.\n",
        "    dropout_prob: Python float. The probability of dropping out a value (NOT of\n",
        "      *keeping* a dimension as in `tf.nn.dropout`).\n",
        "\n",
        "  Returns:\n",
        "    A version of `input_tensor` with dropout applied.\n",
        "  \"\"\"\n",
        "  if dropout_prob is None or dropout_prob == 0.0:\n",
        "    return input_tensor\n",
        "\n",
        "  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
        "  return output\n",
        "\n",
        "\n",
        "def layer_norm(input_tensor, name=None):\n",
        "  \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n",
        "  return tf.contrib.layers.layer_norm(\n",
        "      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n",
        "\n",
        "\n",
        "def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n",
        "  \"\"\"Runs layer normalization followed by dropout.\"\"\"\n",
        "  output_tensor = layer_norm(input_tensor, name)\n",
        "  output_tensor = dropout(output_tensor, dropout_prob)\n",
        "  return output_tensor\n",
        "\n",
        "\n",
        "def create_initializer(initializer_range=0.02):\n",
        "  \"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"\n",
        "  return tf.truncated_normal_initializer(stddev=initializer_range)\n",
        "\n",
        "\n",
        "def embedding_lookup(input_ids,\n",
        "                     vocab_size,\n",
        "                     embedding_size=128,\n",
        "                     initializer_range=0.02,\n",
        "                     word_embedding_name=\"word_embeddings\",\n",
        "                     use_one_hot_embeddings=False):\n",
        "  \"\"\"Looks up words embeddings for id tensor.\n",
        "\n",
        "  Args:\n",
        "    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n",
        "      ids.\n",
        "    vocab_size: int. Size of the embedding vocabulary.\n",
        "    embedding_size: int. Width of the word embeddings.\n",
        "    initializer_range: float. Embedding initialization range.\n",
        "    word_embedding_name: string. Name of the embedding table.\n",
        "    use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
        "      embeddings. If False, use `tf.gather()`.\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, seq_length, embedding_size].\n",
        "  \"\"\"\n",
        "  # This function assumes that the input is of shape [batch_size, seq_length,\n",
        "  # num_inputs].\n",
        "  #\n",
        "  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
        "  # reshape to [batch_size, seq_length, 1].\n",
        "  if input_ids.shape.ndims == 2:\n",
        "    input_ids = tf.expand_dims(input_ids, axis=[-1])\n",
        "\n",
        "  embedding_table = tf.get_variable(\n",
        "      name=word_embedding_name,\n",
        "      shape=[vocab_size, embedding_size],\n",
        "      initializer=create_initializer(initializer_range))\n",
        "\n",
        "  flat_input_ids = tf.reshape(input_ids, [-1])\n",
        "  if use_one_hot_embeddings:\n",
        "    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
        "    output = tf.matmul(one_hot_input_ids, embedding_table)\n",
        "  else:\n",
        "    output = tf.gather(embedding_table, flat_input_ids)\n",
        "\n",
        "  input_shape = get_shape_list(input_ids)\n",
        "\n",
        "  output = tf.reshape(output,\n",
        "                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
        "  return (output, embedding_table)\n",
        "\n",
        "\n",
        "def embedding_postprocessor(input_tensor,\n",
        "                            use_token_type=False,\n",
        "                            token_type_ids=None,\n",
        "                            token_type_vocab_size=16,\n",
        "                            token_type_embedding_name=\"token_type_embeddings\",\n",
        "                            use_position_embeddings=True,\n",
        "                            position_embedding_name=\"position_embeddings\",\n",
        "                            initializer_range=0.02,\n",
        "                            max_position_embeddings=512,\n",
        "                            dropout_prob=0.1):\n",
        "  \"\"\"Performs various post-processing on a word embedding tensor.\n",
        "\n",
        "  Args:\n",
        "    input_tensor: float Tensor of shape [batch_size, seq_length,\n",
        "      embedding_size].\n",
        "    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n",
        "    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "      Must be specified if `use_token_type` is True.\n",
        "    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n",
        "    token_type_embedding_name: string. The name of the embedding table variable\n",
        "      for token type ids.\n",
        "    use_position_embeddings: bool. Whether to add position embeddings for the\n",
        "      position of each token in the sequence.\n",
        "    position_embedding_name: string. The name of the embedding table variable\n",
        "      for positional embeddings.\n",
        "    initializer_range: float. Range of the weight initialization.\n",
        "    max_position_embeddings: int. Maximum sequence length that might ever be\n",
        "      used with this model. This can be longer than the sequence length of\n",
        "      input_tensor, but cannot be shorter.\n",
        "    dropout_prob: float. Dropout probability applied to the final output tensor.\n",
        "\n",
        "  Returns:\n",
        "    float tensor with same shape as `input_tensor`.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: One of the tensor shapes or input values is invalid.\n",
        "  \"\"\"\n",
        "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  width = input_shape[2]\n",
        "\n",
        "  output = input_tensor\n",
        "\n",
        "  if use_token_type:\n",
        "    if token_type_ids is None:\n",
        "      raise ValueError(\"`token_type_ids` must be specified if\"\n",
        "                       \"`use_token_type` is True.\")\n",
        "    token_type_table = tf.get_variable(\n",
        "        name=token_type_embedding_name,\n",
        "        shape=[token_type_vocab_size, width],\n",
        "        initializer=create_initializer(initializer_range))\n",
        "    # This vocab will be small so we always do one-hot here, since it is always\n",
        "    # faster for a small vocabulary.\n",
        "    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
        "    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n",
        "    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
        "    token_type_embeddings = tf.reshape(token_type_embeddings,\n",
        "                                       [batch_size, seq_length, width])\n",
        "    output += token_type_embeddings\n",
        "\n",
        "  if use_position_embeddings:\n",
        "    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
        "    with tf.control_dependencies([assert_op]):\n",
        "      full_position_embeddings = tf.get_variable(\n",
        "          name=position_embedding_name,\n",
        "          shape=[max_position_embeddings, width],\n",
        "          initializer=create_initializer(initializer_range))\n",
        "      # Since the position embedding table is a learned variable, we create it\n",
        "      # using a (long) sequence length `max_position_embeddings`. The actual\n",
        "      # sequence length might be shorter than this, for faster training of\n",
        "      # tasks that do not have long sequences.\n",
        "      #\n",
        "      # So `full_position_embeddings` is effectively an embedding table\n",
        "      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n",
        "      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n",
        "      # perform a slice.\n",
        "      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n",
        "                                     [seq_length, -1])\n",
        "      num_dims = len(output.shape.as_list())\n",
        "\n",
        "      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n",
        "      # we broadcast among the first dimensions, which is typically just\n",
        "      # the batch size.\n",
        "      position_broadcast_shape = []\n",
        "      for _ in range(num_dims - 2):\n",
        "        position_broadcast_shape.append(1)\n",
        "      position_broadcast_shape.extend([seq_length, width])\n",
        "      position_embeddings = tf.reshape(position_embeddings,\n",
        "                                       position_broadcast_shape)\n",
        "      output += position_embeddings\n",
        "\n",
        "  output = layer_norm_and_dropout(output, dropout_prob)\n",
        "  return output\n",
        "\n",
        "\n",
        "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n",
        "  \"\"\"Create 3D attention mask from a 2D tensor mask.\n",
        "\n",
        "  Args:\n",
        "    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n",
        "    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n",
        "  \"\"\"\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "  batch_size = from_shape[0]\n",
        "  from_seq_length = from_shape[1]\n",
        "\n",
        "  to_shape = get_shape_list(to_mask, expected_rank=2)\n",
        "  to_seq_length = to_shape[1]\n",
        "\n",
        "  to_mask = tf.cast(\n",
        "      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n",
        "\n",
        "  # We don't assume that `from_tensor` is a mask (although it could be). We\n",
        "  # don't actually care if we attend *from* padding tokens (only *to* padding)\n",
        "  # tokens so we create a tensor of all ones.\n",
        "  #\n",
        "  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n",
        "  broadcast_ones = tf.ones(\n",
        "      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n",
        "\n",
        "  # Here we broadcast along two dimensions to create the mask.\n",
        "  mask = broadcast_ones * to_mask\n",
        "\n",
        "  return mask\n",
        "\n",
        "\n",
        "def attention_layer(from_tensor,\n",
        "                    to_tensor,\n",
        "                    attention_mask=None,\n",
        "                    num_attention_heads=1,\n",
        "                    size_per_head=512,\n",
        "                    query_act=None,\n",
        "                    key_act=None,\n",
        "                    value_act=None,\n",
        "                    attention_probs_dropout_prob=0.0,\n",
        "                    initializer_range=0.02,\n",
        "                    do_return_2d_tensor=False,\n",
        "                    batch_size=None,\n",
        "                    from_seq_length=None,\n",
        "                    to_seq_length=None):\n",
        "  \"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n",
        "\n",
        "  This is an implementation of multi-headed attention based on \"Attention\n",
        "  is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n",
        "  this is self-attention. Each timestep in `from_tensor` attends to the\n",
        "  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n",
        "\n",
        "  This function first projects `from_tensor` into a \"query\" tensor and\n",
        "  `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n",
        "  of tensors of length `num_attention_heads`, where each tensor is of shape\n",
        "  [batch_size, seq_length, size_per_head].\n",
        "\n",
        "  Then, the query and key tensors are dot-producted and scaled. These are\n",
        "  softmaxed to obtain attention probabilities. The value tensors are then\n",
        "  interpolated by these probabilities, then concatenated back to a single\n",
        "  tensor and returned.\n",
        "\n",
        "  In practice, the multi-headed attention are done with transposes and\n",
        "  reshapes rather than actual separate tensors.\n",
        "\n",
        "  Args:\n",
        "    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n",
        "      from_width].\n",
        "    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n",
        "    attention_mask: (optional) int32 Tensor of shape [batch_size,\n",
        "      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n",
        "      attention scores will effectively be set to -infinity for any positions in\n",
        "      the mask that are 0, and will be unchanged for positions that are 1.\n",
        "    num_attention_heads: int. Number of attention heads.\n",
        "    size_per_head: int. Size of each attention head.\n",
        "    query_act: (optional) Activation function for the query transform.\n",
        "    key_act: (optional) Activation function for the key transform.\n",
        "    value_act: (optional) Activation function for the value transform.\n",
        "    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n",
        "      attention probabilities.\n",
        "    initializer_range: float. Range of the weight initializer.\n",
        "    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n",
        "      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n",
        "      output will be of shape [batch_size, from_seq_length, num_attention_heads\n",
        "      * size_per_head].\n",
        "    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n",
        "      of the 3D version of the `from_tensor` and `to_tensor`.\n",
        "    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
        "      of the 3D version of the `from_tensor`.\n",
        "    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
        "      of the 3D version of the `to_tensor`.\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, from_seq_length,\n",
        "      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n",
        "      true, this will be of shape [batch_size * from_seq_length,\n",
        "      num_attention_heads * size_per_head]).\n",
        "\n",
        "  Raises:\n",
        "    ValueError: Any of the arguments or tensor shapes are invalid.\n",
        "  \"\"\"\n",
        "\n",
        "  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n",
        "                           seq_length, width):\n",
        "    output_tensor = tf.reshape(\n",
        "        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
        "\n",
        "    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
        "    return output_tensor\n",
        "\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n",
        "\n",
        "  if len(from_shape) != len(to_shape):\n",
        "    raise ValueError(\n",
        "        \"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n",
        "\n",
        "  if len(from_shape) == 3:\n",
        "    batch_size = from_shape[0]\n",
        "    from_seq_length = from_shape[1]\n",
        "    to_seq_length = to_shape[1]\n",
        "  elif len(from_shape) == 2:\n",
        "    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
        "      raise ValueError(\n",
        "          \"When passing in rank 2 tensors to attention_layer, the values \"\n",
        "          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
        "          \"must all be specified.\")\n",
        "\n",
        "  # Scalar dimensions referenced here:\n",
        "  #   B = batch size (number of sequences)\n",
        "  #   F = `from_tensor` sequence length\n",
        "  #   T = `to_tensor` sequence length\n",
        "  #   N = `num_attention_heads`\n",
        "  #   H = `size_per_head`\n",
        "\n",
        "  from_tensor_2d = reshape_to_matrix(from_tensor)\n",
        "  to_tensor_2d = reshape_to_matrix(to_tensor)\n",
        "\n",
        "  # `query_layer` = [B*F, N*H]\n",
        "  query_layer = tf.layers.dense(\n",
        "      from_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=query_act,\n",
        "      name=\"query\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `key_layer` = [B*T, N*H]\n",
        "  key_layer = tf.layers.dense(\n",
        "      to_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=key_act,\n",
        "      name=\"key\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `value_layer` = [B*T, N*H]\n",
        "  value_layer = tf.layers.dense(\n",
        "      to_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=value_act,\n",
        "      name=\"value\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `query_layer` = [B, N, F, H]\n",
        "  query_layer = transpose_for_scores(query_layer, batch_size,\n",
        "                                     num_attention_heads, from_seq_length,\n",
        "                                     size_per_head)\n",
        "\n",
        "  # `key_layer` = [B, N, T, H]\n",
        "  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n",
        "                                   to_seq_length, size_per_head)\n",
        "\n",
        "  # Take the dot product between \"query\" and \"key\" to get the raw\n",
        "  # attention scores.\n",
        "  # `attention_scores` = [B, N, F, T]\n",
        "  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
        "  attention_scores = tf.multiply(attention_scores,\n",
        "                                 1.0 / math.sqrt(float(size_per_head)))\n",
        "\n",
        "  if attention_mask is not None:\n",
        "    # `attention_mask` = [B, 1, F, T]\n",
        "    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
        "\n",
        "    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "    # masked positions, this operation will create a tensor which is 0.0 for\n",
        "    # positions we want to attend and -10000.0 for masked positions.\n",
        "    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
        "\n",
        "    # Since we are adding it to the raw scores before the softmax, this is\n",
        "    # effectively the same as removing these entirely.\n",
        "    attention_scores += adder\n",
        "\n",
        "  # Normalize the attention scores to probabilities.\n",
        "  # `attention_probs` = [B, N, F, T]\n",
        "  attention_probs = tf.nn.softmax(attention_scores)\n",
        "\n",
        "  # This is actually dropping out entire tokens to attend to, which might\n",
        "  # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n",
        "\n",
        "  # `value_layer` = [B, T, N, H]\n",
        "  value_layer = tf.reshape(\n",
        "      value_layer,\n",
        "      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n",
        "\n",
        "  # `value_layer` = [B, N, T, H]\n",
        "  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n",
        "\n",
        "  # `context_layer` = [B, N, F, H]\n",
        "  context_layer = tf.matmul(attention_probs, value_layer)\n",
        "\n",
        "  # `context_layer` = [B, F, N, H]\n",
        "  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
        "\n",
        "  if do_return_2d_tensor:\n",
        "    # `context_layer` = [B*F, N*H]\n",
        "    context_layer = tf.reshape(\n",
        "        context_layer,\n",
        "        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n",
        "  else:\n",
        "    # `context_layer` = [B, F, N*H]\n",
        "    context_layer = tf.reshape(\n",
        "        context_layer,\n",
        "        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n",
        "\n",
        "  return context_layer\n",
        "\n",
        "\n",
        "def transformer_model(input_tensor,\n",
        "                      attention_mask=None,\n",
        "                      hidden_size=768,\n",
        "                      num_hidden_layers=12,\n",
        "                      num_attention_heads=12,\n",
        "                      intermediate_size=3072,\n",
        "                      intermediate_act_fn=gelu,\n",
        "                      hidden_dropout_prob=0.1,\n",
        "                      attention_probs_dropout_prob=0.1,\n",
        "                      initializer_range=0.02,\n",
        "                      do_return_all_layers=False):\n",
        "  \"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n",
        "\n",
        "  This is almost an exact implementation of the original Transformer encoder.\n",
        "\n",
        "  See the original paper:\n",
        "  https://arxiv.org/abs/1706.03762\n",
        "\n",
        "  Also see:\n",
        "  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n",
        "\n",
        "  Args:\n",
        "    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n",
        "      seq_length], with 1 for positions that can be attended to and 0 in\n",
        "      positions that should not be.\n",
        "    hidden_size: int. Hidden size of the Transformer.\n",
        "    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n",
        "    num_attention_heads: int. Number of attention heads in the Transformer.\n",
        "    intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed\n",
        "      forward) layer.\n",
        "    intermediate_act_fn: function. The non-linear activation function to apply\n",
        "      to the output of the intermediate/feed-forward layer.\n",
        "    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n",
        "    attention_probs_dropout_prob: float. Dropout probability of the attention\n",
        "      probabilities.\n",
        "    initializer_range: float. Range of the initializer (stddev of truncated\n",
        "      normal).\n",
        "    do_return_all_layers: Whether to also return all layers or just the final\n",
        "      layer.\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n",
        "    hidden layer of the Transformer.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: A Tensor shape or parameter is invalid.\n",
        "  \"\"\"\n",
        "  if hidden_size % num_attention_heads != 0:\n",
        "    raise ValueError(\n",
        "        \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "        \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "\n",
        "  attention_head_size = int(hidden_size / num_attention_heads)\n",
        "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  input_width = input_shape[2]\n",
        "\n",
        "  # The Transformer performs sum residuals on all layers so the input needs\n",
        "  # to be the same as the hidden size.\n",
        "  if input_width != hidden_size:\n",
        "    raise ValueError(\"The width of the input tensor (%d) != hidden size (%d)\" %\n",
        "                     (input_width, hidden_size))\n",
        "\n",
        "  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n",
        "  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n",
        "  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n",
        "  # help the optimizer.\n",
        "  prev_output = reshape_to_matrix(input_tensor)\n",
        "\n",
        "  all_layer_outputs = []\n",
        "  for layer_idx in range(num_hidden_layers):\n",
        "    with tf.variable_scope(\"layer_%d\" % layer_idx):\n",
        "      layer_input = prev_output\n",
        "\n",
        "      with tf.variable_scope(\"attention\"):\n",
        "        attention_heads = []\n",
        "        with tf.variable_scope(\"self\"):\n",
        "          attention_head = attention_layer(\n",
        "              from_tensor=layer_input,\n",
        "              to_tensor=layer_input,\n",
        "              attention_mask=attention_mask,\n",
        "              num_attention_heads=num_attention_heads,\n",
        "              size_per_head=attention_head_size,\n",
        "              attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
        "              initializer_range=initializer_range,\n",
        "              do_return_2d_tensor=True,\n",
        "              batch_size=batch_size,\n",
        "              from_seq_length=seq_length,\n",
        "              to_seq_length=seq_length)\n",
        "          attention_heads.append(attention_head)\n",
        "\n",
        "        attention_output = None\n",
        "        if len(attention_heads) == 1:\n",
        "          attention_output = attention_heads[0]\n",
        "        else:\n",
        "          # In the case where we have other sequences, we just concatenate\n",
        "          # them to the self-attention head before the projection.\n",
        "          attention_output = tf.concat(attention_heads, axis=-1)\n",
        "\n",
        "        # Run a linear projection of `hidden_size` then add a residual\n",
        "        # with `layer_input`.\n",
        "        with tf.variable_scope(\"output\"):\n",
        "          attention_output = tf.layers.dense(\n",
        "              attention_output,\n",
        "              hidden_size,\n",
        "              kernel_initializer=create_initializer(initializer_range))\n",
        "          attention_output = dropout(attention_output, hidden_dropout_prob)\n",
        "          attention_output = layer_norm(attention_output + layer_input)\n",
        "\n",
        "      # The activation is only applied to the \"intermediate\" hidden layer.\n",
        "      with tf.variable_scope(\"intermediate\"):\n",
        "        intermediate_output = tf.layers.dense(\n",
        "            attention_output,\n",
        "            intermediate_size,\n",
        "            activation=intermediate_act_fn,\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "      # Down-project back to `hidden_size` then add the residual.\n",
        "      with tf.variable_scope(\"output\"):\n",
        "        layer_output = tf.layers.dense(\n",
        "            intermediate_output,\n",
        "            hidden_size,\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "        layer_output = dropout(layer_output, hidden_dropout_prob)\n",
        "        layer_output = layer_norm(layer_output + attention_output)\n",
        "        prev_output = layer_output\n",
        "        all_layer_outputs.append(layer_output)\n",
        "\n",
        "  if do_return_all_layers:\n",
        "    final_outputs = []\n",
        "    for layer_output in all_layer_outputs:\n",
        "      final_output = reshape_from_matrix(layer_output, input_shape)\n",
        "      final_outputs.append(final_output)\n",
        "    return final_outputs\n",
        "  else:\n",
        "    final_output = reshape_from_matrix(prev_output, input_shape)\n",
        "    return final_output\n",
        "\n",
        "\n",
        "def get_shape_list(tensor, expected_rank=None, name=None):\n",
        "  \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
        "\n",
        "  Args:\n",
        "    tensor: A tf.Tensor object to find the shape of.\n",
        "    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
        "      specified and the `tensor` has a different rank, and exception will be\n",
        "      thrown.\n",
        "    name: Optional name of the tensor for the error message.\n",
        "\n",
        "  Returns:\n",
        "    A list of dimensions of the shape of tensor. All static dimensions will\n",
        "    be returned as python integers, and dynamic dimensions will be returned\n",
        "    as tf.Tensor scalars.\n",
        "  \"\"\"\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  if expected_rank is not None:\n",
        "    assert_rank(tensor, expected_rank, name)\n",
        "\n",
        "  shape = tensor.shape.as_list()\n",
        "\n",
        "  non_static_indexes = []\n",
        "  for (index, dim) in enumerate(shape):\n",
        "    if dim is None:\n",
        "      non_static_indexes.append(index)\n",
        "\n",
        "  if not non_static_indexes:\n",
        "    return shape\n",
        "\n",
        "  dyn_shape = tf.shape(tensor)\n",
        "  for index in non_static_indexes:\n",
        "    shape[index] = dyn_shape[index]\n",
        "  return shape\n",
        "\n",
        "\n",
        "def reshape_to_matrix(input_tensor):\n",
        "  \"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"\n",
        "  ndims = input_tensor.shape.ndims\n",
        "  if ndims < 2:\n",
        "    raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
        "                     (input_tensor.shape))\n",
        "  if ndims == 2:\n",
        "    return input_tensor\n",
        "\n",
        "  width = input_tensor.shape[-1]\n",
        "  output_tensor = tf.reshape(input_tensor, [-1, width])\n",
        "  return output_tensor\n",
        "\n",
        "\n",
        "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
        "  \"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"\n",
        "  if len(orig_shape_list) == 2:\n",
        "    return output_tensor\n",
        "\n",
        "  output_shape = get_shape_list(output_tensor)\n",
        "\n",
        "  orig_dims = orig_shape_list[0:-1]\n",
        "  width = output_shape[-1]\n",
        "\n",
        "  return tf.reshape(output_tensor, orig_dims + [width])\n",
        "\n",
        "\n",
        "def assert_rank(tensor, expected_rank, name=None):\n",
        "  \"\"\"Raises an exception if the tensor rank is not of the expected rank.\n",
        "\n",
        "  Args:\n",
        "    tensor: A tf.Tensor to check the rank of.\n",
        "    expected_rank: Python integer or list of integers, expected rank.\n",
        "    name: Optional name of the tensor for the error message.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If the expected shape doesn't match the actual shape.\n",
        "  \"\"\"\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  expected_rank_dict = {}\n",
        "  if isinstance(expected_rank, six.integer_types):\n",
        "    expected_rank_dict[expected_rank] = True\n",
        "  else:\n",
        "    for x in expected_rank:\n",
        "      expected_rank_dict[x] = True\n",
        "\n",
        "  actual_rank = tensor.shape.ndims\n",
        "  if actual_rank not in expected_rank_dict:\n",
        "    scope_name = tf.get_variable_scope().name\n",
        "    raise ValueError(\n",
        "        \"For the tensor `%s` in scope `%s`, the actual rank \"\n",
        "        \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n",
        "        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CdlEnZkqmxg"
      },
      "source": [
        "#data_processors.py (for QC dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szjnNOHgqlAK"
      },
      "source": [
        "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
        "# Copyright Tor Vergata, University of Rome. All Rights Reserved.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "#\n",
        "# Data processor for the QC dataset\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "    \"\"\"Constructs a InputExample.\n",
        "\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    self.guid = guid\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b\n",
        "    self.label = label\n",
        "\n",
        "\n",
        "class PaddingInputExample(object):\n",
        "  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_ids,\n",
        "               input_mask,\n",
        "               segment_ids,\n",
        "               label_id,\n",
        "               label_mask=None,\n",
        "               is_real_example=True):\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.label_id = label_id\n",
        "    self.is_real_example = is_real_example\n",
        "    self.label_mask = label_mask\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "  \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "  def get_labeled_examples(self, data_dir):\n",
        "    \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_unlabeled_examples(self, data_dir):\n",
        "    \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_test_examples(self, data_dir):\n",
        "    \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_labels(self):\n",
        "    \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  @classmethod\n",
        "  def _read_tsv(cls, input_file, quotechar=None):\n",
        "    \"\"\"Reads a tab separated value file.\"\"\"\n",
        "    with tf.gfile.Open(input_file, \"r\") as f:\n",
        "      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "      lines = []\n",
        "      for line in reader:\n",
        "        lines.append(line)\n",
        "      return lines\n",
        "\n",
        "class QcFineProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
        "\n",
        "    def get_labeled_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(os.path.join(data_dir, \"labeled.tsv\"), \"train\")\n",
        "\n",
        "    def get_unlabeled_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(os.path.join(data_dir, \"unlabeled.tsv\"), \"train\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(os.path.join(data_dir, \"test.tsv\"), \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"UNK_UNK\", \"ABBR_abb\", \"ABBR_exp\", \"DESC_def\", \"DESC_desc\", \"DESC_manner\", \"DESC_reason\", \"ENTY_animal\", \"ENTY_body\", \"ENTY_color\", \"ENTY_cremat\", \"ENTY_currency\", \"ENTY_dismed\", \"ENTY_event\", \"ENTY_food\", \"ENTY_instru\", \"ENTY_lang\", \"ENTY_letter\", \"ENTY_other\", \"ENTY_plant\", \"ENTY_product\", \"ENTY_religion\", \"ENTY_sport\", \"ENTY_substance\", \"ENTY_symbol\", \"ENTY_techmeth\", \"ENTY_termeq\", \"ENTY_veh\", \"ENTY_word\", \"HUM_desc\", \"HUM_gr\", \"HUM_ind\", \"HUM_title\", \"LOC_city\", \"LOC_country\", \"LOC_mount\", \"LOC_other\", \"LOC_state\", \"NUM_code\", \"NUM_count\", \"NUM_date\", \"NUM_dist\", \"NUM_money\", \"NUM_ord\", \"NUM_other\", \"NUM_perc\", \"NUM_period\", \"NUM_speed\", \"NUM_temp\", \"NUM_volsize\", \"NUM_weight\"]\n",
        "\n",
        "    def _create_examples(self, input_file, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "\n",
        "        with open(input_file, 'r') as f:\n",
        "            contents = f.read()\n",
        "            file_as_list = contents.splitlines()\n",
        "            for line in file_as_list[1:]:\n",
        "                split = line.split(\" \")\n",
        "                question = ' '.join(split[1:])\n",
        "\n",
        "                guid = \"%s-%s\" % (set_type, convert_to_unicode(line))\n",
        "                text_a = convert_to_unicode(question)\n",
        "                inn_split = split[0].split(\":\")\n",
        "                label = inn_split[0] + \"_\" + inn_split[1]\n",
        "                examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "            f.close()\n",
        "\n",
        "        return examples\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI6lUxt8gHgL"
      },
      "source": [
        "# Data Processor for Three Level Sentiment Data\n",
        "\n",
        "*   Loaded from Google Drive\n",
        "*   Three csv files for labeled, test, and unlabeled\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmhYqyGyg_xg"
      },
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "all_data = pd.read_csv('/content/drive/My Drive/data-cloud/normalized/Deep/airline-folder/airline-deep-balanced.csv', encoding='utf-8')\n",
        "all_data = all_data.sample(frac=1).copy()\n",
        "\n",
        "labeled_pd = all_data[0:500].copy()\n",
        "test_pd = all_data[501:1500].copy()\n",
        "unlabeled_pd = all_data[1501:].copy()\n",
        "# setting all sentiment to 3. Here 3 is \"Unknown label\"\n",
        "unlabeled_pd['sentiment']=3 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRi84S7BgNKN"
      },
      "source": [
        "class ThreeLevelSentimentDataProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
        "\n",
        "    def get_labeled_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(labeled_pd, \"train\")\n",
        "\n",
        "    def get_unlabeled_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(unlabeled_pd, \"train\")\n",
        "\n",
        "    def get_test_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(test_pd, \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        # the first one (3) represents unknown labels\n",
        "        return [\"3\", \"2\", \"1\", \"0\"]\n",
        "\n",
        "    def _create_examples(self, input_file, set_type):\n",
        "      \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "      examples = []\n",
        "\n",
        "      for index, row in input_file.iterrows():\n",
        "        label = str(row['sentiment'])\n",
        "        text_a = row['text']\n",
        "        guid = \"%s-%s\" % (set_type, convert_to_unicode(label+\" \"+text_a))\n",
        "        examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "      \n",
        "      return examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBVQXnMqaU-"
      },
      "source": [
        "#ganbert.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64LZWMvxTe5z"
      },
      "source": [
        "# DO NOT REMOVE\n",
        "import tensorflow as tf; tf.contrib.summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcbrLPlKvaPE"
      },
      "source": [
        "!pip install git+https://github.com/guillaumegenthial/tf_metrics.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0i49x-yBuAs"
      },
      "source": [
        "## Flags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNL7DljnqEFD"
      },
      "source": [
        "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
        "# Copyright Tor Vergata, University of Rome. All Rights Reserved.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "#\n",
        "# Here is defined the GAN-BERT model, starting from the run_classifier.py https://github.com/google-research/bert/blob/master/run_classifier.py\n",
        "#\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import csv\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import tf_metrics\n",
        "\n",
        "\n",
        "BERT_BASE_DIR='cased_L-12_H-768_A-12'\n",
        "\n",
        "\n",
        "SEQ_LEN=\"64\"\n",
        "BS=\"64\"\n",
        "LR=\"2e-5\"\n",
        "EPOCHS=\"3\"\n",
        "cur_dir=\"data\"\n",
        "LABEL_RATE=\"0.02\"\n",
        "\n",
        "\n",
        "task_name='three-level-senitmenet'\n",
        "label_rate=f'{LABEL_RATE}' \n",
        "do_train=True\n",
        "do_eval=True\n",
        "do_predict=False\n",
        "data_dir=f'{cur_dir}'\n",
        "vocab_file=f'{BERT_BASE_DIR}/vocab.txt'\n",
        "bert_config_file=f'{BERT_BASE_DIR}/bert_config.json'\n",
        "init_checkpoint=f'{BERT_BASE_DIR}/bert_model.ckpt'\n",
        "max_seq_length=f'{SEQ_LEN}'\n",
        "train_batch_size=f'{BS}'\n",
        "learning_rate=f'{LR}'\n",
        "num_train_epochs=f'{EPOCHS}'\n",
        "warmup_proportion=0.1\n",
        "do_lower_case=False\n",
        "output_dir='ganbert_output_model'\n",
        "\n",
        "\n",
        "flags = tf.compat.v1.flags\n",
        "flags.DEFINE_string('f','','')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"unlabeled_multiplier\", 100,\n",
        "    \"The multiplier to compute the max number of unlabeled examples with respect to the labeled examples.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"data_dir\", data_dir,\n",
        "    \"The input data dir. Should contain the .tsv files (or other data files) \"\n",
        "    \"for the task.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"bert_config_file\", bert_config_file,\n",
        "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
        "    \"This specifies the model architecture.\")\n",
        "\n",
        "flags.DEFINE_string(\"task_name\", task_name, \"The name of the task to train.\")\n",
        "\n",
        "flags.DEFINE_string(\"vocab_file\", vocab_file,\n",
        "                    \"The vocabulary file that the BERT model was trained on.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"output_dir\", output_dir,\n",
        "    \"The output directory where the model checkpoints will be written.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"init_checkpoint\", init_checkpoint,\n",
        "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"do_lower_case\", do_lower_case,\n",
        "    \"Whether to lower case the input text. Should be True for uncased \"\n",
        "    \"models and False for cased models.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_seq_length\", 128,\n",
        "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
        "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
        "    \"than this will be padded.\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_train\", do_train, \"Whether to run training.\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_eval\", do_eval, \"Whether to run eval on the dev set.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"do_predict\", do_predict,\n",
        "    \"Whether to run the model in inference mode on the test set.\")\n",
        "\n",
        "flags.DEFINE_integer(\"train_batch_size\", train_batch_size, \"Total batch size for training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"eval_batch_size\", 8, \"Total batch size for eval.\")\n",
        "\n",
        "flags.DEFINE_integer(\"predict_batch_size\", 8, \"Total batch size for predict.\")\n",
        "\n",
        "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
        "\n",
        "flags.DEFINE_float(\"num_train_epochs\", num_train_epochs,\n",
        "                   \"Total number of training epochs to perform.\")\n",
        "\n",
        "flags.DEFINE_float(\n",
        "    \"warmup_proportion\", warmup_proportion,\n",
        "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
        "    \"E.g., 0.1 = 10% of training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
        "                     \"How often to save the model checkpoint.\")\n",
        "\n",
        "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
        "                     \"How many steps to make in each estimator call.\")\n",
        "\n",
        "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
        "\n",
        "#tpu_name = os.environ['TPU_NAME']\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"tpu_name\", None,\n",
        "    \"The Cloud TPU to use for training. This should be either the name \"\n",
        "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
        "    \"url.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"tpu_zone\", None,\n",
        "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"gcp_project\", None,\n",
        "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"num_tpu_cores\", 8,\n",
        "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n",
        "\n",
        "flags.DEFINE_float(\"label_rate\", label_rate,\n",
        "                   \"Rate for labeled examples (Used only for logging purpose).\")\n",
        "\n",
        "flags.DEFINE_float(\"dropout_keep_rate\", 0.9,\n",
        "                   \"Keep rate for dropout.\")\n",
        "\n",
        "epsilon = 1e-8\n",
        "DKP = FLAGS.dropout_keep_rate\n",
        "LATENT_Z = 100\n",
        "\n",
        "SEED = 0\n",
        "np.random.seed(SEED)\n",
        "tf.compat.v1.set_random_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li29CTycB6De"
      },
      "source": [
        "## Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hK2uyWWB8F3"
      },
      "source": [
        "\n",
        "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
        "                           tokenizer, label_mask):\n",
        "  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "  if isinstance(example, PaddingInputExample):\n",
        "    return InputFeatures(\n",
        "        input_ids=[0] * max_seq_length,\n",
        "        input_mask=[0] * max_seq_length,\n",
        "        segment_ids=[0] * max_seq_length,\n",
        "        label_id=0,\n",
        "        label_mask=label_mask,\n",
        "        is_real_example=False)\n",
        "\n",
        "  label_map = {}\n",
        "  for (i, label) in enumerate(label_list):\n",
        "    label_map[label] = i\n",
        "\n",
        "  tokens_a = tokenizer.tokenize(example.text_a)\n",
        "  tokens_b = None\n",
        "  if example.text_b:\n",
        "    tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "  if tokens_b:\n",
        "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "    # length is less than the specified length.\n",
        "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "  else:\n",
        "    # Account for [CLS] and [SEP] with \"- 2\"\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "      tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "\n",
        "  # The convention in BERT is:\n",
        "  # (a) For sequence pairs:\n",
        "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "  # (b) For single sequences:\n",
        "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "  #  type_ids: 0     0   0   0  0     0 0\n",
        "  #\n",
        "  # Where \"type_ids\" are used to indicate whether this is the first\n",
        "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "  # embedding vector (and position vector). This is not *strictly* necessary\n",
        "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "  # it easier for the model to learn the concept of sequences.\n",
        "  #\n",
        "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "  # used as the \"sentence vector\". Note that this only makes sense because\n",
        "  # the entire model is fine-tuned.\n",
        "  tokens = []\n",
        "  segment_ids = []\n",
        "  tokens.append(\"[CLS]\")\n",
        "  segment_ids.append(0)\n",
        "  for token in tokens_a:\n",
        "    tokens.append(token)\n",
        "    segment_ids.append(0)\n",
        "  tokens.append(\"[SEP]\")\n",
        "  segment_ids.append(0)\n",
        "\n",
        "  if tokens_b:\n",
        "    for token in tokens_b:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(1)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(1)\n",
        "\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "  # tokens are attended to.\n",
        "  input_mask = [1] * len(input_ids)\n",
        "\n",
        "  # Zero-pad up to the sequence length.\n",
        "  while len(input_ids) < max_seq_length:\n",
        "    input_ids.append(0)\n",
        "    input_mask.append(0)\n",
        "    segment_ids.append(0)\n",
        "\n",
        "  assert len(input_ids) == max_seq_length\n",
        "  assert len(input_mask) == max_seq_length\n",
        "  assert len(segment_ids) == max_seq_length\n",
        "\n",
        "  label_id = label_map[example.label]\n",
        "  if ex_index < 5:\n",
        "    tf.logging.info(\"*** Example ***\")\n",
        "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
        "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "        [printable_text(x) for x in tokens]))\n",
        "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "\n",
        "  feature = InputFeatures(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids,\n",
        "      label_id=label_id,\n",
        "      label_mask=label_mask,\n",
        "      is_real_example=True)\n",
        "  return feature\n",
        "\n",
        "\n",
        "def file_based_convert_examples_to_features(\n",
        "    labeled_examples, unlabeled_examples, label_list, max_seq_length, tokenizer, output_file, label_mask_rate, is_testing=False):\n",
        "  \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
        "  all_examples = labeled_examples\n",
        "  if unlabeled_examples:\n",
        "    all_examples = all_examples + unlabeled_examples\n",
        "  label_masks = get_labeled_mask(mask_size=len(all_examples), labeled_size=len(labeled_examples))\n",
        "\n",
        "  to_write_examples = list()\n",
        "  for ex_index, example in enumerate(all_examples):\n",
        "    if ex_index % 10000 == 0:\n",
        "      tf.logging.info(\"Writing example %d\" % ex_index)\n",
        "    feature = convert_single_example(ex_index, example, label_list,\n",
        "                                     max_seq_length, tokenizer, label_masks[ex_index])\n",
        "\n",
        "    def create_int_feature(values):\n",
        "      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
        "      return f\n",
        "\n",
        "    features = collections.OrderedDict()\n",
        "    features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
        "    features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
        "    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
        "    features[\"label_ids\"] = create_int_feature([feature.label_id])\n",
        "    features[\"label_mask\"] = create_int_feature([feature.label_mask])\n",
        "    features[\"is_real_example\"] = create_int_feature(\n",
        "        [int(feature.is_real_example)])\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "    if label_mask_rate == 1:\n",
        "        to_write_examples.append(tf_example)\n",
        "    else:\n",
        "        # IT SIMULATE A LABELED EXAMPLE\n",
        "        if feature.label_mask:\n",
        "            balance = int(1/label_mask_rate)\n",
        "            balance = int(math.log(balance,2))\n",
        "            if balance < 1:\n",
        "                balance = 1\n",
        "            for b in range(0, int(balance)):\n",
        "                to_write_examples.append(tf_example)\n",
        "        else:\n",
        "          to_write_examples.append(tf_example)\n",
        "\n",
        "  writer = tf.python_io.TFRecordWriter(output_file)\n",
        "  written_examples = 0\n",
        "  if not is_testing:\n",
        "    random.shuffle(to_write_examples)\n",
        "  for tf_example in to_write_examples:\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "    written_examples = written_examples + 1\n",
        "  writer.close()\n",
        "\n",
        "  return written_examples\n",
        "\n",
        "\n",
        "def file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  name_to_features = {\n",
        "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"label_ids\": tf.FixedLenFeature([], tf.int64),\n",
        "      \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
        "      \"label_mask\": tf.FixedLenFeature([], tf.int64),\n",
        "  }\n",
        "\n",
        "  def _decode_record(record, name_to_features):\n",
        "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "    example = tf.parse_single_example(record, name_to_features)\n",
        "\n",
        "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "    # So cast all int64 to int32.\n",
        "    for name in list(example.keys()):\n",
        "      t = example[name]\n",
        "      if t.dtype == tf.int64:\n",
        "        t = tf.to_int32(t)\n",
        "      example[name] = t\n",
        "\n",
        "    return example\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    if is_training:\n",
        "        batch_size = FLAGS.train_batch_size\n",
        "    else:\n",
        "        batch_size = params[\"batch_size\"]\n",
        "\n",
        "    # For training, we want a lot of parallel reading and shuffling.\n",
        "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "    d = tf.data.TFRecordDataset(input_file)\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=10000, seed=SEED)\n",
        "\n",
        "    d = d.apply(\n",
        "        tf.contrib.data.map_and_batch(\n",
        "            lambda record: _decode_record(record, name_to_features),\n",
        "            batch_size=batch_size,\n",
        "            drop_remainder=drop_remainder))\n",
        "\n",
        "    return d\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "  # This is a simple heuristic which will always truncate the longer sequence\n",
        "  # one token at a time. This makes more sense than truncating an equal percent\n",
        "  # of tokens from each, since if one sequence is very short then each token\n",
        "  # that's truncated likely contains more information than a longer sequence.\n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b)\n",
        "    if total_length <= max_length:\n",
        "      break\n",
        "    if len(tokens_a) > len(tokens_b):\n",
        "      tokens_a.pop()\n",
        "    else:\n",
        "      tokens_b.pop()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jeb0ECegCMA_"
      },
      "source": [
        "## GAN functuons - Discriminator and Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wlc0p-QCQWQ"
      },
      "source": [
        "############ Defining Discriminator ############\n",
        "def discriminator(x, d_hidden_size, dkp, is_training, num_labels, num_hidden_discriminator = 1, reuse = False):\n",
        "    with tf.compat.v1.variable_scope('Discriminator', reuse = reuse):\n",
        "        layer_hidden = tf.nn.dropout(x, keep_prob=dkp)\n",
        "        for i in range(num_hidden_discriminator):\n",
        "            layer_hidden = tf.layers.dense(layer_hidden, d_hidden_size)\n",
        "            layer_hidden = tf.nn.leaky_relu(layer_hidden)\n",
        "            layer_hidden = tf.nn.dropout(layer_hidden, keep_prob=dkp)\n",
        "        flatten5 = layer_hidden\n",
        "\n",
        "        logit = tf.layers.dense(layer_hidden, (num_labels + 1))\n",
        "        prob = tf.nn.softmax(logit)\n",
        "    return flatten5, logit, prob\n",
        "\n",
        "\n",
        "############ Defining Generator ############\n",
        "def generator(z, g_hidden_size, dkp, is_training, num_hidden_generator = 1, reuse = False):\n",
        "    with tf.compat.v1.variable_scope('Generator', reuse = reuse):\n",
        "        layer_hidden = z\n",
        "\n",
        "        for i in range(num_hidden_generator):\n",
        "            layer_hidden = tf.layers.dense(layer_hidden, g_hidden_size)\n",
        "            layer_hidden = tf.nn.leaky_relu(layer_hidden)\n",
        "            layer_hidden = tf.nn.dropout(layer_hidden, rate = 1 - dkp)\n",
        "        layer_hidden = tf.layers.dense(layer_hidden, g_hidden_size)\n",
        "\n",
        "    return layer_hidden\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y05rlPnMCVSV"
      },
      "source": [
        "## Model Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNB7nK2oCWzE"
      },
      "source": [
        "\n",
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings, label_mask):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  output_layer = model.get_pooled_output()\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "  keep_prob = 1\n",
        "  if is_training:\n",
        "      keep_prob = DKP\n",
        "\n",
        "  D_real_features, D_real_logits, D_real_prob = discriminator(output_layer, hidden_size, keep_prob, is_training,\n",
        "                                                              num_labels, reuse=False)\n",
        "\n",
        "  logits = D_real_logits[:, 1:]\n",
        "  probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "  one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "  if is_training:\n",
        "    per_example_loss =  -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    per_example_loss = tf.boolean_mask(per_example_loss, label_mask)\n",
        "\n",
        "    labeled_example_count = tf.cast(tf.size(per_example_loss), tf.float32)\n",
        "    D_L_Supervised = tf.divide(tf.reduce_sum(per_example_loss), tf.maximum(labeled_example_count, 1))\n",
        "  else:\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    D_L_Supervised = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "  z = tf.random_uniform([FLAGS.train_batch_size, LATENT_Z], minval=0, maxval=1, dtype=tf.float32, seed=SEED, name=None)\n",
        "  x_g = generator(z, hidden_size, keep_prob, is_training=is_training, reuse=False)\n",
        "  D_fake_features, DU_fake_logits, DU_fake_prob = discriminator(x_g, hidden_size, keep_prob, is_training, num_labels, reuse=True)\n",
        "  \n",
        "  D_L_unsupervised1U = -1 * tf.reduce_mean(tf.math.log(1 - D_real_prob[:, 0] + epsilon))\n",
        "  \n",
        "  D_L_unsupervised2U = -1 * tf.reduce_mean(tf.math.log(DU_fake_prob[:, 0] + epsilon))\n",
        "  d_loss =  D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
        "  \n",
        "  g_loss = -1 * tf.reduce_mean(tf.math.log(1 - DU_fake_prob[:, 0] + epsilon))\n",
        "\n",
        "  G_feat_match = tf.reduce_mean(tf.square(tf.reduce_mean(D_real_features, axis=0) - tf.reduce_mean(D_fake_features, axis=0)))\n",
        "  g_loss = g_loss + G_feat_match\n",
        "\n",
        "  return (d_loss, g_loss, per_example_loss, logits, probabilities)\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    label_mask = features[\"label_mask\"]\n",
        "    is_real_example = None\n",
        "    if \"is_real_example\" in features:\n",
        "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "    else:\n",
        "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (d_loss, g_loss, per_example_loss, logits, probabilities) = create_model(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings, label_mask)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "\n",
        "    bert_vars = [v for v in tvars if 'bert' in v.name]\n",
        "    d_vars = bert_vars + [v for v in tvars if 'Discriminator' in v.name]\n",
        "    g_vars = [v for v in tvars if 'Generator' in v.name]\n",
        "\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      d_train_op = create_optimizer(\"d\", d_vars,\n",
        "          d_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "\n",
        "      g_train_op = create_optimizer(\"g\", g_vars,\n",
        "          g_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      logging_hook = tf.train.LoggingTensorHook({\"d_loss\": d_loss, \"g_loss\": g_loss, \"per_example_loss\": per_example_loss}, every_n_iter=1)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=d_loss + g_loss,\n",
        "          train_op=tf.group(d_train_op, g_train_op),\n",
        "          training_hooks=[logging_hook],\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(\n",
        "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
        "        precision = tf_metrics.precision(labels=label_ids, predictions=predictions, num_classes=num_labels,\n",
        "                                         weights=is_real_example)\n",
        "        recall = tf_metrics.recall(labels=label_ids, predictions=predictions, num_classes=num_labels,\n",
        "                                   weights=is_real_example)\n",
        "        f1_micro = tf_metrics.f1(labels=label_ids, predictions=predictions, num_classes=num_labels,\n",
        "                           weights=is_real_example, average='micro')\n",
        "        f1_macro = tf_metrics.f1(labels=label_ids, predictions=predictions, num_classes=num_labels,\n",
        "                                 weights=is_real_example, average='macro')\n",
        "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_precision\": precision,\n",
        "            \"eval_recall\": recall,\n",
        "            \"eval_f1_micro\": f1_micro,\n",
        "            \"eval_f1_macro\": f1_macro,\n",
        "            \"eval_loss\": loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = (metric_fn,\n",
        "                      [per_example_loss, label_ids, logits, is_real_example])\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=d_loss,\n",
        "          eval_metrics=eval_metrics,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    else:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\"probabilities\": probabilities},\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn\n",
        "\n",
        "\n",
        "def get_labeled_mask(mask_size, labeled_size):\n",
        "    labeled_mask = np.zeros([mask_size], dtype = np.int16)\n",
        "    labeled_mask[range(labeled_size)] = 1\n",
        "    labeled_mask = 0.5 < labeled_mask\n",
        "    return labeled_mask\n",
        "\n",
        "\n",
        "def evaluate(estimator, label_rate, eval_examples, task_name, label_list, tokenizer):\n",
        "    num_actual_eval_examples = len(eval_examples)\n",
        "    if FLAGS.use_tpu:\n",
        "        # TPU requires a fixed batch size for all batches, therefore the number\n",
        "        # of examples must be a multiple of the batch size, or else examples\n",
        "        # will get dropped. So we pad with fake examples which are ignored\n",
        "        # later on. These do NOT count towards the metric (all tf.metrics\n",
        "        # support a per-instance weight, and these get a weight of 0.0).\n",
        "        while len(eval_examples) % FLAGS.eval_batch_size != 0:\n",
        "            eval_examples.append(PaddingInputExample())\n",
        "\n",
        "    eval_file = os.path.join(FLAGS.output_dir, \"eval_\"+str(task_name)+\".tf_record\")\n",
        "    file_based_convert_examples_to_features(\n",
        "        eval_examples, None, label_list, FLAGS.max_seq_length, tokenizer, eval_file, label_mask_rate=1)\n",
        "\n",
        "    tf.logging.info(\"***** Running evaluation *****\")\n",
        "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                    len(eval_examples), num_actual_eval_examples,\n",
        "                    len(eval_examples) - num_actual_eval_examples)\n",
        "    tf.logging.info(\"  Batch size = %d\", FLAGS.eval_batch_size)\n",
        "\n",
        "    #  This tells the estimator to run through the entire set.\n",
        "    eval_steps = None\n",
        "    # However, if running eval on the TPU, you will need to specify the\n",
        "    # number of steps.\n",
        "    if FLAGS.use_tpu:\n",
        "        assert len(eval_examples) % FLAGS.eval_batch_size == 0\n",
        "        eval_steps = int(len(eval_examples) // FLAGS.eval_batch_size)\n",
        "\n",
        "    eval_drop_remainder = True if FLAGS.use_tpu else False\n",
        "    eval_input_fn = file_based_input_fn_builder(\n",
        "        input_file=eval_file,\n",
        "        seq_length=FLAGS.max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=eval_drop_remainder)\n",
        "\n",
        "    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "\n",
        "    overall_result_file = open(task_name + \"_statistics_GANBERT\" + str(label_rate) + \".txt\", \"a+\")\n",
        "\n",
        "    for key in sorted(result.keys()):\n",
        "        overall_result_file.write(str(label_rate) + \" \")\n",
        "        overall_result_file.write(\"%s = %s \" % (key, str(result[key])))\n",
        "    overall_result_file.write(\"\\n\")\n",
        "\n",
        "    output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results_\"+str(task_name)+\".txt\")\n",
        "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "        tf.logging.info(\"***** Eval results *****\")\n",
        "        for key in sorted(result.keys()):\n",
        "            tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO5bsa_WCaGO"
      },
      "source": [
        "## Main "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuAbpzU3CdpK"
      },
      "source": [
        "###########################\n",
        "#def main(_):\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "label_rate = FLAGS.label_rate\n",
        "\n",
        "processors = {\"three-level-senitmenet\": ThreeLevelSentimentDataProcessor}\n",
        "\n",
        "validate_case_matches_checkpoint(FLAGS.do_lower_case,\n",
        "                                              FLAGS.init_checkpoint)\n",
        "\n",
        "if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:\n",
        "  raise ValueError(\n",
        "      \"At least one of `do_train`, `do_eval` or `do_predict' must be True.\")\n",
        "\n",
        "bert_config = BertConfig.from_json_file(FLAGS.bert_config_file)\n",
        "\n",
        "if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n",
        "  raise ValueError(\n",
        "      \"Cannot use sequence length %d because the BERT model \"\n",
        "      \"was only trained up to sequence length %d\" %\n",
        "      (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n",
        "\n",
        "tf.gfile.MakeDirs(FLAGS.output_dir)\n",
        "\n",
        "task_name = FLAGS.task_name.lower()\n",
        "\n",
        "if task_name not in processors:\n",
        "  raise ValueError(\"Task not found: %s\" % (task_name))\n",
        "\n",
        "processor = processors[task_name]()\n",
        "\n",
        "label_list = processor.get_labels()\n",
        "\n",
        "tokenizer = FullTokenizer(\n",
        "    vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
        "\n",
        "tpu_cluster_resolver = None\n",
        "if FLAGS.use_tpu and FLAGS.tpu_name:\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
        "      FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
        "\n",
        "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "  cluster=tpu_cluster_resolver,\n",
        "  master=FLAGS.master,\n",
        "  model_dir=FLAGS.output_dir,\n",
        "  save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
        "  tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "      iterations_per_loop=FLAGS.iterations_per_loop,\n",
        "      num_shards=FLAGS.num_tpu_cores,\n",
        "      per_host_input_for_training=is_per_host))\n",
        "\n",
        "num_train_steps = None\n",
        "num_warmup_steps = None\n",
        "if FLAGS.do_train:\n",
        "  labeled_examples = processor.get_labeled_examples()\n",
        "  unlabeled_examples = processor.get_unlabeled_examples()\n",
        "\n",
        "  num_train_examples = len(labeled_examples) + len(unlabeled_examples)\n",
        "\n",
        "  num_train_steps = int(\n",
        "        num_train_examples / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
        "  num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "    bert_config=bert_config,\n",
        "    num_labels=len(label_list),\n",
        "    init_checkpoint=FLAGS.init_checkpoint,\n",
        "    learning_rate=FLAGS.learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=FLAGS.use_tpu,\n",
        "    use_one_hot_embeddings=FLAGS.use_tpu)\n",
        "\n",
        "# If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "# or GPU.\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=FLAGS.use_tpu,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=FLAGS.train_batch_size,\n",
        "    eval_batch_size=FLAGS.eval_batch_size,\n",
        "    predict_batch_size=FLAGS.predict_batch_size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7F3AJi3SRES"
      },
      "source": [
        "###Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KObUMoyRFiB"
      },
      "source": [
        "if FLAGS.do_train:\n",
        "  train_file = os.path.join(FLAGS.output_dir, \"train.tf_record\")\n",
        "  num_written_examples = file_based_convert_examples_to_features(\n",
        "      labeled_examples, unlabeled_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file,\n",
        "      label_mask_rate=label_rate)\n",
        "\n",
        "  real_num_train_steps = int(\n",
        "        num_written_examples / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
        "\n",
        "  tf.logging.info(\"***** Running training *****\")\n",
        "  tf.logging.info(\"  Num examples = %d\", len(labeled_examples) + len(unlabeled_examples))\n",
        "  tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
        "  tf.logging.info(\"  Num steps = %d\", real_num_train_steps)\n",
        "  train_input_fn = file_based_input_fn_builder(\n",
        "      input_file=train_file,\n",
        "      seq_length=FLAGS.max_seq_length,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=real_num_train_steps)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZAEnTbQSS6_"
      },
      "source": [
        "###Eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D-7-paYRDc2"
      },
      "source": [
        "##### ORIGINAL EVAL\n",
        "# if FLAGS.do_eval:\n",
        "#   eval_examples = processor.get_test_examples()\n",
        "#   evaluate(estimator=estimator, label_rate=label_rate, eval_examples=eval_examples,\n",
        "#            task_name=task_name, label_list=label_list, tokenizer=tokenizer)\n",
        "\n",
        "#def evaluate(estimator, label_rate, eval_examples, task_name, label_list, tokenizer):\n",
        "eval_examples = processor.get_test_examples()\n",
        "num_actual_eval_examples = len(eval_examples)\n",
        "if FLAGS.use_tpu:\n",
        "    # TPU requires a fixed batch size for all batches, therefore the number\n",
        "    # of examples must be a multiple of the batch size, or else examples\n",
        "    # will get dropped. So we pad with fake examples which are ignored\n",
        "    # later on. These do NOT count towards the metric (all tf.metrics\n",
        "    # support a per-instance weight, and these get a weight of 0.0).\n",
        "    while len(eval_examples) % FLAGS.eval_batch_size != 0:\n",
        "        eval_examples.append(PaddingInputExample())\n",
        "\n",
        "eval_file = os.path.join(FLAGS.output_dir, \"eval_\"+str(task_name)+\".tf_record\")\n",
        "file_based_convert_examples_to_features(\n",
        "    eval_examples, None, label_list, FLAGS.max_seq_length, tokenizer, eval_file, label_mask_rate=1)\n",
        "\n",
        "tf.logging.info(\"***** Running evaluation *****\")\n",
        "tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                len(eval_examples), num_actual_eval_examples,\n",
        "                len(eval_examples) - num_actual_eval_examples)\n",
        "tf.logging.info(\"  Batch size = %d\", FLAGS.eval_batch_size)\n",
        "\n",
        "#  This tells the estimator to run through the entire set.\n",
        "eval_steps = None\n",
        "# However, if running eval on the TPU, you will need to specify the\n",
        "# number of steps.\n",
        "if FLAGS.use_tpu:\n",
        "    assert len(eval_examples) % FLAGS.eval_batch_size == 0\n",
        "    eval_steps = int(len(eval_examples) // FLAGS.eval_batch_size)\n",
        "\n",
        "eval_drop_remainder = True if FLAGS.use_tpu else False\n",
        "eval_input_fn = file_based_input_fn_builder(\n",
        "    input_file=eval_file,\n",
        "    seq_length=FLAGS.max_seq_length,\n",
        "    is_training=False,\n",
        "    drop_remainder=eval_drop_remainder)\n",
        "\n",
        "result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "\n",
        "overall_result_file = open(task_name + \"_statistics_GANBERT\" + str(label_rate) + \".txt\", \"a+\")\n",
        "\n",
        "for key in sorted(result.keys()):\n",
        "    overall_result_file.write(str(label_rate) + \" \")\n",
        "    overall_result_file.write(\"%s = %s \" % (key, str(result[key])))\n",
        "overall_result_file.write(\"\\n\")\n",
        "\n",
        "output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results_\"+str(task_name)+\".txt\")\n",
        "with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "    tf.logging.info(\"***** Eval results *****\")\n",
        "    for key in sorted(result.keys()):\n",
        "        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
        "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liB7dQzJSV9J"
      },
      "source": [
        "###Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuIMD2trRBFC"
      },
      "source": [
        "\n",
        "if FLAGS.do_predict:\n",
        "  predict_examples = processor.get_test_examples()\n",
        "  num_actual_predict_examples = len(predict_examples)\n",
        "  if FLAGS.use_tpu:\n",
        "    # TPU requires a fixed batch size for all batches, therefore the number\n",
        "    # of examples must be a multiple of the batch size, or else examples\n",
        "    # will get dropped. So we pad with fake examples which are ignored\n",
        "    # later on.\n",
        "    while len(predict_examples) % FLAGS.predict_batch_size != 0:\n",
        "      predict_examples.append(PaddingInputExample())\n",
        "\n",
        "  predict_file = os.path.join(FLAGS.output_dir, \"predict.tf_record\")\n",
        "  file_based_convert_examples_to_features(predict_examples, None, label_list,\n",
        "                                          FLAGS.max_seq_length, tokenizer,\n",
        "                                          predict_file, label_mask_rate=label_rate, is_testing=True)\n",
        "\n",
        "  tf.logging.info(\"***** Running prediction*****\")\n",
        "  tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                  len(predict_examples), num_actual_predict_examples,\n",
        "                  len(predict_examples) - num_actual_predict_examples)\n",
        "  tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\n",
        "\n",
        "  predict_drop_remainder = True if FLAGS.use_tpu else False\n",
        "  predict_input_fn = file_based_input_fn_builder(\n",
        "      input_file=predict_file,\n",
        "      seq_length=FLAGS.max_seq_length,\n",
        "      is_training=False,\n",
        "      drop_remainder=predict_drop_remainder)\n",
        "\n",
        "  result_preds = estimator.predict(input_fn=predict_input_fn)\n",
        "\n",
        "\n",
        "  output_predict_file = os.path.join(FLAGS.output_dir, \"test_results.tsv\")\n",
        "  with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n",
        "    num_written_lines = 0\n",
        "    tf.logging.info(\"***** Predict results *****\")\n",
        "    for (i, prediction) in enumerate(result):\n",
        "      probabilities = prediction[\"probabilities\"]\n",
        "      if i >= num_actual_predict_examples:\n",
        "        break\n",
        "      output_line = \"\\t\".join(\n",
        "          str(class_probability)\n",
        "          for class_probability in probabilities) + \"\\n\"\n",
        "      writer.write(output_line)\n",
        "      num_written_lines += 1\n",
        "  assert num_written_lines == num_actual_predict_examples"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}