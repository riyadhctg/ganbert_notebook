{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gan-Bert_Colab-Clean",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPreGW8s6YEFC8DpW2ZYJA0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riyadhctg/ganbert_notebook/blob/main/Gan_Bert_Colab_Clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbaqCbgp2L6o"
      },
      "source": [
        "#NOTICE\n",
        "This is an attempt to run Gan-BERT as notebook.\n",
        "\n",
        "**Original Gan-Bert Code** - https://github.com/crux82/ganbert\n",
        "\n",
        "**Gan-Bert Paper:**\n",
        "Croce, D., Castellucci, G., & Basili, R. (2020, July). Gan-bert: Generative adversarial learning for robust text classification with a bunch of labeled examples. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (pp. 2114-2119).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmzwNJ8D3FPY"
      },
      "source": [
        "#Instructions\n",
        "\n",
        "\n",
        "\n",
        "1.   Please follow the instructions from the original git repository\n",
        "2.   You can upload your data to Colab runtime using the UI. To do this for this notebook, create a folder named \"data\" in the default colab directory. Then have your three tsv files (labeled, test, and unlabeled) formatted as original Gan-bert data (i.e. QC task), and upload them in the \"data\" folder you just created\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCgLMDDTrVxA"
      },
      "source": [
        "#Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkNUyDSwuplS",
        "outputId": "39fd9bc5-458e-4ed3-dc0e-72a976bb027c"
      },
      "source": [
        "!pip install git+https://github.com/guillaumegenthial/tf_metrics.git \n",
        "!pip install gast==0.2.2 \n",
        "!pip install tensorflow==1.15\n",
        "!pip install 'tensorflow-estimator<1.15.0rc0,>=1.14.0rc0' --force-reinstall\n",
        "\n",
        "!BERT_BASE_DIR=cased_L-12_H-768_A-12\n",
        "\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip \n",
        "!unzip cased_L-12_H-768_A-12.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/guillaumegenthial/tf_metrics.git\n",
            "  Cloning https://github.com/guillaumegenthial/tf_metrics.git to /tmp/pip-req-build-_iu7053z\n",
            "  Running command git clone -q https://github.com/guillaumegenthial/tf_metrics.git /tmp/pip-req-build-_iu7053z\n",
            "Requirement already satisfied (use --upgrade to upgrade): tf-metrics==0.0.1 from git+https://github.com/guillaumegenthial/tf_metrics.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tf-metrics==0.0.1) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-gpu>=1.6 in /usr/local/lib/python3.7/dist-packages (from tf-metrics==0.0.1) (2.4.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (2.10.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.12.4)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.12)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (2.4.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.7.4.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.3.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.36.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.1.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (2.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.12.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (54.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.28.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.4.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.1.0)\n",
            "Building wheels for collected packages: tf-metrics\n",
            "  Building wheel for tf-metrics (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tf-metrics: filename=tf_metrics-0.0.1-cp37-none-any.whl size=7692 sha256=679b0aa7fa77c0b863d39ed5ad2ea67d408e175750a1822f4707669bfc4a39fa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3un9f8o3/wheels/da/6c/c8/663ef339a0666590dc53bd13bab86643a1f9c35b26742d7876\n",
            "Successfully built tf-metrics\n",
            "Processing /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd/gast-0.2.2-cp37-none-any.whl\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-gpu 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2\n",
            "Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Using cached https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.32.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Using cached https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (54.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "\u001b[31mERROR: tensorflow-gpu 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-gpu 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-gpu 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "Successfully installed tensorboard-1.15.0 tensorflow-estimator-1.15.1\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "  Using cached https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-gpu 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-gpu 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-gpu 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorflow-estimator-1.14.0\n",
            "--2021-04-03 20:35:10--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.147.128, 142.250.136.128, 142.250.148.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.147.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 404261442 (386M) [application/zip]\n",
            "Saving to: ‘cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "cased_L-12_H-768_A- 100%[===================>] 385.53M   162MB/s    in 2.4s    \n",
            "\n",
            "2021-04-03 20:35:13 (162 MB/s) - ‘cased_L-12_H-768_A-12.zip’ saved [404261442/404261442]\n",
            "\n",
            "Archive:  cased_L-12_H-768_A-12.zip\n",
            "   creating: cased_L-12_H-768_A-12/\n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: cased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-snHc60eq-8h"
      },
      "source": [
        "#tokenization.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqj9XC0Lq94y"
      },
      "source": [
        "# This file comes originally from https://github.com/google-research/bert/blob/master/tokenization.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import unicodedata\n",
        "import six\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n",
        "  \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n",
        "\n",
        "  # The casing has to be passed in by the user and there is no explicit check\n",
        "  # as to whether it matches the checkpoint. The casing information probably\n",
        "  # should have been stored in the bert_config.json file, but it's not, so\n",
        "  # we have to heuristically detect it to validate.\n",
        "\n",
        "  if not init_checkpoint:\n",
        "    return\n",
        "\n",
        "  m = re.match(\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\", init_checkpoint)\n",
        "  if m is None:\n",
        "    return\n",
        "\n",
        "  model_name = m.group(1)\n",
        "\n",
        "  lower_models = [\n",
        "      \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n",
        "      \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  cased_models = [\n",
        "      \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n",
        "      \"multi_cased_L-12_H-768_A-12\"\n",
        "  ]\n",
        "\n",
        "  is_bad_config = False\n",
        "  if model_name in lower_models and not do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"False\"\n",
        "    case_name = \"lowercased\"\n",
        "    opposite_flag = \"True\"\n",
        "\n",
        "  if model_name in cased_models and do_lower_case:\n",
        "    is_bad_config = True\n",
        "    actual_flag = \"True\"\n",
        "    case_name = \"cased\"\n",
        "    opposite_flag = \"False\"\n",
        "\n",
        "  if is_bad_config:\n",
        "    raise ValueError(\n",
        "        \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n",
        "        \"However, `%s` seems to be a %s model, so you \"\n",
        "        \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n",
        "        \"how the model was pre-training. If this error is wrong, please \"\n",
        "        \"just comment out this check.\" % (actual_flag, init_checkpoint,\n",
        "                                          model_name, case_name, opposite_flag))\n",
        "\n",
        "\n",
        "def convert_to_unicode(text):\n",
        "  \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    elif isinstance(text, unicode):\n",
        "      return text\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def printable_text(text):\n",
        "  \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
        "\n",
        "  # These functions want `str` for both Python2 and Python3, but in one case\n",
        "  # it's a Unicode string and in the other it's a byte string.\n",
        "  if six.PY3:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, bytes):\n",
        "      return text.decode(\"utf-8\", \"ignore\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  elif six.PY2:\n",
        "    if isinstance(text, str):\n",
        "      return text\n",
        "    elif isinstance(text, unicode):\n",
        "      return text.encode(\"utf-8\")\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "  else:\n",
        "    raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "  \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "  vocab = collections.OrderedDict()\n",
        "  index = 0\n",
        "  with tf.gfile.GFile(vocab_file, \"r\") as reader:\n",
        "    while True:\n",
        "      token = convert_to_unicode(reader.readline())\n",
        "      if not token:\n",
        "        break\n",
        "      token = token.strip()\n",
        "      vocab[token] = index\n",
        "      index += 1\n",
        "  return vocab\n",
        "\n",
        "\n",
        "def convert_by_vocab(vocab, items):\n",
        "  \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n",
        "  output = []\n",
        "  for item in items:\n",
        "    output.append(vocab[item])\n",
        "  return output\n",
        "\n",
        "\n",
        "def convert_tokens_to_ids(vocab, tokens):\n",
        "  return convert_by_vocab(vocab, tokens)\n",
        "\n",
        "\n",
        "def convert_ids_to_tokens(inv_vocab, ids):\n",
        "  return convert_by_vocab(inv_vocab, ids)\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "  \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "  text = text.strip()\n",
        "  if not text:\n",
        "    return []\n",
        "  tokens = text.split()\n",
        "  return tokens\n",
        "\n",
        "\n",
        "class FullTokenizer(object):\n",
        "  \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_file, do_lower_case=True):\n",
        "    self.vocab = load_vocab(vocab_file)\n",
        "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    split_tokens = []\n",
        "    for token in self.basic_tokenizer.tokenize(text):\n",
        "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "        split_tokens.append(sub_token)\n",
        "\n",
        "    return split_tokens\n",
        "\n",
        "  def convert_tokens_to_ids(self, tokens):\n",
        "    return convert_by_vocab(self.vocab, tokens)\n",
        "\n",
        "  def convert_ids_to_tokens(self, ids):\n",
        "    return convert_by_vocab(self.inv_vocab, ids)\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "  \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "  def __init__(self, do_lower_case=True):\n",
        "    \"\"\"Constructs a BasicTokenizer.\n",
        "\n",
        "    Args:\n",
        "      do_lower_case: Whether to lower case the input.\n",
        "    \"\"\"\n",
        "    self.do_lower_case = do_lower_case\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "    text = convert_to_unicode(text)\n",
        "    text = self._clean_text(text)\n",
        "\n",
        "    # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "    # models. This is also applied to the English models now, but it doesn't\n",
        "    # matter since the English models were not trained on any Chinese data\n",
        "    # and generally don't have any Chinese data in them (there are Chinese\n",
        "    # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "    # words in the English Wikipedia.).\n",
        "    text = self._tokenize_chinese_chars(text)\n",
        "\n",
        "    orig_tokens = whitespace_tokenize(text)\n",
        "    split_tokens = []\n",
        "    for token in orig_tokens:\n",
        "      if self.do_lower_case:\n",
        "        token = token.lower()\n",
        "        token = self._run_strip_accents(token)\n",
        "      split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "    output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "    return output_tokens\n",
        "\n",
        "  def _run_strip_accents(self, text):\n",
        "    \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "    text = unicodedata.normalize(\"NFD\", text)\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cat = unicodedata.category(char)\n",
        "      if cat == \"Mn\":\n",
        "        continue\n",
        "      output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _run_split_on_punc(self, text):\n",
        "    \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "    chars = list(text)\n",
        "    i = 0\n",
        "    start_new_word = True\n",
        "    output = []\n",
        "    while i < len(chars):\n",
        "      char = chars[i]\n",
        "      if _is_punctuation(char):\n",
        "        output.append([char])\n",
        "        start_new_word = True\n",
        "      else:\n",
        "        if start_new_word:\n",
        "          output.append([])\n",
        "        start_new_word = False\n",
        "        output[-1].append(char)\n",
        "      i += 1\n",
        "\n",
        "    return [\"\".join(x) for x in output]\n",
        "\n",
        "  def _tokenize_chinese_chars(self, text):\n",
        "    \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if self._is_chinese_char(cp):\n",
        "        output.append(\" \")\n",
        "        output.append(char)\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "  def _is_chinese_char(self, cp):\n",
        "    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "    #\n",
        "    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "    # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "    # space-separated words, so they are not treated specially and handled\n",
        "    # like the all of the other languages.\n",
        "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "  def _clean_text(self, text):\n",
        "    \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "    output = []\n",
        "    for char in text:\n",
        "      cp = ord(char)\n",
        "      if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "        continue\n",
        "      if _is_whitespace(char):\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "  \"\"\"Runs WordPiece tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n",
        "    self.vocab = vocab\n",
        "    self.unk_token = unk_token\n",
        "    self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "\n",
        "    This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "    using the given vocabulary.\n",
        "\n",
        "    For example:\n",
        "      input = \"unaffable\"\n",
        "      output = [\"un\", \"##aff\", \"##able\"]\n",
        "\n",
        "    Args:\n",
        "      text: A single token or whitespace separated tokens. This should have\n",
        "        already been passed through `BasicTokenizer.\n",
        "\n",
        "    Returns:\n",
        "      A list of wordpiece tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    text = convert_to_unicode(text)\n",
        "\n",
        "    output_tokens = []\n",
        "    for token in whitespace_tokenize(text):\n",
        "      chars = list(token)\n",
        "      if len(chars) > self.max_input_chars_per_word:\n",
        "        output_tokens.append(self.unk_token)\n",
        "        continue\n",
        "\n",
        "      is_bad = False\n",
        "      start = 0\n",
        "      sub_tokens = []\n",
        "      while start < len(chars):\n",
        "        end = len(chars)\n",
        "        cur_substr = None\n",
        "        while start < end:\n",
        "          substr = \"\".join(chars[start:end])\n",
        "          if start > 0:\n",
        "            substr = \"##\" + substr\n",
        "          if substr in self.vocab:\n",
        "            cur_substr = substr\n",
        "            break\n",
        "          end -= 1\n",
        "        if cur_substr is None:\n",
        "          is_bad = True\n",
        "          break\n",
        "        sub_tokens.append(cur_substr)\n",
        "        start = end\n",
        "\n",
        "      if is_bad:\n",
        "        output_tokens.append(self.unk_token)\n",
        "      else:\n",
        "        output_tokens.extend(sub_tokens)\n",
        "    return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "  \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "  # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "  # as whitespace since they are generally considered as such.\n",
        "  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat == \"Zs\":\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "  \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "  # These are technically control characters but we count them as whitespace\n",
        "  # characters.\n",
        "  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return False\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat in (\"Cc\", \"Cf\"):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "  \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "  cp = ord(char)\n",
        "  # We treat all non-letter/number ASCII as punctuation.\n",
        "  # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "  # Punctuation class but we treat them as punctuation anyways, for\n",
        "  # consistency.\n",
        "  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat.startswith(\"P\"):\n",
        "    return True\n",
        "  return False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7848BuX_q3u5"
      },
      "source": [
        "#optimization.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-SVl_Smq23x"
      },
      "source": [
        "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
        "# Copyright Tor Vergata, University of Rome. All Rights Reserved.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "#\n",
        "# Modification of the https://github.com/google-research/bert/blob/master/optimization.py\n",
        "# script for GAN-BERT.\n",
        "\n",
        "\"\"\"Functions and classes related to optimization (weight updates) for GAN-BERT.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def create_optimizer(prefix_name, tvars, loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
        "  \"\"\"Creates an optimizer training op.\"\"\"\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "  #global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
        "\n",
        "  # Implements linear decay of the learning rate.\n",
        "  learning_rate = tf.train.polynomial_decay(\n",
        "      learning_rate,\n",
        "      global_step,\n",
        "      num_train_steps,\n",
        "      end_learning_rate=0.0,\n",
        "      power=1.0,\n",
        "      cycle=False)\n",
        "\n",
        "  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
        "  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
        "  if num_warmup_steps:\n",
        "    global_steps_int = tf.cast(global_step, tf.int32)\n",
        "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
        "\n",
        "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
        "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
        "\n",
        "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
        "    warmup_learning_rate = init_lr * warmup_percent_done\n",
        "\n",
        "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
        "    learning_rate = (\n",
        "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
        "\n",
        "  # It is recommended that you use this optimizer for fine tuning, since this\n",
        "  # is how the model was trained (note that the Adam m/v variables are NOT\n",
        "  # loaded from init_checkpoint.)\n",
        "  optimizer = AdamWeightDecayOptimizer(\n",
        "      learning_rate=learning_rate,\n",
        "      weight_decay_rate=0.01,\n",
        "      beta_1=0.9,\n",
        "      beta_2=0.999,\n",
        "      epsilon=1e-6,\n",
        "      prefix_name = prefix_name,\n",
        "      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
        "\n",
        "  if use_tpu:\n",
        "    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
        "\n",
        "  #tvars = tf.trainable_variables()\n",
        "  grads = tf.gradients(loss, tvars)\n",
        "\n",
        "  # This is how the model was pre-trained.\n",
        "  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
        "\n",
        "  train_op = optimizer.apply_gradients(\n",
        "      zip(grads, tvars), global_step=global_step)\n",
        "\n",
        "  # Normally the global step update is done inside of `apply_gradients`.\n",
        "  # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
        "  # a different optimizer, you should probably take this line out.\n",
        "  new_global_step = global_step + 1\n",
        "  train_op = tf.group(train_op, [global_step.assign(new_global_step)])\n",
        "  return train_op\n",
        "\n",
        "\n",
        "class AdamWeightDecayOptimizer(tf.compat.v1.train.AdamOptimizer):\n",
        "  \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate,\n",
        "               weight_decay_rate=0.0,\n",
        "               beta_1=0.9,\n",
        "               beta_2=0.999,\n",
        "               epsilon=1e-6,\n",
        "               prefix_name=\"UNK\",\n",
        "               exclude_from_weight_decay=None,\n",
        "               name=\"AdamWeightDecayOptimizer\"):\n",
        "    \"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"\n",
        "    super(AdamWeightDecayOptimizer, self).__init__(False, name)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.weight_decay_rate = weight_decay_rate\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "    self.epsilon = epsilon\n",
        "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "    self.prefix_name=prefix_name\n",
        "\n",
        "  def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    assignments = []\n",
        "    for (grad, param) in grads_and_vars:\n",
        "      if grad is None or param is None:\n",
        "        continue\n",
        "\n",
        "      param_name = self._get_variable_name(param.name)\n",
        "\n",
        "      m = tf.get_variable(\n",
        "          name=param_name + \"/adam_m\" + self.prefix_name,\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "      v = tf.get_variable(\n",
        "          name=param_name + \"/adam_v\" + self.prefix_name,\n",
        "          shape=param.shape.as_list(),\n",
        "          dtype=tf.float32,\n",
        "          trainable=False,\n",
        "          initializer=tf.zeros_initializer())\n",
        "\n",
        "      # Standard Adam update.\n",
        "      next_m = (\n",
        "          tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad))\n",
        "      next_v = (\n",
        "          tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2,\n",
        "                                                    tf.square(grad)))\n",
        "\n",
        "      update = next_m / (tf.sqrt(next_v) + self.epsilon)\n",
        "\n",
        "      # Just adding the square of the weights to the loss function is *not*\n",
        "      # the correct way of using L2 regularization/weight decay with Adam,\n",
        "      # since that will interact with the m and v parameters in strange ways.\n",
        "      #\n",
        "      # Instead we want ot decay the weights in a manner that doesn't interact\n",
        "      # with the m/v parameters. This is equivalent to adding the square\n",
        "      # of the weights to the loss with plain (non-momentum) SGD.\n",
        "      if self._do_use_weight_decay(param_name):\n",
        "        update += self.weight_decay_rate * param\n",
        "\n",
        "      update_with_lr = self.learning_rate * update\n",
        "\n",
        "      next_param = param - update_with_lr\n",
        "\n",
        "      assignments.extend(\n",
        "          [param.assign(next_param),\n",
        "           m.assign(next_m),\n",
        "           v.assign(next_v)])\n",
        "    return tf.group(*assignments, name=name)\n",
        "\n",
        "  def _do_use_weight_decay(self, param_name):\n",
        "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "    if not self.weight_decay_rate:\n",
        "      return False\n",
        "    if self.exclude_from_weight_decay:\n",
        "      for r in self.exclude_from_weight_decay:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def _get_variable_name(self, param_name):\n",
        "    \"\"\"Get the variable name from the tensor name.\"\"\"\n",
        "    m = re.match(\"^(.*):\\\\d+$\", param_name)\n",
        "    if m is not None:\n",
        "      param_name = m.group(1)\n",
        "    return param_name\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQnuNLt-qyU1"
      },
      "source": [
        "#modelling.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnuxaOPWqw-C"
      },
      "source": [
        "# Original BERT model from https://github.com/google-research/bert/blob/master/modeling.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import numpy as np\n",
        "import six\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class BertConfig(object):\n",
        "  \"\"\"Configuration for `BertModel`.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               hidden_size=768,\n",
        "               num_hidden_layers=12,\n",
        "               num_attention_heads=12,\n",
        "               intermediate_size=3072,\n",
        "               hidden_act=\"gelu\",\n",
        "               hidden_dropout_prob=0.1,\n",
        "               attention_probs_dropout_prob=0.1,\n",
        "               max_position_embeddings=512,\n",
        "               type_vocab_size=16,\n",
        "               initializer_range=0.02):\n",
        "    \"\"\"Constructs BertConfig.\n",
        "\n",
        "    Args:\n",
        "      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n",
        "      hidden_size: Size of the encoder layers and the pooler layer.\n",
        "      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
        "      num_attention_heads: Number of attention heads for each attention layer in\n",
        "        the Transformer encoder.\n",
        "      intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
        "        layer in the Transformer encoder.\n",
        "      hidden_act: The non-linear activation function (function or string) in the\n",
        "        encoder and pooler.\n",
        "      hidden_dropout_prob: The dropout probability for all fully connected\n",
        "        layers in the embeddings, encoder, and pooler.\n",
        "      attention_probs_dropout_prob: The dropout ratio for the attention\n",
        "        probabilities.\n",
        "      max_position_embeddings: The maximum sequence length that this model might\n",
        "        ever be used with. Typically set this to something large just in case\n",
        "        (e.g., 512 or 1024 or 2048).\n",
        "      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
        "        `BertModel`.\n",
        "      initializer_range: The stdev of the truncated_normal_initializer for\n",
        "        initializing all weight matrices.\n",
        "    \"\"\"\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.hidden_act = hidden_act\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "    self.max_position_embeddings = max_position_embeddings\n",
        "    self.type_vocab_size = type_vocab_size\n",
        "    self.initializer_range = initializer_range\n",
        "\n",
        "  @classmethod\n",
        "  def from_dict(cls, json_object):\n",
        "    \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
        "    config = BertConfig(vocab_size=None)\n",
        "    for (key, value) in six.iteritems(json_object):\n",
        "      config.__dict__[key] = value\n",
        "    return config\n",
        "\n",
        "  @classmethod\n",
        "  def from_json_file(cls, json_file):\n",
        "    \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "    with tf.gfile.GFile(json_file, \"r\") as reader:\n",
        "      text = reader.read()\n",
        "    return cls.from_dict(json.loads(text))\n",
        "\n",
        "  def to_dict(self):\n",
        "    \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "    output = copy.deepcopy(self.__dict__)\n",
        "    return output\n",
        "\n",
        "  def to_json_string(self):\n",
        "    \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class BertModel(object):\n",
        "  \"\"\"BERT model (\"Bidirectional Encoder Representations from Transformers\").\n",
        "\n",
        "  Example usage:\n",
        "\n",
        "  ```python\n",
        "  # Already been converted into WordPiece token ids\n",
        "  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n",
        "  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n",
        "  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
        "    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "  model = modeling.BertModel(config=config, is_training=True,\n",
        "    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "  label_embeddings = tf.get_variable(...)\n",
        "  pooled_output = model.get_pooled_output()\n",
        "  logits = tf.matmul(pooled_output, label_embeddings)\n",
        "  ...\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               config,\n",
        "               is_training,\n",
        "               input_ids,\n",
        "               input_mask=None,\n",
        "               token_type_ids=None,\n",
        "               use_one_hot_embeddings=False,\n",
        "               scope=None):\n",
        "    \"\"\"Constructor for BertModel.\n",
        "\n",
        "    Args:\n",
        "      config: `BertConfig` instance.\n",
        "      is_training: bool. true for training model, false for eval model. Controls\n",
        "        whether dropout will be applied.\n",
        "      input_ids: int32 Tensor of shape [batch_size, seq_length].\n",
        "      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n",
        "        embeddings or tf.embedding_lookup() for the word embeddings.\n",
        "      scope: (optional) variable scope. Defaults to \"bert\".\n",
        "\n",
        "    Raises:\n",
        "      ValueError: The config is invalid or one of the input tensor shapes\n",
        "        is invalid.\n",
        "    \"\"\"\n",
        "    config = copy.deepcopy(config)\n",
        "    if not is_training:\n",
        "      config.hidden_dropout_prob = 0.0\n",
        "      config.attention_probs_dropout_prob = 0.0\n",
        "\n",
        "    input_shape = get_shape_list(input_ids, expected_rank=2)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_length = input_shape[1]\n",
        "\n",
        "    if input_mask is None:\n",
        "      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n",
        "\n",
        "    if token_type_ids is None:\n",
        "      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n",
        "\n",
        "    with tf.variable_scope(scope, default_name=\"bert\"):\n",
        "      with tf.variable_scope(\"embeddings\"):\n",
        "        # Perform embedding lookup on the word ids.\n",
        "        (self.embedding_output, self.embedding_table) = embedding_lookup(\n",
        "            input_ids=input_ids,\n",
        "            vocab_size=config.vocab_size,\n",
        "            embedding_size=config.hidden_size,\n",
        "            initializer_range=config.initializer_range,\n",
        "            word_embedding_name=\"word_embeddings\",\n",
        "            use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "        # Add positional embeddings and token type embeddings, then layer\n",
        "        # normalize and perform dropout.\n",
        "        self.embedding_output = embedding_postprocessor(\n",
        "            input_tensor=self.embedding_output,\n",
        "            use_token_type=True,\n",
        "            token_type_ids=token_type_ids,\n",
        "            token_type_vocab_size=config.type_vocab_size,\n",
        "            token_type_embedding_name=\"token_type_embeddings\",\n",
        "            use_position_embeddings=True,\n",
        "            position_embedding_name=\"position_embeddings\",\n",
        "            initializer_range=config.initializer_range,\n",
        "            max_position_embeddings=config.max_position_embeddings,\n",
        "            dropout_prob=config.hidden_dropout_prob)\n",
        "\n",
        "      with tf.variable_scope(\"encoder\"):\n",
        "        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n",
        "        # mask of shape [batch_size, seq_length, seq_length] which is used\n",
        "        # for the attention scores.\n",
        "        attention_mask = create_attention_mask_from_input_mask(\n",
        "            input_ids, input_mask)\n",
        "\n",
        "        # Run the stacked transformer.\n",
        "        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n",
        "        self.all_encoder_layers = transformer_model(\n",
        "            input_tensor=self.embedding_output,\n",
        "            attention_mask=attention_mask,\n",
        "            hidden_size=config.hidden_size,\n",
        "            num_hidden_layers=config.num_hidden_layers,\n",
        "            num_attention_heads=config.num_attention_heads,\n",
        "            intermediate_size=config.intermediate_size,\n",
        "            intermediate_act_fn=get_activation(config.hidden_act),\n",
        "            hidden_dropout_prob=config.hidden_dropout_prob,\n",
        "            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
        "            initializer_range=config.initializer_range,\n",
        "            do_return_all_layers=True)\n",
        "\n",
        "      self.sequence_output = self.all_encoder_layers[-1]\n",
        "      # The \"pooler\" converts the encoded sequence tensor of shape\n",
        "      # [batch_size, seq_length, hidden_size] to a tensor of shape\n",
        "      # [batch_size, hidden_size]. This is necessary for segment-level\n",
        "      # (or segment-pair-level) classification tasks where we need a fixed\n",
        "      # dimensional representation of the segment.\n",
        "      with tf.variable_scope(\"pooler\"):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token. We assume that this has been pre-trained\n",
        "        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n",
        "        self.pooled_output = tf.layers.dense(\n",
        "            first_token_tensor,\n",
        "            config.hidden_size,\n",
        "            activation=tf.tanh,\n",
        "            kernel_initializer=create_initializer(config.initializer_range))\n",
        "\n",
        "  def get_pooled_output(self):\n",
        "    return self.pooled_output\n",
        "\n",
        "  def get_sequence_output(self):\n",
        "    \"\"\"Gets final hidden layer of encoder.\n",
        "\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
        "      to the final hidden of the transformer encoder.\n",
        "    \"\"\"\n",
        "    return self.sequence_output\n",
        "\n",
        "  def get_all_encoder_layers(self):\n",
        "    return self.all_encoder_layers\n",
        "\n",
        "  def get_embedding_output(self):\n",
        "    \"\"\"Gets output of the embedding lookup (i.e., input to the transformer).\n",
        "\n",
        "    Returns:\n",
        "      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n",
        "      to the output of the embedding layer, after summing the word\n",
        "      embeddings with the positional embeddings and the token type embeddings,\n",
        "      then performing layer normalization. This is the input to the transformer.\n",
        "    \"\"\"\n",
        "    return self.embedding_output\n",
        "\n",
        "  def get_embedding_table(self):\n",
        "    return self.embedding_table\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "  \"\"\"Gaussian Error Linear Unit.\n",
        "\n",
        "  This is a smoother version of the RELU.\n",
        "  Original paper: https://arxiv.org/abs/1606.08415\n",
        "  Args:\n",
        "    x: float Tensor to perform activation.\n",
        "\n",
        "  Returns:\n",
        "    `x` with the GELU activation applied.\n",
        "  \"\"\"\n",
        "  cdf = 0.5 * (1.0 + tf.tanh(\n",
        "      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "  return x * cdf\n",
        "\n",
        "\n",
        "def get_activation(activation_string):\n",
        "  \"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n",
        "\n",
        "  Args:\n",
        "    activation_string: String name of the activation function.\n",
        "\n",
        "  Returns:\n",
        "    A Python function corresponding to the activation function. If\n",
        "    `activation_string` is None, empty, or \"linear\", this will return None.\n",
        "    If `activation_string` is not a string, it will return `activation_string`.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: The `activation_string` does not correspond to a known\n",
        "      activation.\n",
        "  \"\"\"\n",
        "\n",
        "  # We assume that anything that\"s not a string is already an activation\n",
        "  # function, so we just return it.\n",
        "  if not isinstance(activation_string, six.string_types):\n",
        "    return activation_string\n",
        "\n",
        "  if not activation_string:\n",
        "    return None\n",
        "\n",
        "  act = activation_string.lower()\n",
        "  if act == \"linear\":\n",
        "    return None\n",
        "  elif act == \"relu\":\n",
        "    return tf.nn.relu\n",
        "  elif act == \"gelu\":\n",
        "    return gelu\n",
        "  elif act == \"tanh\":\n",
        "    return tf.tanh\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported activation: %s\" % act)\n",
        "\n",
        "\n",
        "def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
        "  \"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"\n",
        "  assignment_map = {}\n",
        "  initialized_variable_names = {}\n",
        "\n",
        "  name_to_variable = collections.OrderedDict()\n",
        "  for var in tvars:\n",
        "    name = var.name\n",
        "    m = re.match(\"^(.*):\\\\d+$\", name)\n",
        "    if m is not None:\n",
        "      name = m.group(1)\n",
        "    name_to_variable[name] = var\n",
        "\n",
        "  init_vars = tf.train.list_variables(init_checkpoint)\n",
        "\n",
        "  assignment_map = collections.OrderedDict()\n",
        "  for x in init_vars:\n",
        "    (name, var) = (x[0], x[1])\n",
        "    if name not in name_to_variable:\n",
        "      continue\n",
        "    assignment_map[name] = name\n",
        "    initialized_variable_names[name] = 1\n",
        "    initialized_variable_names[name + \":0\"] = 1\n",
        "\n",
        "  return (assignment_map, initialized_variable_names)\n",
        "\n",
        "\n",
        "def dropout(input_tensor, dropout_prob):\n",
        "  \"\"\"Perform dropout.\n",
        "\n",
        "  Args:\n",
        "    input_tensor: float Tensor.\n",
        "    dropout_prob: Python float. The probability of dropping out a value (NOT of\n",
        "      *keeping* a dimension as in `tf.nn.dropout`).\n",
        "\n",
        "  Returns:\n",
        "    A version of `input_tensor` with dropout applied.\n",
        "  \"\"\"\n",
        "  if dropout_prob is None or dropout_prob == 0.0:\n",
        "    return input_tensor\n",
        "\n",
        "  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n",
        "  return output\n",
        "\n",
        "\n",
        "def layer_norm(input_tensor, name=None):\n",
        "  \"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"\n",
        "  return tf.contrib.layers.layer_norm(\n",
        "      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n",
        "\n",
        "\n",
        "def layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n",
        "  \"\"\"Runs layer normalization followed by dropout.\"\"\"\n",
        "  output_tensor = layer_norm(input_tensor, name)\n",
        "  output_tensor = dropout(output_tensor, dropout_prob)\n",
        "  return output_tensor\n",
        "\n",
        "\n",
        "def create_initializer(initializer_range=0.02):\n",
        "  \"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"\n",
        "  return tf.truncated_normal_initializer(stddev=initializer_range)\n",
        "\n",
        "\n",
        "def embedding_lookup(input_ids,\n",
        "                     vocab_size,\n",
        "                     embedding_size=128,\n",
        "                     initializer_range=0.02,\n",
        "                     word_embedding_name=\"word_embeddings\",\n",
        "                     use_one_hot_embeddings=False):\n",
        "  \"\"\"Looks up words embeddings for id tensor.\n",
        "\n",
        "  Args:\n",
        "    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n",
        "      ids.\n",
        "    vocab_size: int. Size of the embedding vocabulary.\n",
        "    embedding_size: int. Width of the word embeddings.\n",
        "    initializer_range: float. Embedding initialization range.\n",
        "    word_embedding_name: string. Name of the embedding table.\n",
        "    use_one_hot_embeddings: bool. If True, use one-hot method for word\n",
        "      embeddings. If False, use `tf.gather()`.\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, seq_length, embedding_size].\n",
        "  \"\"\"\n",
        "  # This function assumes that the input is of shape [batch_size, seq_length,\n",
        "  # num_inputs].\n",
        "  #\n",
        "  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n",
        "  # reshape to [batch_size, seq_length, 1].\n",
        "  if input_ids.shape.ndims == 2:\n",
        "    input_ids = tf.expand_dims(input_ids, axis=[-1])\n",
        "\n",
        "  embedding_table = tf.get_variable(\n",
        "      name=word_embedding_name,\n",
        "      shape=[vocab_size, embedding_size],\n",
        "      initializer=create_initializer(initializer_range))\n",
        "\n",
        "  flat_input_ids = tf.reshape(input_ids, [-1])\n",
        "  if use_one_hot_embeddings:\n",
        "    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n",
        "    output = tf.matmul(one_hot_input_ids, embedding_table)\n",
        "  else:\n",
        "    output = tf.gather(embedding_table, flat_input_ids)\n",
        "\n",
        "  input_shape = get_shape_list(input_ids)\n",
        "\n",
        "  output = tf.reshape(output,\n",
        "                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n",
        "  return (output, embedding_table)\n",
        "\n",
        "\n",
        "def embedding_postprocessor(input_tensor,\n",
        "                            use_token_type=False,\n",
        "                            token_type_ids=None,\n",
        "                            token_type_vocab_size=16,\n",
        "                            token_type_embedding_name=\"token_type_embeddings\",\n",
        "                            use_position_embeddings=True,\n",
        "                            position_embedding_name=\"position_embeddings\",\n",
        "                            initializer_range=0.02,\n",
        "                            max_position_embeddings=512,\n",
        "                            dropout_prob=0.1):\n",
        "  \"\"\"Performs various post-processing on a word embedding tensor.\n",
        "\n",
        "  Args:\n",
        "    input_tensor: float Tensor of shape [batch_size, seq_length,\n",
        "      embedding_size].\n",
        "    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n",
        "    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n",
        "      Must be specified if `use_token_type` is True.\n",
        "    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n",
        "    token_type_embedding_name: string. The name of the embedding table variable\n",
        "      for token type ids.\n",
        "    use_position_embeddings: bool. Whether to add position embeddings for the\n",
        "      position of each token in the sequence.\n",
        "    position_embedding_name: string. The name of the embedding table variable\n",
        "      for positional embeddings.\n",
        "    initializer_range: float. Range of the weight initialization.\n",
        "    max_position_embeddings: int. Maximum sequence length that might ever be\n",
        "      used with this model. This can be longer than the sequence length of\n",
        "      input_tensor, but cannot be shorter.\n",
        "    dropout_prob: float. Dropout probability applied to the final output tensor.\n",
        "\n",
        "  Returns:\n",
        "    float tensor with same shape as `input_tensor`.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: One of the tensor shapes or input values is invalid.\n",
        "  \"\"\"\n",
        "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  width = input_shape[2]\n",
        "\n",
        "  output = input_tensor\n",
        "\n",
        "  if use_token_type:\n",
        "    if token_type_ids is None:\n",
        "      raise ValueError(\"`token_type_ids` must be specified if\"\n",
        "                       \"`use_token_type` is True.\")\n",
        "    token_type_table = tf.get_variable(\n",
        "        name=token_type_embedding_name,\n",
        "        shape=[token_type_vocab_size, width],\n",
        "        initializer=create_initializer(initializer_range))\n",
        "    # This vocab will be small so we always do one-hot here, since it is always\n",
        "    # faster for a small vocabulary.\n",
        "    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n",
        "    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n",
        "    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n",
        "    token_type_embeddings = tf.reshape(token_type_embeddings,\n",
        "                                       [batch_size, seq_length, width])\n",
        "    output += token_type_embeddings\n",
        "\n",
        "  if use_position_embeddings:\n",
        "    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n",
        "    with tf.control_dependencies([assert_op]):\n",
        "      full_position_embeddings = tf.get_variable(\n",
        "          name=position_embedding_name,\n",
        "          shape=[max_position_embeddings, width],\n",
        "          initializer=create_initializer(initializer_range))\n",
        "      # Since the position embedding table is a learned variable, we create it\n",
        "      # using a (long) sequence length `max_position_embeddings`. The actual\n",
        "      # sequence length might be shorter than this, for faster training of\n",
        "      # tasks that do not have long sequences.\n",
        "      #\n",
        "      # So `full_position_embeddings` is effectively an embedding table\n",
        "      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n",
        "      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n",
        "      # perform a slice.\n",
        "      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n",
        "                                     [seq_length, -1])\n",
        "      num_dims = len(output.shape.as_list())\n",
        "\n",
        "      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n",
        "      # we broadcast among the first dimensions, which is typically just\n",
        "      # the batch size.\n",
        "      position_broadcast_shape = []\n",
        "      for _ in range(num_dims - 2):\n",
        "        position_broadcast_shape.append(1)\n",
        "      position_broadcast_shape.extend([seq_length, width])\n",
        "      position_embeddings = tf.reshape(position_embeddings,\n",
        "                                       position_broadcast_shape)\n",
        "      output += position_embeddings\n",
        "\n",
        "  output = layer_norm_and_dropout(output, dropout_prob)\n",
        "  return output\n",
        "\n",
        "\n",
        "def create_attention_mask_from_input_mask(from_tensor, to_mask):\n",
        "  \"\"\"Create 3D attention mask from a 2D tensor mask.\n",
        "\n",
        "  Args:\n",
        "    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n",
        "    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n",
        "  \"\"\"\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "  batch_size = from_shape[0]\n",
        "  from_seq_length = from_shape[1]\n",
        "\n",
        "  to_shape = get_shape_list(to_mask, expected_rank=2)\n",
        "  to_seq_length = to_shape[1]\n",
        "\n",
        "  to_mask = tf.cast(\n",
        "      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n",
        "\n",
        "  # We don't assume that `from_tensor` is a mask (although it could be). We\n",
        "  # don't actually care if we attend *from* padding tokens (only *to* padding)\n",
        "  # tokens so we create a tensor of all ones.\n",
        "  #\n",
        "  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n",
        "  broadcast_ones = tf.ones(\n",
        "      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n",
        "\n",
        "  # Here we broadcast along two dimensions to create the mask.\n",
        "  mask = broadcast_ones * to_mask\n",
        "\n",
        "  return mask\n",
        "\n",
        "\n",
        "def attention_layer(from_tensor,\n",
        "                    to_tensor,\n",
        "                    attention_mask=None,\n",
        "                    num_attention_heads=1,\n",
        "                    size_per_head=512,\n",
        "                    query_act=None,\n",
        "                    key_act=None,\n",
        "                    value_act=None,\n",
        "                    attention_probs_dropout_prob=0.0,\n",
        "                    initializer_range=0.02,\n",
        "                    do_return_2d_tensor=False,\n",
        "                    batch_size=None,\n",
        "                    from_seq_length=None,\n",
        "                    to_seq_length=None):\n",
        "  \"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n",
        "\n",
        "  This is an implementation of multi-headed attention based on \"Attention\n",
        "  is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n",
        "  this is self-attention. Each timestep in `from_tensor` attends to the\n",
        "  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n",
        "\n",
        "  This function first projects `from_tensor` into a \"query\" tensor and\n",
        "  `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n",
        "  of tensors of length `num_attention_heads`, where each tensor is of shape\n",
        "  [batch_size, seq_length, size_per_head].\n",
        "\n",
        "  Then, the query and key tensors are dot-producted and scaled. These are\n",
        "  softmaxed to obtain attention probabilities. The value tensors are then\n",
        "  interpolated by these probabilities, then concatenated back to a single\n",
        "  tensor and returned.\n",
        "\n",
        "  In practice, the multi-headed attention are done with transposes and\n",
        "  reshapes rather than actual separate tensors.\n",
        "\n",
        "  Args:\n",
        "    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n",
        "      from_width].\n",
        "    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n",
        "    attention_mask: (optional) int32 Tensor of shape [batch_size,\n",
        "      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n",
        "      attention scores will effectively be set to -infinity for any positions in\n",
        "      the mask that are 0, and will be unchanged for positions that are 1.\n",
        "    num_attention_heads: int. Number of attention heads.\n",
        "    size_per_head: int. Size of each attention head.\n",
        "    query_act: (optional) Activation function for the query transform.\n",
        "    key_act: (optional) Activation function for the key transform.\n",
        "    value_act: (optional) Activation function for the value transform.\n",
        "    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n",
        "      attention probabilities.\n",
        "    initializer_range: float. Range of the weight initializer.\n",
        "    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n",
        "      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n",
        "      output will be of shape [batch_size, from_seq_length, num_attention_heads\n",
        "      * size_per_head].\n",
        "    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n",
        "      of the 3D version of the `from_tensor` and `to_tensor`.\n",
        "    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
        "      of the 3D version of the `from_tensor`.\n",
        "    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n",
        "      of the 3D version of the `to_tensor`.\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, from_seq_length,\n",
        "      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n",
        "      true, this will be of shape [batch_size * from_seq_length,\n",
        "      num_attention_heads * size_per_head]).\n",
        "\n",
        "  Raises:\n",
        "    ValueError: Any of the arguments or tensor shapes are invalid.\n",
        "  \"\"\"\n",
        "\n",
        "  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n",
        "                           seq_length, width):\n",
        "    output_tensor = tf.reshape(\n",
        "        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n",
        "\n",
        "    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n",
        "    return output_tensor\n",
        "\n",
        "  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n",
        "  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n",
        "\n",
        "  if len(from_shape) != len(to_shape):\n",
        "    raise ValueError(\n",
        "        \"The rank of `from_tensor` must match the rank of `to_tensor`.\")\n",
        "\n",
        "  if len(from_shape) == 3:\n",
        "    batch_size = from_shape[0]\n",
        "    from_seq_length = from_shape[1]\n",
        "    to_seq_length = to_shape[1]\n",
        "  elif len(from_shape) == 2:\n",
        "    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n",
        "      raise ValueError(\n",
        "          \"When passing in rank 2 tensors to attention_layer, the values \"\n",
        "          \"for `batch_size`, `from_seq_length`, and `to_seq_length` \"\n",
        "          \"must all be specified.\")\n",
        "\n",
        "  # Scalar dimensions referenced here:\n",
        "  #   B = batch size (number of sequences)\n",
        "  #   F = `from_tensor` sequence length\n",
        "  #   T = `to_tensor` sequence length\n",
        "  #   N = `num_attention_heads`\n",
        "  #   H = `size_per_head`\n",
        "\n",
        "  from_tensor_2d = reshape_to_matrix(from_tensor)\n",
        "  to_tensor_2d = reshape_to_matrix(to_tensor)\n",
        "\n",
        "  # `query_layer` = [B*F, N*H]\n",
        "  query_layer = tf.layers.dense(\n",
        "      from_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=query_act,\n",
        "      name=\"query\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `key_layer` = [B*T, N*H]\n",
        "  key_layer = tf.layers.dense(\n",
        "      to_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=key_act,\n",
        "      name=\"key\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `value_layer` = [B*T, N*H]\n",
        "  value_layer = tf.layers.dense(\n",
        "      to_tensor_2d,\n",
        "      num_attention_heads * size_per_head,\n",
        "      activation=value_act,\n",
        "      name=\"value\",\n",
        "      kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "  # `query_layer` = [B, N, F, H]\n",
        "  query_layer = transpose_for_scores(query_layer, batch_size,\n",
        "                                     num_attention_heads, from_seq_length,\n",
        "                                     size_per_head)\n",
        "\n",
        "  # `key_layer` = [B, N, T, H]\n",
        "  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n",
        "                                   to_seq_length, size_per_head)\n",
        "\n",
        "  # Take the dot product between \"query\" and \"key\" to get the raw\n",
        "  # attention scores.\n",
        "  # `attention_scores` = [B, N, F, T]\n",
        "  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
        "  attention_scores = tf.multiply(attention_scores,\n",
        "                                 1.0 / math.sqrt(float(size_per_head)))\n",
        "\n",
        "  if attention_mask is not None:\n",
        "    # `attention_mask` = [B, 1, F, T]\n",
        "    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n",
        "\n",
        "    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "    # masked positions, this operation will create a tensor which is 0.0 for\n",
        "    # positions we want to attend and -10000.0 for masked positions.\n",
        "    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n",
        "\n",
        "    # Since we are adding it to the raw scores before the softmax, this is\n",
        "    # effectively the same as removing these entirely.\n",
        "    attention_scores += adder\n",
        "\n",
        "  # Normalize the attention scores to probabilities.\n",
        "  # `attention_probs` = [B, N, F, T]\n",
        "  attention_probs = tf.nn.softmax(attention_scores)\n",
        "\n",
        "  # This is actually dropping out entire tokens to attend to, which might\n",
        "  # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n",
        "\n",
        "  # `value_layer` = [B, T, N, H]\n",
        "  value_layer = tf.reshape(\n",
        "      value_layer,\n",
        "      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n",
        "\n",
        "  # `value_layer` = [B, N, T, H]\n",
        "  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n",
        "\n",
        "  # `context_layer` = [B, N, F, H]\n",
        "  context_layer = tf.matmul(attention_probs, value_layer)\n",
        "\n",
        "  # `context_layer` = [B, F, N, H]\n",
        "  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n",
        "\n",
        "  if do_return_2d_tensor:\n",
        "    # `context_layer` = [B*F, N*H]\n",
        "    context_layer = tf.reshape(\n",
        "        context_layer,\n",
        "        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n",
        "  else:\n",
        "    # `context_layer` = [B, F, N*H]\n",
        "    context_layer = tf.reshape(\n",
        "        context_layer,\n",
        "        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n",
        "\n",
        "  return context_layer\n",
        "\n",
        "\n",
        "def transformer_model(input_tensor,\n",
        "                      attention_mask=None,\n",
        "                      hidden_size=768,\n",
        "                      num_hidden_layers=12,\n",
        "                      num_attention_heads=12,\n",
        "                      intermediate_size=3072,\n",
        "                      intermediate_act_fn=gelu,\n",
        "                      hidden_dropout_prob=0.1,\n",
        "                      attention_probs_dropout_prob=0.1,\n",
        "                      initializer_range=0.02,\n",
        "                      do_return_all_layers=False):\n",
        "  \"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n",
        "\n",
        "  This is almost an exact implementation of the original Transformer encoder.\n",
        "\n",
        "  See the original paper:\n",
        "  https://arxiv.org/abs/1706.03762\n",
        "\n",
        "  Also see:\n",
        "  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n",
        "\n",
        "  Args:\n",
        "    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n",
        "    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n",
        "      seq_length], with 1 for positions that can be attended to and 0 in\n",
        "      positions that should not be.\n",
        "    hidden_size: int. Hidden size of the Transformer.\n",
        "    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n",
        "    num_attention_heads: int. Number of attention heads in the Transformer.\n",
        "    intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed\n",
        "      forward) layer.\n",
        "    intermediate_act_fn: function. The non-linear activation function to apply\n",
        "      to the output of the intermediate/feed-forward layer.\n",
        "    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n",
        "    attention_probs_dropout_prob: float. Dropout probability of the attention\n",
        "      probabilities.\n",
        "    initializer_range: float. Range of the initializer (stddev of truncated\n",
        "      normal).\n",
        "    do_return_all_layers: Whether to also return all layers or just the final\n",
        "      layer.\n",
        "\n",
        "  Returns:\n",
        "    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n",
        "    hidden layer of the Transformer.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: A Tensor shape or parameter is invalid.\n",
        "  \"\"\"\n",
        "  if hidden_size % num_attention_heads != 0:\n",
        "    raise ValueError(\n",
        "        \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "        \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
        "\n",
        "  attention_head_size = int(hidden_size / num_attention_heads)\n",
        "  input_shape = get_shape_list(input_tensor, expected_rank=3)\n",
        "  batch_size = input_shape[0]\n",
        "  seq_length = input_shape[1]\n",
        "  input_width = input_shape[2]\n",
        "\n",
        "  # The Transformer performs sum residuals on all layers so the input needs\n",
        "  # to be the same as the hidden size.\n",
        "  if input_width != hidden_size:\n",
        "    raise ValueError(\"The width of the input tensor (%d) != hidden size (%d)\" %\n",
        "                     (input_width, hidden_size))\n",
        "\n",
        "  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n",
        "  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n",
        "  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n",
        "  # help the optimizer.\n",
        "  prev_output = reshape_to_matrix(input_tensor)\n",
        "\n",
        "  all_layer_outputs = []\n",
        "  for layer_idx in range(num_hidden_layers):\n",
        "    with tf.variable_scope(\"layer_%d\" % layer_idx):\n",
        "      layer_input = prev_output\n",
        "\n",
        "      with tf.variable_scope(\"attention\"):\n",
        "        attention_heads = []\n",
        "        with tf.variable_scope(\"self\"):\n",
        "          attention_head = attention_layer(\n",
        "              from_tensor=layer_input,\n",
        "              to_tensor=layer_input,\n",
        "              attention_mask=attention_mask,\n",
        "              num_attention_heads=num_attention_heads,\n",
        "              size_per_head=attention_head_size,\n",
        "              attention_probs_dropout_prob=attention_probs_dropout_prob,\n",
        "              initializer_range=initializer_range,\n",
        "              do_return_2d_tensor=True,\n",
        "              batch_size=batch_size,\n",
        "              from_seq_length=seq_length,\n",
        "              to_seq_length=seq_length)\n",
        "          attention_heads.append(attention_head)\n",
        "\n",
        "        attention_output = None\n",
        "        if len(attention_heads) == 1:\n",
        "          attention_output = attention_heads[0]\n",
        "        else:\n",
        "          # In the case where we have other sequences, we just concatenate\n",
        "          # them to the self-attention head before the projection.\n",
        "          attention_output = tf.concat(attention_heads, axis=-1)\n",
        "\n",
        "        # Run a linear projection of `hidden_size` then add a residual\n",
        "        # with `layer_input`.\n",
        "        with tf.variable_scope(\"output\"):\n",
        "          attention_output = tf.layers.dense(\n",
        "              attention_output,\n",
        "              hidden_size,\n",
        "              kernel_initializer=create_initializer(initializer_range))\n",
        "          attention_output = dropout(attention_output, hidden_dropout_prob)\n",
        "          attention_output = layer_norm(attention_output + layer_input)\n",
        "\n",
        "      # The activation is only applied to the \"intermediate\" hidden layer.\n",
        "      with tf.variable_scope(\"intermediate\"):\n",
        "        intermediate_output = tf.layers.dense(\n",
        "            attention_output,\n",
        "            intermediate_size,\n",
        "            activation=intermediate_act_fn,\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "\n",
        "      # Down-project back to `hidden_size` then add the residual.\n",
        "      with tf.variable_scope(\"output\"):\n",
        "        layer_output = tf.layers.dense(\n",
        "            intermediate_output,\n",
        "            hidden_size,\n",
        "            kernel_initializer=create_initializer(initializer_range))\n",
        "        layer_output = dropout(layer_output, hidden_dropout_prob)\n",
        "        layer_output = layer_norm(layer_output + attention_output)\n",
        "        prev_output = layer_output\n",
        "        all_layer_outputs.append(layer_output)\n",
        "\n",
        "  if do_return_all_layers:\n",
        "    final_outputs = []\n",
        "    for layer_output in all_layer_outputs:\n",
        "      final_output = reshape_from_matrix(layer_output, input_shape)\n",
        "      final_outputs.append(final_output)\n",
        "    return final_outputs\n",
        "  else:\n",
        "    final_output = reshape_from_matrix(prev_output, input_shape)\n",
        "    return final_output\n",
        "\n",
        "\n",
        "def get_shape_list(tensor, expected_rank=None, name=None):\n",
        "  \"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n",
        "\n",
        "  Args:\n",
        "    tensor: A tf.Tensor object to find the shape of.\n",
        "    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n",
        "      specified and the `tensor` has a different rank, and exception will be\n",
        "      thrown.\n",
        "    name: Optional name of the tensor for the error message.\n",
        "\n",
        "  Returns:\n",
        "    A list of dimensions of the shape of tensor. All static dimensions will\n",
        "    be returned as python integers, and dynamic dimensions will be returned\n",
        "    as tf.Tensor scalars.\n",
        "  \"\"\"\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  if expected_rank is not None:\n",
        "    assert_rank(tensor, expected_rank, name)\n",
        "\n",
        "  shape = tensor.shape.as_list()\n",
        "\n",
        "  non_static_indexes = []\n",
        "  for (index, dim) in enumerate(shape):\n",
        "    if dim is None:\n",
        "      non_static_indexes.append(index)\n",
        "\n",
        "  if not non_static_indexes:\n",
        "    return shape\n",
        "\n",
        "  dyn_shape = tf.shape(tensor)\n",
        "  for index in non_static_indexes:\n",
        "    shape[index] = dyn_shape[index]\n",
        "  return shape\n",
        "\n",
        "\n",
        "def reshape_to_matrix(input_tensor):\n",
        "  \"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"\n",
        "  ndims = input_tensor.shape.ndims\n",
        "  if ndims < 2:\n",
        "    raise ValueError(\"Input tensor must have at least rank 2. Shape = %s\" %\n",
        "                     (input_tensor.shape))\n",
        "  if ndims == 2:\n",
        "    return input_tensor\n",
        "\n",
        "  width = input_tensor.shape[-1]\n",
        "  output_tensor = tf.reshape(input_tensor, [-1, width])\n",
        "  return output_tensor\n",
        "\n",
        "\n",
        "def reshape_from_matrix(output_tensor, orig_shape_list):\n",
        "  \"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"\n",
        "  if len(orig_shape_list) == 2:\n",
        "    return output_tensor\n",
        "\n",
        "  output_shape = get_shape_list(output_tensor)\n",
        "\n",
        "  orig_dims = orig_shape_list[0:-1]\n",
        "  width = output_shape[-1]\n",
        "\n",
        "  return tf.reshape(output_tensor, orig_dims + [width])\n",
        "\n",
        "\n",
        "def assert_rank(tensor, expected_rank, name=None):\n",
        "  \"\"\"Raises an exception if the tensor rank is not of the expected rank.\n",
        "\n",
        "  Args:\n",
        "    tensor: A tf.Tensor to check the rank of.\n",
        "    expected_rank: Python integer or list of integers, expected rank.\n",
        "    name: Optional name of the tensor for the error message.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If the expected shape doesn't match the actual shape.\n",
        "  \"\"\"\n",
        "  if name is None:\n",
        "    name = tensor.name\n",
        "\n",
        "  expected_rank_dict = {}\n",
        "  if isinstance(expected_rank, six.integer_types):\n",
        "    expected_rank_dict[expected_rank] = True\n",
        "  else:\n",
        "    for x in expected_rank:\n",
        "      expected_rank_dict[x] = True\n",
        "\n",
        "  actual_rank = tensor.shape.ndims\n",
        "  if actual_rank not in expected_rank_dict:\n",
        "    scope_name = tf.get_variable_scope().name\n",
        "    raise ValueError(\n",
        "        \"For the tensor `%s` in scope `%s`, the actual rank \"\n",
        "        \"`%d` (shape = %s) is not equal to the expected rank `%s`\" %\n",
        "        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CdlEnZkqmxg"
      },
      "source": [
        "# data_processors.py - data processor for qc dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szjnNOHgqlAK"
      },
      "source": [
        "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
        "# Copyright Tor Vergata, University of Rome. All Rights Reserved.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "#\n",
        "# Data processor for the QC dataset\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "  def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "    \"\"\"Constructs a InputExample.\n",
        "\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "    self.guid = guid\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b\n",
        "    self.label = label\n",
        "\n",
        "\n",
        "class PaddingInputExample(object):\n",
        "  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_ids,\n",
        "               input_mask,\n",
        "               segment_ids,\n",
        "               label_id,\n",
        "               label_mask=None,\n",
        "               is_real_example=True):\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.label_id = label_id\n",
        "    self.is_real_example = is_real_example\n",
        "    self.label_mask = label_mask\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "  \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "  def get_labeled_examples(self, data_dir):\n",
        "    \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_unlabeled_examples(self, data_dir):\n",
        "    \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_test_examples(self, data_dir):\n",
        "    \"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def get_labels(self):\n",
        "    \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  @classmethod\n",
        "  def _read_tsv(cls, input_file, quotechar=None):\n",
        "    \"\"\"Reads a tab separated value file.\"\"\"\n",
        "    with tf.gfile.Open(input_file, \"r\") as f:\n",
        "      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "      lines = []\n",
        "      for line in reader:\n",
        "        lines.append(line)\n",
        "      return lines\n",
        "\n",
        "class QcFineProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the MultiNLI data set (GLUE version).\"\"\"\n",
        "\n",
        "    def get_labeled_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(os.path.join(data_dir, \"labeled.tsv\"), \"train\")\n",
        "\n",
        "    def get_unlabeled_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(os.path.join(data_dir, \"unlabeled.tsv\"), \"train\")\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(os.path.join(data_dir, \"test.tsv\"), \"test\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"UNK_UNK\", \"ABBR_abb\", \"ABBR_exp\", \"DESC_def\", \"DESC_desc\", \"DESC_manner\", \"DESC_reason\", \"ENTY_animal\", \"ENTY_body\", \"ENTY_color\", \"ENTY_cremat\", \"ENTY_currency\", \"ENTY_dismed\", \"ENTY_event\", \"ENTY_food\", \"ENTY_instru\", \"ENTY_lang\", \"ENTY_letter\", \"ENTY_other\", \"ENTY_plant\", \"ENTY_product\", \"ENTY_religion\", \"ENTY_sport\", \"ENTY_substance\", \"ENTY_symbol\", \"ENTY_techmeth\", \"ENTY_termeq\", \"ENTY_veh\", \"ENTY_word\", \"HUM_desc\", \"HUM_gr\", \"HUM_ind\", \"HUM_title\", \"LOC_city\", \"LOC_country\", \"LOC_mount\", \"LOC_other\", \"LOC_state\", \"NUM_code\", \"NUM_count\", \"NUM_date\", \"NUM_dist\", \"NUM_money\", \"NUM_ord\", \"NUM_other\", \"NUM_perc\", \"NUM_period\", \"NUM_speed\", \"NUM_temp\", \"NUM_volsize\", \"NUM_weight\"]\n",
        "\n",
        "    def _create_examples(self, input_file, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "\n",
        "        with open(input_file, 'r') as f:\n",
        "            contents = f.read()\n",
        "            file_as_list = contents.splitlines()\n",
        "            for line in file_as_list[1:]:\n",
        "                split = line.split(\" \")\n",
        "                question = ' '.join(split[1:])\n",
        "\n",
        "                guid = \"%s-%s\" % (set_type, convert_to_unicode(line))\n",
        "                text_a = convert_to_unicode(question)\n",
        "                inn_split = split[0].split(\":\")\n",
        "                label = inn_split[0] + \"_\" + inn_split[1]\n",
        "                examples.append(InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "            f.close()\n",
        "\n",
        "        return examples\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBVQXnMqaU-"
      },
      "source": [
        "#ganbert.py - (in parts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9tKeskel1dm",
        "outputId": "583c8b97-52e4-4697-d167-06512ab37319"
      },
      "source": [
        "import tensorflow as tf; tf.contrib.summary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'tensorflow.contrib.summary.summary' from '/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/summary/summary.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dcbrLPlKvaPE",
        "outputId": "426321aa-8936-45c5-f708-980b3b323fe9"
      },
      "source": [
        "!pip install git+https://github.com/guillaumegenthial/tf_metrics.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/guillaumegenthial/tf_metrics.git\n",
            "  Cloning https://github.com/guillaumegenthial/tf_metrics.git to /tmp/pip-req-build-_2n6domt\n",
            "  Running command git clone -q https://github.com/guillaumegenthial/tf_metrics.git /tmp/pip-req-build-_2n6domt\n",
            "Requirement already satisfied (use --upgrade to upgrade): tf-metrics==0.0.1 from git+https://github.com/guillaumegenthial/tf_metrics.git in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tf-metrics==0.0.1) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-gpu>=1.6 in /usr/local/lib/python3.7/dist-packages (from tf-metrics==0.0.1) (2.4.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.12)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.3.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.36.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.6.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.7.4.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.12.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.12.1)\n",
            "Collecting tensorboard~=2.4\n",
            "  Using cached https://files.pythonhosted.org/packages/64/21/eebd23060763fedeefb78bc2b286e00fa1d8abda6f70efa2ee08c28af0d4/tensorboard-2.4.1-py3-none-any.whl\n",
            "Collecting gast==0.3.3\n",
            "  Using cached https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.12.4)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
            "  Using cached https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.1.2)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (2.10.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.32.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.28.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.4.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (54.2.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (4.7.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu>=1.6->tf-metrics==0.0.1) (3.1.0)\n",
            "Building wheels for collected packages: tf-metrics\n",
            "  Building wheel for tf-metrics (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tf-metrics: filename=tf_metrics-0.0.1-cp37-none-any.whl size=7692 sha256=1329129f93ac375cf0c40bf55171dea1c4211af1ac45a37a1945e56fac92ec2b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gc046pap/wheels/da/6c/c8/663ef339a0666590dc53bd13bab86643a1f9c35b26742d7876\n",
            "Successfully built tf-metrics\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, gast, tensorflow-estimator\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: gast 0.2.2\n",
            "    Uninstalling gast-0.2.2:\n",
            "      Successfully uninstalled gast-0.2.2\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "Successfully installed gast-0.3.3 tensorboard-2.4.1 tensorflow-estimator-2.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNL7DljnqEFD",
        "outputId": "350beff9-b5e0-4d13-b5ae-cbd4ba5eb6ee"
      },
      "source": [
        "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
        "# Copyright Tor Vergata, University of Rome. All Rights Reserved.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "#\n",
        "# Here is defined the GAN-BERT model, starting from the run_classifier.py https://github.com/google-research/bert/blob/master/run_classifier.py\n",
        "#\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import csv\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import tf_metrics\n",
        "\n",
        "\n",
        "BERT_BASE_DIR='cased_L-12_H-768_A-12'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "SEQ_LEN=\"64\"\n",
        "BS=\"64\"\n",
        "LR=\"2e-5\"\n",
        "EPOCHS=\"3\"\n",
        "cur_dir=\"data\"\n",
        "LABEL_RATE=\"0.02\"\n",
        "\n",
        "# task_name='QC-fine'\n",
        "# label_rate= '𝐿𝐴𝐵𝐸𝐿𝑅𝐴𝑇𝐸−−𝑑𝑜𝑡𝑟𝑎𝑖𝑛=𝑡𝑟𝑢𝑒−−𝑑𝑜𝑒𝑣𝑎𝑙=𝑡𝑟𝑢𝑒−−𝑑𝑜𝑝𝑟𝑒𝑑𝑖𝑐𝑡=𝑓𝑎𝑙𝑠𝑒−−𝑑𝑎𝑡𝑎𝑑𝑖𝑟= data' #'𝐿𝐴𝐵𝐸𝐿𝑅𝐴𝑇𝐸−−𝑑𝑜𝑡𝑟𝑎𝑖𝑛=𝑡𝑟𝑢𝑒−−𝑑𝑜𝑒𝑣𝑎𝑙=𝑡𝑟𝑢𝑒−−𝑑𝑜𝑝𝑟𝑒𝑑𝑖𝑐𝑡=𝑓𝑎𝑙𝑠𝑒−−𝑑𝑎𝑡𝑎𝑑𝑖𝑟= {cur_dir}'\n",
        "# vocab_file= '𝐵𝐸𝑅𝑇𝐵𝐴𝑆𝐸𝐷𝐼𝑅/𝑣𝑜𝑐𝑎𝑏.𝑡𝑥𝑡−−𝑏𝑒𝑟𝑡𝑐𝑜𝑛𝑓𝑖𝑔𝑓𝑖𝑙𝑒= cased_L-12_H-768_A-12/bert_config.json' #'𝐵𝐸𝑅𝑇𝐵𝐴𝑆𝐸𝐷𝐼𝑅/𝑣𝑜𝑐𝑎𝑏.𝑡𝑥𝑡−−𝑏𝑒𝑟𝑡𝑐𝑜𝑛𝑓𝑖𝑔𝑓𝑖𝑙𝑒= BERT_BASE_DIR/bert_config.json'\n",
        "# init_checkpoint= '𝐵𝐸𝑅𝑇𝐵𝐴𝑆𝐸𝐷𝐼𝑅/𝑏𝑒𝑟𝑡𝑚𝑜𝑑𝑒𝑙.𝑐𝑘𝑝𝑡−−𝑚𝑎𝑥𝑠𝑒𝑞𝑙𝑒𝑛𝑔𝑡ℎ= 64' #'𝐵𝐸𝑅𝑇𝐵𝐴𝑆𝐸𝐷𝐼𝑅/𝑏𝑒𝑟𝑡𝑚𝑜𝑑𝑒𝑙.𝑐𝑘𝑝𝑡−−𝑚𝑎𝑥𝑠𝑒𝑞𝑙𝑒𝑛𝑔𝑡ℎ= {SEQ_LEN}'\n",
        "# train_batch_size='𝐵𝑆−−𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔𝑟𝑎𝑡𝑒= 0' #0.00002 #2e-5 #'𝐵𝑆−−𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔𝑟𝑎𝑡𝑒= {LR}'\n",
        "# num_train_epochs=3 #'${EPOCHS}'\n",
        "# warmup_proportion=0.1\n",
        "# do_lower_case=False\n",
        "# output_dir='ganbert_output_model'\n",
        "\n",
        "task_name='QC-fine'\n",
        "label_rate=f'{LABEL_RATE}' \n",
        "do_train=True\n",
        "do_eval=True\n",
        "do_predict=False\n",
        "data_dir=f'{cur_dir}'\n",
        "vocab_file=f'{BERT_BASE_DIR}/vocab.txt'\n",
        "bert_config_file=f'{BERT_BASE_DIR}/bert_config.json'\n",
        "init_checkpoint=f'{BERT_BASE_DIR}/bert_model.ckpt'\n",
        "max_seq_length=f'{SEQ_LEN}'\n",
        "train_batch_size=f'{BS}'\n",
        "learning_rate=f'{LR}'\n",
        "num_train_epochs=f'{EPOCHS}'\n",
        "warmup_proportion=0.1\n",
        "do_lower_case=False\n",
        "output_dir='ganbert_output_model'\n",
        "\n",
        "\n",
        "flags = tf.compat.v1.flags\n",
        "flags.DEFINE_string('f','','')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"unlabeled_multiplier\", 100,\n",
        "    \"The multiplier to compute the max number of unlabeled examples with respect to the labeled examples.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"data_dir\", data_dir,\n",
        "    \"The input data dir. Should contain the .tsv files (or other data files) \"\n",
        "    \"for the task.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"bert_config_file\", bert_config_file,\n",
        "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
        "    \"This specifies the model architecture.\")\n",
        "\n",
        "flags.DEFINE_string(\"task_name\", task_name, \"The name of the task to train.\")\n",
        "\n",
        "flags.DEFINE_string(\"vocab_file\", vocab_file,\n",
        "                    \"The vocabulary file that the BERT model was trained on.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"output_dir\", output_dir,\n",
        "    \"The output directory where the model checkpoints will be written.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"init_checkpoint\", init_checkpoint,\n",
        "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"do_lower_case\", do_lower_case,\n",
        "    \"Whether to lower case the input text. Should be True for uncased \"\n",
        "    \"models and False for cased models.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"max_seq_length\", 128,\n",
        "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
        "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
        "    \"than this will be padded.\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_train\", do_train, \"Whether to run training.\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_eval\", do_eval, \"Whether to run eval on the dev set.\")\n",
        "\n",
        "flags.DEFINE_bool(\n",
        "    \"do_predict\", do_predict,\n",
        "    \"Whether to run the model in inference mode on the test set.\")\n",
        "\n",
        "flags.DEFINE_integer(\"train_batch_size\", train_batch_size, \"Total batch size for training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"eval_batch_size\", 8, \"Total batch size for eval.\")\n",
        "\n",
        "flags.DEFINE_integer(\"predict_batch_size\", 8, \"Total batch size for predict.\")\n",
        "\n",
        "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
        "\n",
        "flags.DEFINE_float(\"num_train_epochs\", num_train_epochs,\n",
        "                   \"Total number of training epochs to perform.\")\n",
        "\n",
        "flags.DEFINE_float(\n",
        "    \"warmup_proportion\", warmup_proportion,\n",
        "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
        "    \"E.g., 0.1 = 10% of training.\")\n",
        "\n",
        "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
        "                     \"How often to save the model checkpoint.\")\n",
        "\n",
        "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
        "                     \"How many steps to make in each estimator call.\")\n",
        "\n",
        "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
        "\n",
        "#tpu_name = os.environ['TPU_NAME']\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"tpu_name\", None,\n",
        "    \"The Cloud TPU to use for training. This should be either the name \"\n",
        "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
        "    \"url.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"tpu_zone\", None,\n",
        "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "flags.DEFINE_string(\n",
        "    \"gcp_project\", None,\n",
        "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
        "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
        "    \"metadata.\")\n",
        "\n",
        "flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
        "\n",
        "flags.DEFINE_integer(\n",
        "    \"num_tpu_cores\", 8,\n",
        "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n",
        "\n",
        "flags.DEFINE_float(\"label_rate\", label_rate,\n",
        "                   \"Rate for labeled examples (Used only for logging purpose).\")\n",
        "\n",
        "flags.DEFINE_float(\"dropout_keep_rate\", 0.9,\n",
        "                   \"Keep rate for dropout.\")\n",
        "\n",
        "epsilon = 1e-8\n",
        "DKP = FLAGS.dropout_keep_rate\n",
        "LATENT_Z = 100\n",
        "\n",
        "SEED = 0\n",
        "np.random.seed(SEED)\n",
        "tf.compat.v1.set_random_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "\n",
        "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
        "                           tokenizer, label_mask):\n",
        "  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "  if isinstance(example, PaddingInputExample):\n",
        "    return InputFeatures(\n",
        "        input_ids=[0] * max_seq_length,\n",
        "        input_mask=[0] * max_seq_length,\n",
        "        segment_ids=[0] * max_seq_length,\n",
        "        label_id=0,\n",
        "        label_mask=label_mask,\n",
        "        is_real_example=False)\n",
        "\n",
        "  label_map = {}\n",
        "  for (i, label) in enumerate(label_list):\n",
        "    label_map[label] = i\n",
        "\n",
        "  tokens_a = tokenizer.tokenize(example.text_a)\n",
        "  tokens_b = None\n",
        "  if example.text_b:\n",
        "    tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "  if tokens_b:\n",
        "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "    # length is less than the specified length.\n",
        "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "  else:\n",
        "    # Account for [CLS] and [SEP] with \"- 2\"\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "      tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "\n",
        "  # The convention in BERT is:\n",
        "  # (a) For sequence pairs:\n",
        "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "  # (b) For single sequences:\n",
        "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "  #  type_ids: 0     0   0   0  0     0 0\n",
        "  #\n",
        "  # Where \"type_ids\" are used to indicate whether this is the first\n",
        "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "  # embedding vector (and position vector). This is not *strictly* necessary\n",
        "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "  # it easier for the model to learn the concept of sequences.\n",
        "  #\n",
        "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "  # used as the \"sentence vector\". Note that this only makes sense because\n",
        "  # the entire model is fine-tuned.\n",
        "  tokens = []\n",
        "  segment_ids = []\n",
        "  tokens.append(\"[CLS]\")\n",
        "  segment_ids.append(0)\n",
        "  for token in tokens_a:\n",
        "    tokens.append(token)\n",
        "    segment_ids.append(0)\n",
        "  tokens.append(\"[SEP]\")\n",
        "  segment_ids.append(0)\n",
        "\n",
        "  if tokens_b:\n",
        "    for token in tokens_b:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(1)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(1)\n",
        "\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "  # tokens are attended to.\n",
        "  input_mask = [1] * len(input_ids)\n",
        "\n",
        "  # Zero-pad up to the sequence length.\n",
        "  while len(input_ids) < max_seq_length:\n",
        "    input_ids.append(0)\n",
        "    input_mask.append(0)\n",
        "    segment_ids.append(0)\n",
        "\n",
        "  assert len(input_ids) == max_seq_length\n",
        "  assert len(input_mask) == max_seq_length\n",
        "  assert len(segment_ids) == max_seq_length\n",
        "\n",
        "  label_id = label_map[example.label]\n",
        "  if ex_index < 5:\n",
        "    tf.logging.info(\"*** Example ***\")\n",
        "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
        "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "        [printable_text(x) for x in tokens]))\n",
        "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "\n",
        "  feature = InputFeatures(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids,\n",
        "      label_id=label_id,\n",
        "      label_mask=label_mask,\n",
        "      is_real_example=True)\n",
        "  return feature\n",
        "\n",
        "\n",
        "def file_based_convert_examples_to_features(\n",
        "    labeled_examples, unlabeled_examples, label_list, max_seq_length, tokenizer, output_file, label_mask_rate, is_testing=False):\n",
        "  \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
        "  all_examples = labeled_examples\n",
        "  if unlabeled_examples:\n",
        "    all_examples = all_examples + unlabeled_examples\n",
        "  label_masks = get_labeled_mask(mask_size=len(all_examples), labeled_size=len(labeled_examples))\n",
        "\n",
        "  to_write_examples = list()\n",
        "  for ex_index, example in enumerate(all_examples):\n",
        "    if ex_index % 10000 == 0:\n",
        "      tf.logging.info(\"Writing example %d\" % ex_index)\n",
        "    feature = convert_single_example(ex_index, example, label_list,\n",
        "                                     max_seq_length, tokenizer, label_masks[ex_index])\n",
        "\n",
        "    def create_int_feature(values):\n",
        "      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
        "      return f\n",
        "\n",
        "    features = collections.OrderedDict()\n",
        "    features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
        "    features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
        "    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
        "    features[\"label_ids\"] = create_int_feature([feature.label_id])\n",
        "    features[\"label_mask\"] = create_int_feature([feature.label_mask])\n",
        "    features[\"is_real_example\"] = create_int_feature(\n",
        "        [int(feature.is_real_example)])\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "\n",
        "    if label_mask_rate == 1:\n",
        "        to_write_examples.append(tf_example)\n",
        "    else:\n",
        "        # IT SIMULATE A LABELED EXAMPLE\n",
        "        if feature.label_mask:\n",
        "            balance = int(1/label_mask_rate)\n",
        "            balance = int(math.log(balance,2))\n",
        "            if balance < 1:\n",
        "                balance = 1\n",
        "            for b in range(0, int(balance)):\n",
        "                to_write_examples.append(tf_example)\n",
        "        else:\n",
        "          to_write_examples.append(tf_example)\n",
        "\n",
        "  writer = tf.python_io.TFRecordWriter(output_file)\n",
        "  written_examples = 0\n",
        "  if not is_testing:\n",
        "    random.shuffle(to_write_examples)\n",
        "  for tf_example in to_write_examples:\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "    written_examples = written_examples + 1\n",
        "  writer.close()\n",
        "\n",
        "  return written_examples\n",
        "\n",
        "\n",
        "def file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  name_to_features = {\n",
        "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"label_ids\": tf.FixedLenFeature([], tf.int64),\n",
        "      \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
        "      \"label_mask\": tf.FixedLenFeature([], tf.int64),\n",
        "  }\n",
        "\n",
        "  def _decode_record(record, name_to_features):\n",
        "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "    example = tf.parse_single_example(record, name_to_features)\n",
        "\n",
        "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "    # So cast all int64 to int32.\n",
        "    for name in list(example.keys()):\n",
        "      t = example[name]\n",
        "      if t.dtype == tf.int64:\n",
        "        t = tf.to_int32(t)\n",
        "      example[name] = t\n",
        "\n",
        "    return example\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    if is_training:\n",
        "        batch_size = FLAGS.train_batch_size\n",
        "    else:\n",
        "        batch_size = params[\"batch_size\"]\n",
        "\n",
        "    # For training, we want a lot of parallel reading and shuffling.\n",
        "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "    d = tf.data.TFRecordDataset(input_file)\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=10000, seed=SEED)\n",
        "\n",
        "    d = d.apply(\n",
        "        tf.contrib.data.map_and_batch(\n",
        "            lambda record: _decode_record(record, name_to_features),\n",
        "            batch_size=batch_size,\n",
        "            drop_remainder=drop_remainder))\n",
        "\n",
        "    return d\n",
        "\n",
        "  return input_fn\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "  # This is a simple heuristic which will always truncate the longer sequence\n",
        "  # one token at a time. This makes more sense than truncating an equal percent\n",
        "  # of tokens from each, since if one sequence is very short then each token\n",
        "  # that's truncated likely contains more information than a longer sequence.\n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b)\n",
        "    if total_length <= max_length:\n",
        "      break\n",
        "    if len(tokens_a) > len(tokens_b):\n",
        "      tokens_a.pop()\n",
        "    else:\n",
        "      tokens_b.pop()\n",
        "\n",
        "\n",
        "############ Defining Discriminator ############\n",
        "def discriminator(x, d_hidden_size, dkp, is_training, num_labels, num_hidden_discriminator = 1, reuse = False):\n",
        "    with tf.compat.v1.variable_scope('Discriminator', reuse = reuse):\n",
        "        layer_hidden = tf.nn.dropout(x, keep_prob=dkp)\n",
        "        for i in range(num_hidden_discriminator):\n",
        "            layer_hidden = tf.layers.dense(layer_hidden, d_hidden_size)\n",
        "            layer_hidden = tf.nn.leaky_relu(layer_hidden)\n",
        "            layer_hidden = tf.nn.dropout(layer_hidden, keep_prob=dkp)\n",
        "        flatten5 = layer_hidden\n",
        "\n",
        "        logit = tf.layers.dense(layer_hidden, (num_labels + 1))\n",
        "        prob = tf.nn.softmax(logit)\n",
        "    return flatten5, logit, prob\n",
        "\n",
        "\n",
        "############ Defining Generator ############\n",
        "def generator(z, g_hidden_size, dkp, is_training, num_hidden_generator = 1, reuse = False):\n",
        "    with tf.compat.v1.variable_scope('Generator', reuse = reuse):\n",
        "        layer_hidden = z\n",
        "\n",
        "        for i in range(num_hidden_generator):\n",
        "            layer_hidden = tf.layers.dense(layer_hidden, g_hidden_size)\n",
        "            layer_hidden = tf.nn.leaky_relu(layer_hidden)\n",
        "            layer_hidden = tf.nn.dropout(layer_hidden, rate = 1 - dkp)\n",
        "        layer_hidden = tf.layers.dense(layer_hidden, g_hidden_size)\n",
        "\n",
        "    return layer_hidden\n",
        "\n",
        "\n",
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings, label_mask):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  output_layer = model.get_pooled_output()\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "  keep_prob = 1\n",
        "  if is_training:\n",
        "      keep_prob = DKP\n",
        "\n",
        "  D_real_features, D_real_logits, D_real_prob = discriminator(output_layer, hidden_size, keep_prob, is_training,\n",
        "                                                              num_labels, reuse=False)\n",
        "\n",
        "  logits = D_real_logits[:, 1:]\n",
        "  probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "  one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "  if is_training:\n",
        "    per_example_loss =  -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    per_example_loss = tf.boolean_mask(per_example_loss, label_mask)\n",
        "\n",
        "    labeled_example_count = tf.cast(tf.size(per_example_loss), tf.float32)\n",
        "    D_L_Supervised = tf.divide(tf.reduce_sum(per_example_loss), tf.maximum(labeled_example_count, 1))\n",
        "  else:\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    D_L_Supervised = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "  z = tf.random_uniform([FLAGS.train_batch_size, LATENT_Z], minval=0, maxval=1, dtype=tf.float32, seed=SEED, name=None)\n",
        "  x_g = generator(z, hidden_size, keep_prob, is_training=is_training, reuse=False)\n",
        "  D_fake_features, DU_fake_logits, DU_fake_prob = discriminator(x_g, hidden_size, keep_prob, is_training, num_labels, reuse=True)\n",
        "  \n",
        "  D_L_unsupervised1U = -1 * tf.reduce_mean(tf.math.log(1 - D_real_prob[:, 0] + epsilon))\n",
        "  \n",
        "  D_L_unsupervised2U = -1 * tf.reduce_mean(tf.math.log(DU_fake_prob[:, 0] + epsilon))\n",
        "  d_loss =  D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
        "  \n",
        "  g_loss = -1 * tf.reduce_mean(tf.math.log(1 - DU_fake_prob[:, 0] + epsilon))\n",
        "\n",
        "  G_feat_match = tf.reduce_mean(tf.square(tf.reduce_mean(D_real_features, axis=0) - tf.reduce_mean(D_fake_features, axis=0)))\n",
        "  g_loss = g_loss + G_feat_match\n",
        "\n",
        "  return (d_loss, g_loss, per_example_loss, logits, probabilities)\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    label_mask = features[\"label_mask\"]\n",
        "    is_real_example = None\n",
        "    if \"is_real_example\" in features:\n",
        "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "    else:\n",
        "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (d_loss, g_loss, per_example_loss, logits, probabilities) = create_model(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings, label_mask)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "\n",
        "    bert_vars = [v for v in tvars if 'bert' in v.name]\n",
        "    d_vars = bert_vars + [v for v in tvars if 'Discriminator' in v.name]\n",
        "    g_vars = [v for v in tvars if 'Generator' in v.name]\n",
        "\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    for var in tvars:\n",
        "      init_string = \"\"\n",
        "      if var.name in initialized_variable_names:\n",
        "        init_string = \", *INIT_FROM_CKPT*\"\n",
        "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      init_string)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      d_train_op = create_optimizer(\"d\", d_vars,\n",
        "          d_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "\n",
        "      g_train_op = create_optimizer(\"g\", g_vars,\n",
        "          g_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      logging_hook = tf.train.LoggingTensorHook({\"d_loss\": d_loss, \"g_loss\": g_loss, \"per_example_loss\": per_example_loss}, every_n_iter=1)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=d_loss + g_loss,\n",
        "          train_op=tf.group(d_train_op, g_train_op),\n",
        "          training_hooks=[logging_hook],\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(\n",
        "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
        "        precision = tf_metrics.precision(labels=label_ids, predictions=predictions, num_classes=num_labels,\n",
        "                                         weights=is_real_example)\n",
        "        recall = tf_metrics.recall(labels=label_ids, predictions=predictions, num_classes=num_labels,\n",
        "                                   weights=is_real_example)\n",
        "        f1_micro = tf_metrics.f1(labels=label_ids, predictions=predictions, num_classes=num_labels,\n",
        "                           weights=is_real_example, average='micro')\n",
        "        f1_macro = tf_metrics.f1(labels=label_ids, predictions=predictions, num_classes=num_labels,\n",
        "                                 weights=is_real_example, average='macro')\n",
        "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_precision\": precision,\n",
        "            \"eval_recall\": recall,\n",
        "            \"eval_f1_micro\": f1_micro,\n",
        "            \"eval_f1_macro\": f1_macro,\n",
        "            \"eval_loss\": loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = (metric_fn,\n",
        "                      [per_example_loss, label_ids, logits, is_real_example])\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=d_loss,\n",
        "          eval_metrics=eval_metrics,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    else:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\"probabilities\": probabilities},\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn\n",
        "\n",
        "\n",
        "def get_labeled_mask(mask_size, labeled_size):\n",
        "    labeled_mask = np.zeros([mask_size], dtype = np.int16)\n",
        "    labeled_mask[range(labeled_size)] = 1\n",
        "    labeled_mask = 0.5 < labeled_mask\n",
        "    return labeled_mask\n",
        "\n",
        "\n",
        "def evaluate(estimator, label_rate, eval_examples, task_name, label_list, tokenizer):\n",
        "    num_actual_eval_examples = len(eval_examples)\n",
        "    if FLAGS.use_tpu:\n",
        "        # TPU requires a fixed batch size for all batches, therefore the number\n",
        "        # of examples must be a multiple of the batch size, or else examples\n",
        "        # will get dropped. So we pad with fake examples which are ignored\n",
        "        # later on. These do NOT count towards the metric (all tf.metrics\n",
        "        # support a per-instance weight, and these get a weight of 0.0).\n",
        "        while len(eval_examples) % FLAGS.eval_batch_size != 0:\n",
        "            eval_examples.append(PaddingInputExample())\n",
        "\n",
        "    eval_file = os.path.join(FLAGS.output_dir, \"eval_\"+str(task_name)+\".tf_record\")\n",
        "    file_based_convert_examples_to_features(\n",
        "        eval_examples, None, label_list, FLAGS.max_seq_length, tokenizer, eval_file, label_mask_rate=1)\n",
        "\n",
        "    tf.logging.info(\"***** Running evaluation *****\")\n",
        "    tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                    len(eval_examples), num_actual_eval_examples,\n",
        "                    len(eval_examples) - num_actual_eval_examples)\n",
        "    tf.logging.info(\"  Batch size = %d\", FLAGS.eval_batch_size)\n",
        "\n",
        "    #  This tells the estimator to run through the entire set.\n",
        "    eval_steps = None\n",
        "    # However, if running eval on the TPU, you will need to specify the\n",
        "    # number of steps.\n",
        "    if FLAGS.use_tpu:\n",
        "        assert len(eval_examples) % FLAGS.eval_batch_size == 0\n",
        "        eval_steps = int(len(eval_examples) // FLAGS.eval_batch_size)\n",
        "\n",
        "    eval_drop_remainder = True if FLAGS.use_tpu else False\n",
        "    eval_input_fn = file_based_input_fn_builder(\n",
        "        input_file=eval_file,\n",
        "        seq_length=FLAGS.max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=eval_drop_remainder)\n",
        "\n",
        "    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "\n",
        "    overall_result_file = open(task_name + \"_statistics_GANBERT\" + str(label_rate) + \".txt\", \"a+\")\n",
        "\n",
        "    for key in sorted(result.keys()):\n",
        "        overall_result_file.write(str(label_rate) + \" \")\n",
        "        overall_result_file.write(\"%s = %s \" % (key, str(result[key])))\n",
        "    overall_result_file.write(\"\\n\")\n",
        "\n",
        "    output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results_\"+str(task_name)+\".txt\")\n",
        "    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "        tf.logging.info(\"***** Eval results *****\")\n",
        "        for key in sorted(result.keys()):\n",
        "            tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "###########################\n",
        "#def main(_):\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "label_rate = FLAGS.label_rate\n",
        "\n",
        "processors = {\"qc-fine\": QcFineProcessor}\n",
        "\n",
        "validate_case_matches_checkpoint(FLAGS.do_lower_case,\n",
        "                                              FLAGS.init_checkpoint)\n",
        "\n",
        "if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:\n",
        "  raise ValueError(\n",
        "      \"At least one of `do_train`, `do_eval` or `do_predict' must be True.\")\n",
        "\n",
        "bert_config = BertConfig.from_json_file(FLAGS.bert_config_file)\n",
        "\n",
        "if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n",
        "  raise ValueError(\n",
        "      \"Cannot use sequence length %d because the BERT model \"\n",
        "      \"was only trained up to sequence length %d\" %\n",
        "      (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n",
        "\n",
        "tf.gfile.MakeDirs(FLAGS.output_dir)\n",
        "\n",
        "task_name = FLAGS.task_name.lower()\n",
        "\n",
        "if task_name not in processors:\n",
        "  raise ValueError(\"Task not found: %s\" % (task_name))\n",
        "\n",
        "processor = processors[task_name]()\n",
        "\n",
        "label_list = processor.get_labels()\n",
        "\n",
        "tokenizer = FullTokenizer(\n",
        "    vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
        "\n",
        "tpu_cluster_resolver = None\n",
        "if FLAGS.use_tpu and FLAGS.tpu_name:\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
        "      FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
        "\n",
        "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "  cluster=tpu_cluster_resolver,\n",
        "  master=FLAGS.master,\n",
        "  model_dir=FLAGS.output_dir,\n",
        "  save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
        "  tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "      iterations_per_loop=FLAGS.iterations_per_loop,\n",
        "      num_shards=FLAGS.num_tpu_cores,\n",
        "      per_host_input_for_training=is_per_host))\n",
        "\n",
        "num_train_steps = None\n",
        "num_warmup_steps = None\n",
        "if FLAGS.do_train:\n",
        "  labeled_examples = processor.get_labeled_examples(FLAGS.data_dir)\n",
        "  unlabeled_examples = processor.get_unlabeled_examples(FLAGS.data_dir)\n",
        "\n",
        "  num_train_examples = len(labeled_examples) + len(unlabeled_examples)\n",
        "\n",
        "  num_train_steps = int(\n",
        "        num_train_examples / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
        "  num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "    bert_config=bert_config,\n",
        "    num_labels=len(label_list),\n",
        "    init_checkpoint=FLAGS.init_checkpoint,\n",
        "    learning_rate=FLAGS.learning_rate,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=FLAGS.use_tpu,\n",
        "    use_one_hot_embeddings=FLAGS.use_tpu)\n",
        "\n",
        "# If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "# or GPU.\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=FLAGS.use_tpu,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=FLAGS.train_batch_size,\n",
        "    eval_batch_size=FLAGS.eval_batch_size,\n",
        "    predict_batch_size=FLAGS.predict_batch_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#   flags.mark_flag_as_required(\"data_dir\")\n",
        "#   flags.mark_flag_as_required(\"task_name\")\n",
        "#   flags.mark_flag_as_required(\"vocab_file\")\n",
        "#   flags.mark_flag_as_required(\"bert_config_file\")\n",
        "#   flags.mark_flag_as_required(\"output_dir\")\n",
        "#   tf.app.run()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f5048de79e0>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'ganbert_output_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f521efaed90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7F3AJi3SRES"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KObUMoyRFiB",
        "outputId": "afd017cb-227c-4d81-d5b8-050f01221ef8"
      },
      "source": [
        "if FLAGS.do_train:\n",
        "  train_file = os.path.join(FLAGS.output_dir, \"train.tf_record\")\n",
        "  num_written_examples = file_based_convert_examples_to_features(\n",
        "      labeled_examples, unlabeled_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file,\n",
        "      label_mask_rate=label_rate)\n",
        "\n",
        "  real_num_train_steps = int(\n",
        "        num_written_examples / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
        "\n",
        "  tf.logging.info(\"***** Running training *****\")\n",
        "  tf.logging.info(\"  Num examples = %d\", len(labeled_examples) + len(unlabeled_examples))\n",
        "  tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
        "  tf.logging.info(\"  Num steps = %d\", real_num_train_steps)\n",
        "  train_input_fn = file_based_input_fn_builder(\n",
        "      input_file=train_file,\n",
        "      seq_length=FLAGS.max_seq_length,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=real_num_train_steps)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: train-NUM:count How many pitchers occupy the shelf beside the crouching woman in Edgar Degas 's 1886 painting The Tub ?\n",
            "INFO:tensorflow:tokens: [CLS] How many pitchers occupy the shelf beside the c ##rouch ##ing woman in Edgar De ##gas ' s 1886 painting The Tu ##b ? [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1731 1242 26970 12774 1103 12202 3148 1103 172 22454 1158 1590 1107 9407 3177 11305 112 188 6332 3504 1109 17037 1830 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: NUM_count (id = 39)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: train-ENTY:other Which side of the face do most artists tend to show more of in self-portraits ?\n",
            "INFO:tensorflow:tokens: [CLS] Which side of the face do most artists tend to show more of in self - portraits ? [SEP]\n",
            "INFO:tensorflow:input_ids: 101 5979 1334 1104 1103 1339 1202 1211 2719 6613 1106 1437 1167 1104 1107 2191 118 10285 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: ENTY_other (id = 18)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: train-ENTY:food What chocolate bar created by Frank Mars and his wife is often called a Milky Way with peanuts ?\n",
            "INFO:tensorflow:tokens: [CLS] What chocolate bar created by Frank Mars and his wife is often called a Milk ##y Way with p ##eanut ##s ? [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1327 8888 2927 1687 1118 2748 7403 1105 1117 1676 1110 1510 1270 170 18165 1183 4714 1114 185 23629 1116 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: ENTY_food (id = 14)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: train-DESC:reason Why do roosters sing at five o 'clock in the morning ?\n",
            "INFO:tensorflow:tokens: [CLS] Why do r ##ooster ##s sing at five o ' clock in the morning ? [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2009 1202 187 24163 1116 6928 1120 1421 184 112 4705 1107 1103 2106 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: DESC_reason (id = 6)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: train-HUM:ind What is a person called that likes fire ?\n",
            "INFO:tensorflow:tokens: [CLS] What is a person called that likes fire ? [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1327 1110 170 1825 1270 1115 7407 1783 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: HUM_ind (id = 31)\n",
            "INFO:tensorflow:***** Running training *****\n",
            "INFO:tensorflow:  Num examples = 5452\n",
            "INFO:tensorflow:  Batch size = 64\n",
            "INFO:tensorflow:  Num steps = 276\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-8-1fb162a80cd9>:384: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-8-1fb162a80cd9>:361: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Running train on CPU\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (64, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (64, 128)\n",
            "INFO:tensorflow:  name = is_real_example, shape = (64,)\n",
            "INFO:tensorflow:  name = label_ids, shape = (64,)\n",
            "INFO:tensorflow:  name = label_mask, shape = (64,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (64, 128)\n",
            "WARNING:tensorflow:From <ipython-input-4-066242a57313>:344: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-4-066242a57313>:657: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (28996, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = Discriminator/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = Discriminator/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = Discriminator/dense_1/kernel:0, shape = (768, 52)\n",
            "INFO:tensorflow:  name = Discriminator/dense_1/bias:0, shape = (52,)\n",
            "INFO:tensorflow:  name = Generator/dense/kernel:0, shape = (100, 768)\n",
            "INFO:tensorflow:  name = Generator/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = Generator/dense_1/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = Generator/dense_1/bias:0, shape = (768,)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/model_fn.py:337: scalar (from tensorflow.python.framework.tensor_shape) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.TensorShape([]).\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into ganbert_output_model/model.ckpt.\n",
            "INFO:tensorflow:d_loss = 8.37627, g_loss = 0.31659892, per_example_loss = [3.2549248 3.544004  4.719287  4.8986874 5.219719  3.8004317 4.190979 ]\n",
            "INFO:tensorflow:d_loss = 8.532689, g_loss = 0.31442744, per_example_loss = [3.645586  4.904687  4.9368005 3.6032662 5.541831  4.7109303 3.4708266] (11.640 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.0859141\n",
            "INFO:tensorflow:examples/sec: 5.4985\n",
            "INFO:tensorflow:d_loss = 8.857919, g_loss = 0.31756023, per_example_loss = [5.2947826 4.4886503 4.434355 ] (7.828 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.127744\n",
            "INFO:tensorflow:examples/sec: 8.17565\n",
            "INFO:tensorflow:d_loss = 7.8597183, g_loss = 0.31295487, per_example_loss = [3.2119064 3.094573  4.3662977 2.9120808 5.133434 ] (7.584 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131852\n",
            "INFO:tensorflow:examples/sec: 8.43853\n",
            "INFO:tensorflow:d_loss = 8.131758, g_loss = 0.31365952, per_example_loss = [3.4959173 4.3782506 4.23169  ] (7.588 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131787\n",
            "INFO:tensorflow:examples/sec: 8.43439\n",
            "INFO:tensorflow:d_loss = 7.8979836, g_loss = 0.30852044, per_example_loss = [4.0043564 3.9465756 2.964244  4.2480946] (7.694 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.129972\n",
            "INFO:tensorflow:examples/sec: 8.31823\n",
            "INFO:tensorflow:d_loss = 8.707732, g_loss = 0.30234045, per_example_loss = [4.0139885 4.01038   5.733119  4.755191 ] (7.575 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132014\n",
            "INFO:tensorflow:examples/sec: 8.44891\n",
            "INFO:tensorflow:d_loss = 7.840413, g_loss = 0.27975646, per_example_loss = [3.322837  2.8295655 4.0346394 3.46105   3.285286  5.406573  3.789616\n",
            " 3.0517066 5.0824814 3.5194767] (7.545 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132525\n",
            "INFO:tensorflow:examples/sec: 8.4816\n",
            "INFO:tensorflow:d_loss = 7.5012813, g_loss = 0.2817609, per_example_loss = [3.2603188 3.3204832 2.3651078 3.0466676 5.2052355] (7.174 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.139403\n",
            "INFO:tensorflow:examples/sec: 8.9218\n",
            "INFO:tensorflow:d_loss = 7.864175, g_loss = 0.27896297, per_example_loss = [2.6189454 2.8783271 5.701695  3.134325  4.9169564 2.0802898 3.2015557\n",
            " 6.244363 ] (7.687 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.130088\n",
            "INFO:tensorflow:examples/sec: 8.32563\n",
            "INFO:tensorflow:d_loss = 8.679763, g_loss = 0.27679974, per_example_loss = [5.788993  3.8739998 6.467615  5.3443594 4.2575755 3.1938145 3.7153492] (7.520 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132973\n",
            "INFO:tensorflow:examples/sec: 8.51029\n",
            "INFO:tensorflow:d_loss = 6.538028, g_loss = 0.24468099, per_example_loss = [5.3811626 1.4417653 1.1798917 2.1646814] (7.604 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131516\n",
            "INFO:tensorflow:examples/sec: 8.41703\n",
            "INFO:tensorflow:d_loss = 7.7379208, g_loss = 0.21379034, per_example_loss = [4.8227363 4.852803  3.5427608 4.8419247 4.1322117 3.6321259 3.8762622\n",
            " 0.8708069 3.634921  3.0406227 4.4464827] (7.548 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132483\n",
            "INFO:tensorflow:examples/sec: 8.47894\n",
            "INFO:tensorflow:d_loss = 7.5107517, g_loss = 0.1952762, per_example_loss = [1.632306  4.8470693 4.251655  4.3739667 2.731522 ] (7.260 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.137735\n",
            "INFO:tensorflow:examples/sec: 8.81506\n",
            "INFO:tensorflow:d_loss = 7.2759733, g_loss = 0.21601799, per_example_loss = [2.387915  2.4922879 2.287107  5.6527596 3.2648768 5.2659907 3.9113941\n",
            " 1.8398103] (7.839 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.127579\n",
            "INFO:tensorflow:examples/sec: 8.16506\n",
            "INFO:tensorflow:d_loss = 7.710161, g_loss = 0.1878123, per_example_loss = [4.379095  3.0544405 4.1318846] (7.553 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132393\n",
            "INFO:tensorflow:examples/sec: 8.47317\n",
            "INFO:tensorflow:d_loss = 6.553917, g_loss = 0.15074566, per_example_loss = [4.6259227  2.423693   0.30268866 4.86886    0.6453671  5.171577\n",
            " 0.6102515  4.4014964  2.5035381  4.3697424  0.5819956 ] (7.896 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.126648\n",
            "INFO:tensorflow:examples/sec: 8.10546\n",
            "INFO:tensorflow:d_loss = 6.670677, g_loss = 0.14466617, per_example_loss = [2.9121366 5.8890576 1.6500139 1.3546815] (7.684 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.130145\n",
            "INFO:tensorflow:examples/sec: 8.32927\n",
            "INFO:tensorflow:d_loss = 7.3050823, g_loss = 0.19764037, per_example_loss = [3.74703   3.553451  4.2879524 2.6557608 3.6136084] (7.831 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.127705\n",
            "INFO:tensorflow:examples/sec: 8.17315\n",
            "INFO:tensorflow:d_loss = 6.6158257, g_loss = 0.15423004, per_example_loss = [1.1933327 2.2229953 4.1487575 4.195859 ] (7.869 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.127072\n",
            "INFO:tensorflow:examples/sec: 8.13258\n",
            "INFO:tensorflow:d_loss = 5.9515843, g_loss = 0.13201441, per_example_loss = [1.1405408 1.1277819 2.792779  4.1057277] (7.818 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.127906\n",
            "INFO:tensorflow:examples/sec: 8.18601\n",
            "INFO:tensorflow:d_loss = 6.1040993, g_loss = 0.16341448, per_example_loss = [4.4921727 0.6390123 0.9316499 3.32566   0.7155326 4.605308 ] (7.370 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.135678\n",
            "INFO:tensorflow:examples/sec: 8.68338\n",
            "INFO:tensorflow:d_loss = 7.003764, g_loss = 0.19574693, per_example_loss = [2.8265245 5.6582336 2.3676333 2.921215 ] (7.368 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.135731\n",
            "INFO:tensorflow:examples/sec: 8.6868\n",
            "INFO:tensorflow:d_loss = 8.243254, g_loss = 0.22235766, per_example_loss = [4.0960574 4.619376  5.4843245 5.1257887 4.041376 ] (7.868 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.127101\n",
            "INFO:tensorflow:examples/sec: 8.13444\n",
            "INFO:tensorflow:d_loss = 6.985818, g_loss = 0.29970658, per_example_loss = [3.9859576 3.663775  3.7001922 3.3546448 2.7458777 3.5165105] (7.559 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132284\n",
            "INFO:tensorflow:examples/sec: 8.46615\n",
            "INFO:tensorflow:d_loss = 7.3366575, g_loss = 0.24533585, per_example_loss = [3.6809154 3.9419863 3.8337998 4.0730066 3.3682833] (7.819 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.127888\n",
            "INFO:tensorflow:examples/sec: 8.18481\n",
            "INFO:tensorflow:d_loss = 6.547513, g_loss = 0.1590961, per_example_loss = [1.4100429 5.532476  1.7440071] (7.532 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.13277\n",
            "INFO:tensorflow:examples/sec: 8.49726\n",
            "INFO:tensorflow:d_loss = 5.8461237, g_loss = 0.13653915, per_example_loss = [3.3620927 1.2454962 2.1638932 1.9399519] (7.615 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.13132\n",
            "INFO:tensorflow:examples/sec: 8.4045\n",
            "INFO:tensorflow:d_loss = 5.959479, g_loss = 0.13257545, per_example_loss = [1.5706445 1.6655143 3.4886603 1.5207391 3.1796498 2.3908277 2.8113525] (7.664 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.130484\n",
            "INFO:tensorflow:examples/sec: 8.35099\n",
            "INFO:tensorflow:d_loss = 6.320071, g_loss = 0.12937662, per_example_loss = [2.6151428 2.3572626 2.156299  3.1766815] (7.449 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.134255\n",
            "INFO:tensorflow:examples/sec: 8.59232\n",
            "INFO:tensorflow:d_loss = 6.739365, g_loss = 0.15023848, per_example_loss = [4.287638  1.7850304 6.693296  0.7528573 0.8030002 3.7384944 2.888054\n",
            " 1.399048  3.2155113 2.026686  2.3248873] (7.534 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132723\n",
            "INFO:tensorflow:examples/sec: 8.49428\n",
            "INFO:tensorflow:d_loss = 6.4382524, g_loss = 0.1581667, per_example_loss = [3.6428394 2.356298  1.3969876 3.0423222 1.4673954 3.0660973] (7.870 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.127061\n",
            "INFO:tensorflow:examples/sec: 8.13192\n",
            "INFO:tensorflow:d_loss = 6.4076576, g_loss = 0.1422422, per_example_loss = [0.89733404 3.6223063  5.259677   0.5974054 ] (7.598 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131603\n",
            "INFO:tensorflow:examples/sec: 8.42257\n",
            "INFO:tensorflow:d_loss = 6.690717, g_loss = 0.1256446, per_example_loss = [2.388038  3.4371355 2.1365309 2.4603    4.5923    3.5742788] (7.612 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131376\n",
            "INFO:tensorflow:examples/sec: 8.40808\n",
            "INFO:tensorflow:d_loss = 5.796112, g_loss = 0.13179955, per_example_loss = [1.1334546 2.3032048 4.2653513 1.1467638 5.604286  1.207993  1.1918286] (7.654 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.130646\n",
            "INFO:tensorflow:examples/sec: 8.36137\n",
            "INFO:tensorflow:d_loss = 6.0222654, g_loss = 0.15719283, per_example_loss = [3.730672  4.142247  1.9642478 1.0160477 2.3430002 2.2416954 4.1770134\n",
            " 1.1722227 3.1477587] (7.470 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133872\n",
            "INFO:tensorflow:examples/sec: 8.56782\n",
            "INFO:tensorflow:d_loss = 5.6282544, g_loss = 0.1625753, per_example_loss = [0.8977862 3.7453303 3.5305536 0.8430337] (7.744 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.129138\n",
            "INFO:tensorflow:examples/sec: 8.26482\n",
            "INFO:tensorflow:d_loss = 5.945982, g_loss = 0.16667078, per_example_loss = [3.4442432 2.2129207 0.2392244 4.129523 ] (7.740 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.129196\n",
            "INFO:tensorflow:examples/sec: 8.26853\n",
            "INFO:tensorflow:d_loss = 6.453286, g_loss = 0.1511007, per_example_loss = [2.892438   5.357032   0.14221689 3.5644464  0.02482889 3.7748182\n",
            " 4.7807293  0.89652675] (7.500 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133321\n",
            "INFO:tensorflow:examples/sec: 8.53256\n",
            "INFO:tensorflow:d_loss = 5.088681, g_loss = 0.15200555, per_example_loss = [0.04566035 0.02329529 4.2027493  0.77171487 0.56513417 2.767929  ] (7.704 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.1298\n",
            "INFO:tensorflow:examples/sec: 8.30721\n",
            "INFO:tensorflow:d_loss = 5.7990837, g_loss = 0.1535654, per_example_loss = [0.21751325 3.3020034  3.8926435  0.9265537  2.0470107 ] (7.461 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.134035\n",
            "INFO:tensorflow:examples/sec: 8.57827\n",
            "INFO:tensorflow:d_loss = 6.0949197, g_loss = 0.15398967, per_example_loss = [2.0610974  0.04924324 1.2611599  4.432681   3.1665113  3.0019994\n",
            " 3.8518023  0.12279557] (8.024 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.124619\n",
            "INFO:tensorflow:examples/sec: 7.97564\n",
            "INFO:tensorflow:d_loss = 5.9003005, g_loss = 0.13059412, per_example_loss = [2.6831405  0.07636342 3.3710387  4.173456   1.53944    1.1578501\n",
            " 0.1500554  2.9220238 ] (7.652 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.130691\n",
            "INFO:tensorflow:examples/sec: 8.36422\n",
            "INFO:tensorflow:d_loss = 5.3077593, g_loss = 0.120422795, per_example_loss = [0.04113982 3.3997664  2.3776722  0.5934193  1.1071327  0.06330109] (7.418 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.134797\n",
            "INFO:tensorflow:examples/sec: 8.62701\n",
            "INFO:tensorflow:d_loss = 5.885581, g_loss = 0.13595724, per_example_loss = [0.754671   0.23068579 0.47569412 0.93051493 4.313114   0.6964428\n",
            " 3.329648   4.95362    0.39738598 1.5680845 ] (7.763 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.12882\n",
            "INFO:tensorflow:examples/sec: 8.2445\n",
            "INFO:tensorflow:d_loss = 6.082922, g_loss = 0.1281239, per_example_loss = [0.42736137 3.0291543  6.817262   0.1561739  0.1399716  1.0587323\n",
            " 4.9578047  0.19216794] (7.866 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.127122\n",
            "INFO:tensorflow:examples/sec: 8.13581\n",
            "INFO:tensorflow:d_loss = 5.350972, g_loss = 0.12211849, per_example_loss = [0.06271289 2.3407807  2.8470898  0.1268557  0.67752    2.4194388 ] (7.381 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.135481\n",
            "INFO:tensorflow:examples/sec: 8.67076\n",
            "INFO:tensorflow:d_loss = 4.300454, g_loss = 0.10995409, per_example_loss = [0.25449228 0.03072253 1.357362   0.0238118  0.6022487 ] (7.926 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.126173\n",
            "INFO:tensorflow:examples/sec: 8.07507\n",
            "INFO:tensorflow:d_loss = 5.421205, g_loss = 0.11694067, per_example_loss = [0.6827697  2.2720017  2.2997646  2.8090923  0.03615758] (7.626 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131127\n",
            "INFO:tensorflow:examples/sec: 8.39215\n",
            "INFO:tensorflow:d_loss = 7.289974, g_loss = 0.11546764, per_example_loss = [3.5332272 3.5373955 2.6907797 4.384212 ] (7.749 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.129055\n",
            "INFO:tensorflow:examples/sec: 8.25954\n",
            "INFO:tensorflow:d_loss = 5.595053, g_loss = 0.11813218, per_example_loss = [2.381791   0.23392412 4.8996763  3.191702   1.852282   0.01769131\n",
            " 0.66977364] (7.527 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132855\n",
            "INFO:tensorflow:examples/sec: 8.50271\n",
            "INFO:tensorflow:d_loss = 6.368349, g_loss = 0.12203698, per_example_loss = [2.2669709  2.853098   0.11336051 0.0269861  4.197654   0.13122012\n",
            " 5.5421453  4.9702306 ] (7.541 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132613\n",
            "INFO:tensorflow:examples/sec: 8.48723\n",
            "INFO:tensorflow:d_loss = 6.8544474, g_loss = 0.11507923, per_example_loss = [0.35693437 0.05006224 4.001037   4.888897   0.67267597 3.78943\n",
            " 3.513194   6.0622735  2.4836454 ] (7.792 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.128339\n",
            "INFO:tensorflow:examples/sec: 8.21372\n",
            "INFO:tensorflow:d_loss = 4.087062, g_loss = 0.11377327, per_example_loss = [0.02715445 0.06113364 0.0458823 ] (7.735 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.258579\n",
            "INFO:tensorflow:examples/sec: 16.5491\n",
            "INFO:tensorflow:d_loss = 6.9913836, g_loss = 0.12338333, per_example_loss = [3.415775   2.521393   4.2010107  0.16989644 4.12463    4.2399893 ] (7.902 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.253089\n",
            "INFO:tensorflow:examples/sec: 16.1977\n",
            "INFO:tensorflow:d_loss = 5.849147, g_loss = 0.13873708, per_example_loss = [2.1433163 1.5038624] (7.534 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.265467\n",
            "INFO:tensorflow:examples/sec: 16.9899\n",
            "INFO:tensorflow:d_loss = 5.2532372, g_loss = 0.13905951, per_example_loss = [1.8629652  0.27917686] (8.207 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.243706\n",
            "INFO:tensorflow:examples/sec: 15.5972\n",
            "INFO:tensorflow:d_loss = 7.095731, g_loss = 0.12651783, per_example_loss = [4.4619894  0.19261098 2.625186   4.162554  ] (7.673 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.260668\n",
            "INFO:tensorflow:examples/sec: 16.6827\n",
            "INFO:tensorflow:d_loss = 6.7475834, g_loss = 0.12579024, per_example_loss = [1.3325937  1.5671334  0.04836474 1.5093428  4.565113   0.08717182\n",
            " 3.8621488  5.8406     2.295244   2.655974  ] (7.771 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.257376\n",
            "INFO:tensorflow:examples/sec: 16.472\n",
            "INFO:tensorflow:d_loss = 7.0570354, g_loss = 0.12586232, per_example_loss = [3.1296906  2.2164655  5.086279   0.06342115 1.4935529  0.6986834\n",
            " 1.5621694  5.1637073  3.1300015 ] (7.472 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.267669\n",
            "INFO:tensorflow:examples/sec: 17.1308\n",
            "INFO:tensorflow:d_loss = 7.081267, g_loss = 0.13178271, per_example_loss = [1.3496834  4.4178     3.4515018  0.08728062 3.783684  ] (7.549 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.264953\n",
            "INFO:tensorflow:examples/sec: 16.957\n",
            "INFO:tensorflow:d_loss = 6.7006583, g_loss = 0.12139284, per_example_loss = [4.211328   1.6108999  0.02305442 0.03876208 2.4526472  5.059952\n",
            " 0.5954337 ] (7.911 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.252795\n",
            "INFO:tensorflow:examples/sec: 16.1789\n",
            "INFO:tensorflow:d_loss = 7.7875795, g_loss = 0.13353583, per_example_loss = [1.7448673 3.0535564 4.636694 ] (7.637 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.261877\n",
            "INFO:tensorflow:examples/sec: 16.7602\n",
            "INFO:tensorflow:d_loss = 6.0084715, g_loss = 0.114787884, per_example_loss = [2.6488266  0.03504814 2.6028693  0.00534949 0.0581024 ] (7.940 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.251895\n",
            "INFO:tensorflow:examples/sec: 16.1213\n",
            "INFO:tensorflow:d_loss = 5.6689625, g_loss = 0.11545539, per_example_loss = [0.06280716 0.00413373 3.4676838  0.8996594  2.0671744  0.02274601\n",
            " 0.01847851 1.4898179  0.00144746] (7.666 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.260898\n",
            "INFO:tensorflow:examples/sec: 16.6975\n",
            "INFO:tensorflow:d_loss = 6.0062456, g_loss = 0.10497693, per_example_loss = [4.443959   1.001621   1.7616657  1.2197512  0.35372177 0.00169991\n",
            " 2.7272391  0.05359992 2.5392706  0.47550184] (7.742 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.258321\n",
            "INFO:tensorflow:examples/sec: 16.5326\n",
            "INFO:tensorflow:d_loss = 6.656764, g_loss = 0.10877303, per_example_loss = [3.5162125  2.6796079  0.00371172 1.604063   3.0870872 ] (7.514 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.266194\n",
            "INFO:tensorflow:examples/sec: 17.0364\n",
            "INFO:tensorflow:d_loss = 6.5847683, g_loss = 0.112696104, per_example_loss = [0.04970511 1.3693472  5.049663  ] (7.539 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.265301\n",
            "INFO:tensorflow:examples/sec: 16.9793\n",
            "INFO:tensorflow:d_loss = 5.9833174, g_loss = 0.16073345, per_example_loss = [3.547122   0.00428995 0.9996059  0.00550528 3.3204155  0.00443226\n",
            " 2.9472058  3.206124   1.1184809 ] (7.822 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.255658\n",
            "INFO:tensorflow:examples/sec: 16.3621\n",
            "INFO:tensorflow:d_loss = 6.0515747, g_loss = 0.15915856, per_example_loss = [0.01661429 0.82191384 3.1695302  2.9125457 ] (7.596 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.263318\n",
            "INFO:tensorflow:examples/sec: 16.8524\n",
            "INFO:tensorflow:d_loss = 5.479074, g_loss = 0.20356832, per_example_loss = [0.07464773 0.0270259  3.1969109 ] (7.446 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.268612\n",
            "INFO:tensorflow:examples/sec: 17.1912\n",
            "INFO:tensorflow:d_loss = 6.1852155, g_loss = 0.18437995, per_example_loss = [0.01814083 3.9104428  4.6890125  0.92918247 1.7001286  0.02178035\n",
            " 0.03100491 0.7434931 ] (7.660 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.261103\n",
            "INFO:tensorflow:examples/sec: 16.7106\n",
            "INFO:tensorflow:d_loss = 6.19466, g_loss = 0.13901572, per_example_loss = [2.2023563  0.0500255  0.04605547 4.3525743  0.00892601 0.01119404\n",
            " 0.05900611 1.3518752  4.824568   0.00504174 0.02332173 2.5036669 ] (8.021 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.249343\n",
            "INFO:tensorflow:examples/sec: 15.9579\n",
            "INFO:tensorflow:d_loss = 5.2706738, g_loss = 0.11344168, per_example_loss = [0.01178317 0.01779051] (8.019 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.249431\n",
            "INFO:tensorflow:examples/sec: 15.9636\n",
            "INFO:tensorflow:d_loss = 7.1388493, g_loss = 0.093548186, per_example_loss = [0.00246317 0.02816838 3.5538661  2.955524  ] (7.603 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.263049\n",
            "INFO:tensorflow:examples/sec: 16.8351\n",
            "INFO:tensorflow:d_loss = 7.736066, g_loss = 0.09250071, per_example_loss = [4.3366327  0.01208484 5.965883   0.02685834 0.03769581 2.9365244\n",
            " 3.0259655  2.4034724 ] (7.833 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.255333\n",
            "INFO:tensorflow:examples/sec: 16.3413\n",
            "INFO:tensorflow:d_loss = 7.105043, g_loss = 0.10953138, per_example_loss = [3.0034924  0.19103055 2.1455636  0.05134882] (7.587 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.263602\n",
            "INFO:tensorflow:examples/sec: 16.8705\n",
            "INFO:tensorflow:d_loss = 6.7783804, g_loss = 0.11270695, per_example_loss = [3.6204507  1.9797676  0.16046385 0.04269086 0.00129473 0.07031365] (7.745 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.129111\n",
            "INFO:tensorflow:examples/sec: 8.26312\n",
            "INFO:tensorflow:d_loss = 7.1984735, g_loss = 0.098402336, per_example_loss = [2.434887   3.7658744  2.8482456  0.43399253 0.008656   5.278406\n",
            " 5.0672307  0.61945605 0.00885512 2.5488653  0.2793286  0.01198625] (7.610 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131411\n",
            "INFO:tensorflow:examples/sec: 8.41029\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 102 vs previous value: 102. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:d_loss = 6.199274, g_loss = 0.08637065, per_example_loss = [0.08461168 1.2378101  0.02721095 0.00527265 4.0589986 ] (7.366 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.271501\n",
            "INFO:tensorflow:examples/sec: 17.3761\n",
            "INFO:tensorflow:d_loss = 5.550757, g_loss = 0.090131536, per_example_loss = [0.49787176 0.01613887 0.09211778 0.43750867 0.22173198 0.04819174\n",
            " 0.04987298 3.1284792  2.5936327 ] (7.688 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.260137\n",
            "INFO:tensorflow:examples/sec: 16.6487\n",
            "INFO:tensorflow:d_loss = 5.880356, g_loss = 0.0936135, per_example_loss = [3.016842   0.8905752  0.03105864] (7.940 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.251903\n",
            "INFO:tensorflow:examples/sec: 16.1218\n",
            "INFO:tensorflow:d_loss = 4.4551687, g_loss = 0.1088576, per_example_loss = [0.524071   0.0623597  0.0100255  0.31727603] (7.504 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.266517\n",
            "INFO:tensorflow:examples/sec: 17.0571\n",
            "INFO:tensorflow:d_loss = 5.7269936, g_loss = 0.1155363, per_example_loss = [1.2376912  3.2570434  0.49604422 2.1223502  0.5191671 ] (7.729 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.258766\n",
            "INFO:tensorflow:examples/sec: 16.561\n",
            "INFO:tensorflow:d_loss = 5.466834, g_loss = 0.10490504, per_example_loss = [3.602427   0.00190898 2.685067   0.7456488  0.0018176  0.01094186\n",
            " 0.09837546 1.0430322  1.3766626 ] (7.624 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.262327\n",
            "INFO:tensorflow:examples/sec: 16.7889\n",
            "INFO:tensorflow:d_loss = 4.773663, g_loss = 0.11135636, per_example_loss = [0.18149365 0.00254688 0.6548453  0.05804604 0.06911211 0.00074692\n",
            " 1.8245026 ] (7.686 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.260209\n",
            "INFO:tensorflow:examples/sec: 16.6534\n",
            "INFO:tensorflow:d_loss = 6.672823, g_loss = 0.117937066, per_example_loss = [0.00295607 4.5369782  0.06845585 4.939681  ] (7.635 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.261958\n",
            "INFO:tensorflow:examples/sec: 16.7653\n",
            "INFO:tensorflow:d_loss = 6.768919, g_loss = 0.1418779, per_example_loss = [3.488594   0.00068081 0.00029846 3.6899195  1.6802015  4.7219977\n",
            " 0.56890213 3.4883513 ] (7.479 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.267433\n",
            "INFO:tensorflow:examples/sec: 17.1157\n",
            "INFO:tensorflow:d_loss = 5.763912, g_loss = 0.10605195, per_example_loss = [0.10770314 1.596008   2.854612   0.06293523] (7.784 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.256955\n",
            "INFO:tensorflow:examples/sec: 16.4452\n",
            "INFO:tensorflow:d_loss = 5.828768, g_loss = 0.12493175, per_example_loss = [0.00084603 0.00045075 0.16425534 0.00115007 1.3921394  4.1372604\n",
            " 3.571711   0.63008416] (7.585 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.263671\n",
            "INFO:tensorflow:examples/sec: 16.8749\n",
            "INFO:tensorflow:d_loss = 5.699937, g_loss = 0.112233944, per_example_loss = [0.00147556 3.9913702  0.2245475  0.31727505] (7.328 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.272946\n",
            "INFO:tensorflow:examples/sec: 17.4686\n",
            "INFO:tensorflow:d_loss = 4.8488626, g_loss = 0.1030068, per_example_loss = [0.54327595 0.00036543] (7.465 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.267932\n",
            "INFO:tensorflow:examples/sec: 17.1476\n",
            "INFO:tensorflow:d_loss = 6.210688, g_loss = 0.12041111, per_example_loss = [0.00064126 1.2142396  3.5396385 ] (7.568 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.264269\n",
            "INFO:tensorflow:examples/sec: 16.9132\n",
            "INFO:tensorflow:d_loss = 7.0512905, g_loss = 0.09230318, per_example_loss = [0.38512015 2.5554209  3.8708045  0.2482598  3.3348846 ] (7.414 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.269758\n",
            "INFO:tensorflow:examples/sec: 17.2645\n",
            "INFO:tensorflow:d_loss = 5.6127973, g_loss = 0.0895798, per_example_loss = [0.68009627 1.1605515  0.00071726 1.8190389  0.00167289] (7.580 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.263868\n",
            "INFO:tensorflow:examples/sec: 16.8876\n",
            "INFO:tensorflow:d_loss = 5.782495, g_loss = 0.08573204, per_example_loss = [0.1383799  0.39620224 3.8897002  1.3125579  0.11050124 0.02623844\n",
            " 0.08493269] (7.663 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.260985\n",
            "INFO:tensorflow:examples/sec: 16.703\n",
            "INFO:tensorflow:d_loss = 6.2487974, g_loss = 0.091937385, per_example_loss = [0.05407967 0.18545507 5.209166   0.00165647 0.00250966] (8.049 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.248478\n",
            "INFO:tensorflow:examples/sec: 15.9026\n",
            "INFO:tensorflow:d_loss = 6.510143, g_loss = 0.081676915, per_example_loss = [4.0623817  0.17510462 0.27668977 0.09434035 5.261613   0.24524\n",
            " 0.00610997] (7.710 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.259401\n",
            "INFO:tensorflow:examples/sec: 16.6017\n",
            "INFO:tensorflow:d_loss = 6.5108585, g_loss = 0.07338082, per_example_loss = [2.6935706  2.8148751  1.0316826  0.01367028 0.00347629] (7.554 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.264777\n",
            "INFO:tensorflow:examples/sec: 16.9457\n",
            "INFO:tensorflow:d_loss = 6.1580114, g_loss = 0.08844518, per_example_loss = [2.5517187  0.00223375 0.00163493 0.00234151 2.8914518  0.1771897 ] (7.667 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.260849\n",
            "INFO:tensorflow:examples/sec: 16.6943\n",
            "INFO:tensorflow:d_loss = 6.9282746, g_loss = 0.07394053, per_example_loss = [1.4593958  7.281781   2.5098932  1.0074478  0.35774735 0.04311604\n",
            " 0.13258272 0.0325895 ] (7.513 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.266198\n",
            "INFO:tensorflow:examples/sec: 17.0367\n",
            "INFO:tensorflow:d_loss = 7.8564053, g_loss = 0.07322605, per_example_loss = [0.24621454 4.0930157  0.48807812 1.8346041  2.9093094  5.7192435 ] (7.611 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.262774\n",
            "INFO:tensorflow:examples/sec: 16.8175\n",
            "INFO:tensorflow:d_loss = 6.6311774, g_loss = 0.07202177, per_example_loss = [0.2680533  0.13664612 2.7527711  3.9304366  0.09178135] (7.315 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.136704\n",
            "INFO:tensorflow:examples/sec: 8.74909\n",
            "INFO:tensorflow:d_loss = 7.8386736, g_loss = 0.097660854, per_example_loss = [3.7257254  0.0294237  6.897945   0.13818587 5.4420485  0.11065494] (7.352 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.136015\n",
            "INFO:tensorflow:examples/sec: 8.70499\n",
            "INFO:tensorflow:d_loss = 7.613082, g_loss = 0.08966202, per_example_loss = [4.554477   0.04864842 2.4220319  0.08451551 4.0052285  4.395125  ] (7.306 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.136879\n",
            "INFO:tensorflow:examples/sec: 8.76027\n",
            "INFO:tensorflow:d_loss = 8.82406, g_loss = 0.10256912, per_example_loss = [5.4024873  2.7132957  4.7491636  4.896786   0.28931272] (7.535 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.1327\n",
            "INFO:tensorflow:examples/sec: 8.49283\n",
            "INFO:tensorflow:d_loss = 6.4789934, g_loss = 0.10484272, per_example_loss = [3.7898536  0.25970158 1.2378299  2.3167062  0.10846999 0.20042798\n",
            " 2.5785842  4.7236648  0.03105784 2.2929168  0.05522337 0.04937441] (7.366 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.135757\n",
            "INFO:tensorflow:examples/sec: 8.68845\n",
            "INFO:tensorflow:d_loss = 6.5685067, g_loss = 0.09599829, per_example_loss = [4.6499867  0.01299732 0.03882377] (7.567 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132153\n",
            "INFO:tensorflow:examples/sec: 8.45782\n",
            "INFO:tensorflow:d_loss = 6.250154, g_loss = 0.08791255, per_example_loss = [0.13104832 0.03975812 4.1362524  0.00617525 3.2301645  3.238078\n",
            " 0.08746829 0.02764902 0.07662061 0.01797435] (7.428 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.134619\n",
            "INFO:tensorflow:examples/sec: 8.61561\n",
            "INFO:tensorflow:d_loss = 7.4536133, g_loss = 0.08249033, per_example_loss = [2.1654375  2.407433   0.05444551 3.2490332  3.747288  ] (7.717 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.129577\n",
            "INFO:tensorflow:examples/sec: 8.29296\n",
            "INFO:tensorflow:d_loss = 6.551612, g_loss = 0.084598474, per_example_loss = [2.5981889  0.02259125 2.201579   2.920794   1.3268864  3.8106327\n",
            " 0.0174301  0.02123046 0.01296696 1.0306718 ] (7.523 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132936\n",
            "INFO:tensorflow:examples/sec: 8.50792\n",
            "INFO:tensorflow:d_loss = 6.2556734, g_loss = 0.0892705, per_example_loss = [0.0193736  3.095503   2.4565673  0.04222379 0.03626439 1.9512131\n",
            " 0.01878191] (7.229 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.138319\n",
            "INFO:tensorflow:examples/sec: 8.85239\n",
            "INFO:tensorflow:d_loss = 6.6466026, g_loss = 0.09382758, per_example_loss = [0.9862516  3.8421593  2.646927   0.01938131 0.02011274 1.6949273 ] (7.164 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.139595\n",
            "INFO:tensorflow:examples/sec: 8.93407\n",
            "INFO:tensorflow:d_loss = 6.783096, g_loss = 0.08050757, per_example_loss = [0.38244405 3.7512448  0.02254463 2.4207566  0.07451485 2.3938448\n",
            " 0.06726846 3.6817973  2.9709792 ] (7.562 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132247\n",
            "INFO:tensorflow:examples/sec: 8.46383\n",
            "INFO:tensorflow:d_loss = 5.579416, g_loss = 0.077915385, per_example_loss = [0.01198307 0.99026126 0.02987978 2.9053423  0.02234449 0.01223358\n",
            " 1.8668225 ] (7.702 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.129828\n",
            "INFO:tensorflow:examples/sec: 8.309\n",
            "INFO:tensorflow:d_loss = 6.6008034, g_loss = 0.09136093, per_example_loss = [0.26491243 0.85385746 4.254358   1.9861687  2.5379486 ] (7.512 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133126\n",
            "INFO:tensorflow:examples/sec: 8.52009\n",
            "INFO:tensorflow:d_loss = 5.846943, g_loss = 0.08117566, per_example_loss = [3.6842918  0.01053628 0.12416486 2.7819042  1.2915823  0.1177273 ] (7.262 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.137705\n",
            "INFO:tensorflow:examples/sec: 8.81309\n",
            "INFO:tensorflow:d_loss = 6.382848, g_loss = 0.08614508, per_example_loss = [1.1141008  1.930733   3.7355742  0.16843732 2.8676155  0.08264246\n",
            " 3.0667942  1.918739   0.04436323 3.0727396 ] (7.378 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.135537\n",
            "INFO:tensorflow:examples/sec: 8.67437\n",
            "INFO:tensorflow:d_loss = 6.587254, g_loss = 0.07109137, per_example_loss = [0.04656024 2.9877465  3.9364738  3.5306208  1.980743   0.14404933\n",
            " 1.9089764 ] (7.415 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.134863\n",
            "INFO:tensorflow:examples/sec: 8.63123\n",
            "INFO:tensorflow:d_loss = 5.304766, g_loss = 0.069077626, per_example_loss = [0.02954789 0.04198205 0.01305627 0.08668572 0.639704   0.04775291\n",
            " 3.1830194 ] (7.541 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132597\n",
            "INFO:tensorflow:examples/sec: 8.48618\n",
            "INFO:tensorflow:d_loss = 6.4005275, g_loss = 0.07099116, per_example_loss = [2.1245654  0.10096659 2.0262809  0.81300694 2.9597185  0.04741986\n",
            " 3.9355168  1.1389327  1.8365434 ] (7.471 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133853\n",
            "INFO:tensorflow:examples/sec: 8.56662\n",
            "INFO:tensorflow:d_loss = 5.3607745, g_loss = 0.07192071, per_example_loss = [0.05965005 0.54658395 3.737578   0.01862677 0.02952868] (7.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.139643\n",
            "INFO:tensorflow:examples/sec: 8.93715\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 168 vs previous value: 168. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:d_loss = 4.752559, g_loss = 0.06899602, per_example_loss = [0.06690773 0.86022997 0.02930551 0.05305585 0.02816178] (7.509 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.13318\n",
            "INFO:tensorflow:examples/sec: 8.52349\n",
            "INFO:tensorflow:d_loss = 5.6220646, g_loss = 0.07485298, per_example_loss = [0.02490482 1.5417633  2.4088778  0.03928441] (7.423 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.134714\n",
            "INFO:tensorflow:examples/sec: 8.62172\n",
            "INFO:tensorflow:d_loss = 5.3490777, g_loss = 0.07563368, per_example_loss = [0.03408794 0.04729912 1.1880281  0.06254919 0.1457494  0.0429532\n",
            " 3.4407961 ] (7.506 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133238\n",
            "INFO:tensorflow:examples/sec: 8.52721\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 171 vs previous value: 171. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:d_loss = 5.516115, g_loss = 0.07780212, per_example_loss = [0.0101716  0.0118284  0.1293509  0.02456209 2.4555764  3.2473035\n",
            " 0.01563388 0.15175699] (7.493 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133466\n",
            "INFO:tensorflow:examples/sec: 8.54181\n",
            "INFO:tensorflow:d_loss = 6.485531, g_loss = 0.06983017, per_example_loss = [3.738072   0.01360501] (7.280 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.137365\n",
            "INFO:tensorflow:examples/sec: 8.79139\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 173 vs previous value: 173. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:d_loss = 4.772988, g_loss = 0.07825136, per_example_loss = [0.5850878  0.01903166 0.00598934 0.04015901] (7.208 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.138741\n",
            "INFO:tensorflow:examples/sec: 8.87941\n",
            "INFO:tensorflow:d_loss = 6.20551, g_loss = 0.08382679, per_example_loss = [0.8695009  0.01260752 0.03161308 4.035933   0.01782213 2.3374414\n",
            " 3.8928533  0.12289911] (7.534 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132726\n",
            "INFO:tensorflow:examples/sec: 8.49449\n",
            "INFO:tensorflow:d_loss = 5.6282816, g_loss = 0.06852463, per_example_loss = [0.54127306 0.04771961 0.01212582 1.7876581  1.5907885  0.05825851\n",
            " 1.4374893 ] (7.557 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132319\n",
            "INFO:tensorflow:examples/sec: 8.46841\n",
            "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 176 vs previous value: 176. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
            "INFO:tensorflow:d_loss = 5.2358637, g_loss = 0.092354104, per_example_loss = [1.0353441  0.0563047  0.05109592 0.00958484 1.0319163 ] (7.759 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.128878\n",
            "INFO:tensorflow:examples/sec: 8.24821\n",
            "INFO:tensorflow:d_loss = 5.6479726, g_loss = 0.07298778, per_example_loss = [1.1333253  1.4278874  0.03401063 1.2341067  2.051149   0.04473561\n",
            " 0.0310108  2.473318  ] (7.765 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.128783\n",
            "INFO:tensorflow:examples/sec: 8.2421\n",
            "INFO:tensorflow:d_loss = 5.55826, g_loss = 0.06967026, per_example_loss = [0.0288523  2.7590091  0.11472598 0.0551188  0.01998912] (7.655 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.130633\n",
            "INFO:tensorflow:examples/sec: 8.36049\n",
            "INFO:tensorflow:d_loss = 6.579339, g_loss = 0.079076186, per_example_loss = [0.0287279  2.6085958  0.45337307 3.837276  ] (7.710 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.129695\n",
            "INFO:tensorflow:examples/sec: 8.3005\n",
            "INFO:tensorflow:d_loss = 6.796177, g_loss = 0.08093565, per_example_loss = [1.2882597 1.3454188 3.0274167] (7.342 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.136214\n",
            "INFO:tensorflow:examples/sec: 8.71768\n",
            "INFO:tensorflow:d_loss = 6.037997, g_loss = 0.060126822, per_example_loss = [1.305885   0.04269269 0.00572624 1.0703365  1.80549    0.10133269\n",
            " 0.04591794 3.6886628  2.1617746 ] (7.627 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131107\n",
            "INFO:tensorflow:examples/sec: 8.39085\n",
            "INFO:tensorflow:d_loss = 5.7328434, g_loss = 0.06075337, per_example_loss = [0.00633648 0.8688934  1.2118956 ] (7.762 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.128828\n",
            "INFO:tensorflow:examples/sec: 8.24501\n",
            "INFO:tensorflow:d_loss = 5.6768026, g_loss = 0.05847381, per_example_loss = [0.07552645 1.6101899  0.02438644 0.01455147 1.4981343  0.00946109\n",
            " 0.3014119  1.3311687 ] (7.319 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.136622\n",
            "INFO:tensorflow:examples/sec: 8.74379\n",
            "INFO:tensorflow:d_loss = 7.197131, g_loss = 0.061014228, per_example_loss = [3.9897957  2.502214   2.4070656  0.02579778] (7.492 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133479\n",
            "INFO:tensorflow:examples/sec: 8.54267\n",
            "INFO:tensorflow:d_loss = 5.040695, g_loss = 0.060621336, per_example_loss = [0.29298392 0.38021347 0.02737567 0.04585395 0.0645654  0.45154333\n",
            " 0.01014281 0.00769851] (7.466 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133948\n",
            "INFO:tensorflow:examples/sec: 8.57266\n",
            "INFO:tensorflow:d_loss = 6.256011, g_loss = 0.063511305, per_example_loss = [1.4827855  2.1316066  0.00093667 2.3444326  0.01150839 0.03479166\n",
            " 2.4702804  2.6587758  0.07515541] (7.572 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132058\n",
            "INFO:tensorflow:examples/sec: 8.45171\n",
            "INFO:tensorflow:d_loss = 5.874611, g_loss = 0.06083964, per_example_loss = [0.01217635 0.08957324 0.13762268 3.3899713 ] (7.674 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.130311\n",
            "INFO:tensorflow:examples/sec: 8.33992\n",
            "INFO:tensorflow:d_loss = 7.0407476, g_loss = 0.06065166, per_example_loss = [6.224233   0.03740613 0.05844316] (7.564 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132201\n",
            "INFO:tensorflow:examples/sec: 8.46084\n",
            "INFO:tensorflow:d_loss = 6.3239036, g_loss = 0.061138395, per_example_loss = [0.38571516 1.1657013  1.1336195  3.94114    1.0305727  0.09081916] (7.655 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.130633\n",
            "INFO:tensorflow:examples/sec: 8.36049\n",
            "INFO:tensorflow:d_loss = 5.9805565, g_loss = 0.0647161, per_example_loss = [0.05358185 1.2972782  0.92311317 1.8292841 ] (7.467 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133927\n",
            "INFO:tensorflow:examples/sec: 8.57133\n",
            "INFO:tensorflow:d_loss = 5.8490663, g_loss = 0.062109165, per_example_loss = [1.2432556  0.0386553  0.3981973  0.07228256 2.6203823  0.09515695\n",
            " 2.4712033 ] (7.409 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.134954\n",
            "INFO:tensorflow:examples/sec: 8.63708\n",
            "INFO:tensorflow:d_loss = 5.4104943, g_loss = 0.06165101, per_example_loss = [0.3557386  1.7815273  0.38004214 0.08655179 0.11662196 0.39797503\n",
            " 2.2788167 ] (7.372 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.135645\n",
            "INFO:tensorflow:examples/sec: 8.68128\n",
            "INFO:tensorflow:d_loss = 6.087349, g_loss = 0.058881517, per_example_loss = [2.3395169  0.04710785 1.6615634  0.01876203 0.3826675  0.01339625\n",
            " 4.3362055  1.9983023  0.32250994] (7.661 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.130529\n",
            "INFO:tensorflow:examples/sec: 8.35387\n",
            "INFO:tensorflow:d_loss = 6.0874004, g_loss = 0.06501028, per_example_loss = [3.7049747  1.7966093  1.2252655  0.8826941  0.04741668 0.18589714] (7.528 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132825\n",
            "INFO:tensorflow:examples/sec: 8.50079\n",
            "INFO:tensorflow:d_loss = 5.6850634, g_loss = 0.05941437, per_example_loss = [2.9304535  0.14759092 1.3885227  0.41299227 0.10696966] (7.468 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133906\n",
            "INFO:tensorflow:examples/sec: 8.56997\n",
            "INFO:tensorflow:d_loss = 4.9678006, g_loss = 0.06500361, per_example_loss = [0.05856233 0.08054957 0.05257007 1.4755353 ] (7.807 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.128087\n",
            "INFO:tensorflow:examples/sec: 8.19755\n",
            "INFO:tensorflow:d_loss = 5.0757976, g_loss = 0.06662719, per_example_loss = [0.11020041 0.02265955 0.05308084 2.4561296  0.09852228] (7.494 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.13344\n",
            "INFO:tensorflow:examples/sec: 8.54016\n",
            "INFO:tensorflow:d_loss = 5.3826585, g_loss = 0.063273825, per_example_loss = [1.3597628  1.4120415  0.9074478  0.5249356  1.7730181  0.01716965\n",
            " 0.05521818 0.32727006 0.05395555] (7.526 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132873\n",
            "INFO:tensorflow:examples/sec: 8.50386\n",
            "INFO:tensorflow:d_loss = 6.038851, g_loss = 0.06611157, per_example_loss = [1.9854294  1.4885238  1.3909613  1.4801006  0.16983406 2.707226\n",
            " 2.1116915  0.5871473 ] (7.399 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.135153\n",
            "INFO:tensorflow:examples/sec: 8.64982\n",
            "INFO:tensorflow:d_loss = 5.219901, g_loss = 0.069431745, per_example_loss = [0.00604172 1.5506032  0.04987955 0.8136544  0.77631557 0.02492272] (7.709 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.129718\n",
            "INFO:tensorflow:examples/sec: 8.30196\n",
            "INFO:tensorflow:d_loss = 5.000904, g_loss = 0.075005956, per_example_loss = [0.00646393 0.01556768 0.05694633 1.8770709  0.01404297 1.682343\n",
            " 0.04331126 1.3742555  0.08973073 0.03020565 0.1110908 ] (7.402 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.13509\n",
            "INFO:tensorflow:examples/sec: 8.64576\n",
            "INFO:tensorflow:d_loss = 5.291334, g_loss = 0.07850987, per_example_loss = [1.5582604  0.13217609 1.7590005  0.09361219 0.03475608 0.00075753] (7.446 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.1343\n",
            "INFO:tensorflow:examples/sec: 8.59521\n",
            "INFO:tensorflow:d_loss = 4.9974895, g_loss = 0.07935372, per_example_loss = [2.084074   0.87913895 0.01805467 0.0163297  0.02893534 0.01230659] (7.469 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133881\n",
            "INFO:tensorflow:examples/sec: 8.56841\n",
            "INFO:tensorflow:d_loss = 5.5349455, g_loss = 0.08080995, per_example_loss = [0.9534625  0.03993991 2.1485245  2.3132048  1.1578765  0.03515449] (7.501 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133318\n",
            "INFO:tensorflow:examples/sec: 8.53238\n",
            "INFO:tensorflow:d_loss = 5.177452, g_loss = 0.080899246, per_example_loss = [0.03834732 1.4270068  0.41707724] (7.605 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131498\n",
            "INFO:tensorflow:examples/sec: 8.41589\n",
            "INFO:tensorflow:d_loss = 6.5684595, g_loss = 0.0852178, per_example_loss = [0.02660741 4.7903333  0.00100395 3.5717244 ] (7.644 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.13082\n",
            "INFO:tensorflow:examples/sec: 8.37247\n",
            "INFO:tensorflow:d_loss = 5.5577526, g_loss = 0.07098652, per_example_loss = [0.07175566 1.9524332  0.9156919  0.6710587  1.5614439  1.4442385 ] (7.820 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.12788\n",
            "INFO:tensorflow:examples/sec: 8.18435\n",
            "INFO:tensorflow:d_loss = 5.26756, g_loss = 0.06294967, per_example_loss = [0.04271279 2.0653133  0.02524371] (7.481 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133682\n",
            "INFO:tensorflow:examples/sec: 8.55566\n",
            "INFO:tensorflow:d_loss = 5.1725416, g_loss = 0.0764085, per_example_loss = [2.431276   1.294387   0.7862167  0.00057514 0.15662655 0.03962453\n",
            " 0.00227907] (7.362 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.135831\n",
            "INFO:tensorflow:examples/sec: 8.69316\n",
            "INFO:tensorflow:d_loss = 5.3358626, g_loss = 0.07608855, per_example_loss = [0.00095679 0.48869002 0.98681927 0.07863457 0.96503854 0.00178047\n",
            " 1.9393944  2.3492103  0.00155162 0.31361246] (7.337 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.136294\n",
            "INFO:tensorflow:examples/sec: 8.7228\n",
            "INFO:tensorflow:d_loss = 6.406171, g_loss = 0.06526034, per_example_loss = [2.8555708 2.2877946 0.3791225] (7.444 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.134338\n",
            "INFO:tensorflow:examples/sec: 8.59762\n",
            "INFO:tensorflow:d_loss = 5.265386, g_loss = 0.0719106, per_example_loss = [0.9485044  0.04307482 0.9526748  0.9246843  2.5123992  0.00077718\n",
            " 0.08556475] (7.614 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131338\n",
            "INFO:tensorflow:examples/sec: 8.40563\n",
            "INFO:tensorflow:d_loss = 5.414035, g_loss = 0.064644344, per_example_loss = [0.02944025 0.81132656 0.00140271 1.840654   1.3592223 ] (7.492 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133469\n",
            "INFO:tensorflow:examples/sec: 8.54202\n",
            "INFO:tensorflow:d_loss = 5.5338635, g_loss = 0.07043114, per_example_loss = [2.7679384  0.07616272 1.3717533  1.2531672  0.07767961 2.6512794\n",
            " 0.00105588 0.8033114  0.05056148] (7.409 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.134979\n",
            "INFO:tensorflow:examples/sec: 8.63867\n",
            "INFO:tensorflow:d_loss = 5.269084, g_loss = 0.067202434, per_example_loss = [0.03074207 0.09721465 0.02094449 1.4398766  1.863168  ] (7.475 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133777\n",
            "INFO:tensorflow:examples/sec: 8.56176\n",
            "INFO:tensorflow:d_loss = 5.4610224, g_loss = 0.07007731, per_example_loss = [0.41441712 0.5028509  2.0217628  1.0838332  0.00040904] (7.923 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.126214\n",
            "INFO:tensorflow:examples/sec: 8.07769\n",
            "INFO:tensorflow:d_loss = 4.7804713, g_loss = 0.063174315, per_example_loss = [0.01623177 0.02370819 0.00055667 0.6322537 ] (7.560 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132272\n",
            "INFO:tensorflow:examples/sec: 8.46538\n",
            "INFO:tensorflow:d_loss = 5.8219833, g_loss = 0.06699357, per_example_loss = [0.51554036 0.03380206 4.4914975  1.0533072  0.5350524  0.00051545] (7.565 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132193\n",
            "INFO:tensorflow:examples/sec: 8.46035\n",
            "INFO:tensorflow:d_loss = 4.47437, g_loss = 0.07267349, per_example_loss = [0.00095012] (7.620 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131232\n",
            "INFO:tensorflow:examples/sec: 8.39885\n",
            "INFO:tensorflow:d_loss = 5.5197306, g_loss = 0.062501326, per_example_loss = [0.01973809 0.53708434 0.1186919  2.8303673  0.06399953 1.9581971 ] (7.582 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131888\n",
            "INFO:tensorflow:examples/sec: 8.44085\n",
            "INFO:tensorflow:d_loss = 4.8047767, g_loss = 0.0732014, per_example_loss = [0.00125973 0.5833346 ] (7.335 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.136328\n",
            "INFO:tensorflow:examples/sec: 8.725\n",
            "INFO:tensorflow:d_loss = 4.7960186, g_loss = 0.07058594, per_example_loss = [0.308788   0.4753441  0.016845   0.0331304  0.24637267] (7.461 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.134027\n",
            "INFO:tensorflow:examples/sec: 8.5777\n",
            "INFO:tensorflow:d_loss = 4.972569, g_loss = 0.07168526, per_example_loss = [1.5800505  1.1259034  0.02401432 0.02188636 0.01514308 0.20928434\n",
            " 0.01605687 0.02319478 0.8795027 ] (7.486 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133587\n",
            "INFO:tensorflow:examples/sec: 8.54958\n",
            "INFO:tensorflow:d_loss = 4.9741373, g_loss = 0.083022796, per_example_loss = [0.01299931 0.02473515 0.00137128 2.3903089  0.02715631 0.00047494] (7.745 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.129114\n",
            "INFO:tensorflow:examples/sec: 8.26328\n",
            "INFO:tensorflow:d_loss = 4.736321, g_loss = 0.072362, per_example_loss = [0.0359705  0.5294436  0.19295476 0.00241964] (7.714 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.129645\n",
            "INFO:tensorflow:examples/sec: 8.29727\n",
            "INFO:tensorflow:d_loss = 5.120282, g_loss = 0.09146883, per_example_loss = [0.20118739 0.4440105  0.00082173 1.431704   0.50607973] (7.501 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133309\n",
            "INFO:tensorflow:examples/sec: 8.53177\n",
            "INFO:tensorflow:d_loss = 4.90709, g_loss = 0.069581054, per_example_loss = [0.00111268 0.00074847 0.9315732 ] (7.502 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133297\n",
            "INFO:tensorflow:examples/sec: 8.53099\n",
            "INFO:tensorflow:d_loss = 4.8765445, g_loss = 0.07566239, per_example_loss = [0.00298412 0.59792644 1.8868892  0.01954803 0.00111375 0.25453842\n",
            " 0.07516283] (7.600 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131577\n",
            "INFO:tensorflow:examples/sec: 8.42092\n",
            "INFO:tensorflow:d_loss = 5.3850484, g_loss = 0.07917247, per_example_loss = [1.9261754  0.00272059 0.01713367 3.0390556  1.7377123  0.20415051\n",
            " 0.03409601] (7.484 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133613\n",
            "INFO:tensorflow:examples/sec: 8.55122\n",
            "INFO:tensorflow:d_loss = 4.640749, g_loss = 0.06822184, per_example_loss = [0.03426179 0.05301504 0.09038211 0.80386424] (7.449 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.134256\n",
            "INFO:tensorflow:examples/sec: 8.59238\n",
            "INFO:tensorflow:d_loss = 5.3199844, g_loss = 0.067492664, per_example_loss = [0.05107462 1.1735134  1.6043489  0.9346658  0.7197069  0.09133459\n",
            " 1.623254  ] (7.521 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132959\n",
            "INFO:tensorflow:examples/sec: 8.50936\n",
            "INFO:tensorflow:d_loss = 5.552899, g_loss = 0.07225011, per_example_loss = [2.653749   0.6134117  0.01631199] (7.702 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.129834\n",
            "INFO:tensorflow:examples/sec: 8.30939\n",
            "INFO:tensorflow:d_loss = 4.934012, g_loss = 0.082142666, per_example_loss = [1.5125725  0.00206912 0.00113411 0.00197156 0.04043438 0.48272002\n",
            " 0.00148675 1.3552526  0.02667044 0.00224779] (7.478 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133708\n",
            "INFO:tensorflow:examples/sec: 8.55728\n",
            "INFO:tensorflow:d_loss = 4.683834, g_loss = 0.08662122, per_example_loss = [0.5412897  0.06376625 0.01210404 0.00315371 0.00081851 0.5618507\n",
            " 0.00453408 0.19461703] (7.561 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132259\n",
            "INFO:tensorflow:examples/sec: 8.46459\n",
            "INFO:tensorflow:d_loss = 5.0925074, g_loss = 0.06597187, per_example_loss = [0.03814951 0.68172735 1.7576964  0.812586   0.15651657 0.62657416\n",
            " 0.23679158 0.00186924] (7.556 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132352\n",
            "INFO:tensorflow:examples/sec: 8.4705\n",
            "INFO:tensorflow:d_loss = 5.002663, g_loss = 0.076692306, per_example_loss = [1.9079144  0.15111847 0.031068   0.00089748 0.05359563 0.7469576\n",
            " 0.6156752 ] (7.392 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.135281\n",
            "INFO:tensorflow:examples/sec: 8.658\n",
            "INFO:tensorflow:d_loss = 5.0304203, g_loss = 0.07067765, per_example_loss = [0.4556036  0.3228502  0.06827583 0.80726224 0.00129735 2.0206482\n",
            " 0.00108124] (7.406 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.135022\n",
            "INFO:tensorflow:examples/sec: 8.64139\n",
            "INFO:tensorflow:d_loss = 4.8522744, g_loss = 0.06763004, per_example_loss = [0.004872  0.0544227 0.745645 ] (7.575 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132013\n",
            "INFO:tensorflow:examples/sec: 8.4488\n",
            "INFO:tensorflow:d_loss = 4.744403, g_loss = 0.08171015, per_example_loss = [0.32269284 0.34255615 0.00198905 0.00225231 0.01885842 0.886017\n",
            " 0.3811115  0.00068653] (7.399 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.13516\n",
            "INFO:tensorflow:examples/sec: 8.65024\n",
            "INFO:tensorflow:d_loss = 5.0778008, g_loss = 0.07319184, per_example_loss = [0.36370206 0.01596513 0.09716327 0.3636292  1.8259021  1.846765\n",
            " 0.05624858 0.07597701] (7.519 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132996\n",
            "INFO:tensorflow:examples/sec: 8.51178\n",
            "INFO:tensorflow:d_loss = 4.767984, g_loss = 0.07440512, per_example_loss = [0.03352151 0.38248596 0.09869403 0.00122127 0.33640453 0.00147365\n",
            " 0.02547988 1.1272724  0.04191301] (7.539 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132635\n",
            "INFO:tensorflow:examples/sec: 8.48865\n",
            "INFO:tensorflow:d_loss = 5.415532, g_loss = 0.07092835, per_example_loss = [0.00152912 1.1564933  1.5196068  0.8725222  0.00172145 1.4022956\n",
            " 0.12142948 1.8130541 ] (7.579 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.13194\n",
            "INFO:tensorflow:examples/sec: 8.44418\n",
            "INFO:tensorflow:d_loss = 4.8456492, g_loss = 0.06609614, per_example_loss = [0.14173077 0.10005117 0.33156762 0.13456355 1.1491114  0.06314319\n",
            " 0.03182188] (7.743 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.12915\n",
            "INFO:tensorflow:examples/sec: 8.26562\n",
            "INFO:tensorflow:d_loss = 4.695601, g_loss = 0.0733035, per_example_loss = [0.02372309 0.00132247 0.70047545 0.11540939 0.4861809  0.00355113\n",
            " 0.00062339] (7.684 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.130152\n",
            "INFO:tensorflow:examples/sec: 8.32971\n",
            "INFO:tensorflow:d_loss = 5.6151824, g_loss = 0.06885243, per_example_loss = [1.3913374  3.5681493  0.00284279 2.1085353  0.00130663 1.4950291\n",
            " 0.00139437 0.00097359 1.0770442 ] (7.513 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133101\n",
            "INFO:tensorflow:examples/sec: 8.51847\n",
            "INFO:tensorflow:d_loss = 4.773329, g_loss = 0.07485521, per_example_loss = [0.3961231  0.2336284  0.00078314 0.04524116 0.3576094  0.49033916\n",
            " 1.5328711  0.00196098 0.02657305] (7.293 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.13711\n",
            "INFO:tensorflow:examples/sec: 8.77506\n",
            "INFO:tensorflow:d_loss = 4.5912156, g_loss = 0.072711185, per_example_loss = [0.040256   0.00111792 0.00148103] (7.355 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.135954\n",
            "INFO:tensorflow:examples/sec: 8.70109\n",
            "INFO:tensorflow:d_loss = 4.931654, g_loss = 0.073743485, per_example_loss = [0.00333622 0.07831495 0.04888968 1.362855  ] (7.353 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.136008\n",
            "INFO:tensorflow:examples/sec: 8.7045\n",
            "INFO:tensorflow:d_loss = 4.708738, g_loss = 0.06311644, per_example_loss = [0.00138306 0.03157496 0.6507491 ] (7.383 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.135451\n",
            "INFO:tensorflow:examples/sec: 8.66885\n",
            "INFO:tensorflow:d_loss = 4.718819, g_loss = 0.08426168, per_example_loss = [0.0004859  0.01995032 0.00102158 1.7986239  0.00126746 0.12990674] (7.658 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.130578\n",
            "INFO:tensorflow:examples/sec: 8.35699\n",
            "INFO:tensorflow:d_loss = 4.6624446, g_loss = 0.07127494, per_example_loss = [0.37230915 0.06662159 0.00121151 0.07245088 0.02826353] (7.494 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133438\n",
            "INFO:tensorflow:examples/sec: 8.54002\n",
            "INFO:tensorflow:d_loss = 4.828374, g_loss = 0.072264954, per_example_loss = [0.00161612 0.41817537 0.38690323 0.05474158 0.05696919 0.04284517\n",
            " 1.238799  ] (7.289 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.137199\n",
            "INFO:tensorflow:examples/sec: 8.78076\n",
            "INFO:tensorflow:d_loss = 5.422857, g_loss = 0.065625794, per_example_loss = [1.2443405  1.7999026  0.8732629  0.02993346 2.3634725  0.0310531\n",
            " 0.0255955  0.00367763] (7.567 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132152\n",
            "INFO:tensorflow:examples/sec: 8.45771\n",
            "INFO:tensorflow:d_loss = 5.1424403, g_loss = 0.07918024, per_example_loss = [0.22946565 0.18605234 0.01462971 2.2750514  1.0324904  0.00053106\n",
            " 0.24826397] (7.328 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.136471\n",
            "INFO:tensorflow:examples/sec: 8.73413\n",
            "INFO:tensorflow:d_loss = 4.4649773, g_loss = 0.07232064, per_example_loss = [0.0006415  0.022055   0.26326522] (7.289 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.137188\n",
            "INFO:tensorflow:examples/sec: 8.78003\n",
            "INFO:tensorflow:d_loss = 4.800153, g_loss = 0.06628443, per_example_loss = [0.10414897 0.85463756 0.512622   0.00205258 0.0269709 ] (7.511 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133153\n",
            "INFO:tensorflow:examples/sec: 8.52179\n",
            "INFO:tensorflow:d_loss = 5.013203, g_loss = 0.067207195, per_example_loss = [1.2098377  0.27613586 0.50666416 0.2016398  0.15037957 0.45696205] (7.392 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.13529\n",
            "INFO:tensorflow:examples/sec: 8.65853\n",
            "INFO:tensorflow:d_loss = 5.025491, g_loss = 0.070327595, per_example_loss = [0.9564152  0.03706135 1.1203617  0.06859433] (7.566 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.13217\n",
            "INFO:tensorflow:examples/sec: 8.45887\n",
            "INFO:tensorflow:d_loss = 5.1400757, g_loss = 0.06891591, per_example_loss = [0.06060342 1.3204098  0.00127711 0.00383891 1.5469836  0.00255984\n",
            " 0.04524879 1.8251742 ] (7.220 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.138509\n",
            "INFO:tensorflow:examples/sec: 8.8646\n",
            "INFO:tensorflow:d_loss = 5.0923595, g_loss = 0.08178276, per_example_loss = [1.3433717  0.00084579 1.2268734  0.02889897 0.54147625] (7.763 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.128816\n",
            "INFO:tensorflow:examples/sec: 8.24419\n",
            "INFO:tensorflow:d_loss = 5.0137787, g_loss = 0.07804873, per_example_loss = [0.00225123 0.91287273 0.04431066 1.3924505  0.3440363 ] (7.624 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131151\n",
            "INFO:tensorflow:examples/sec: 8.3937\n",
            "INFO:tensorflow:d_loss = 4.907693, g_loss = 0.07447493, per_example_loss = [0.00162945 0.00074109 0.8567873  0.9233108  0.00170562] (7.527 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132859\n",
            "INFO:tensorflow:examples/sec: 8.50295\n",
            "INFO:tensorflow:d_loss = 4.9895544, g_loss = 0.0741412, per_example_loss = [1.0386147 0.0807242] (7.327 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.136477\n",
            "INFO:tensorflow:examples/sec: 8.73455\n",
            "INFO:tensorflow:d_loss = 4.6084, g_loss = 0.06766005, per_example_loss = [0.06345359 0.04070178 0.03243302 0.6250287  0.00112554 0.00123151] (7.528 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132834\n",
            "INFO:tensorflow:examples/sec: 8.50139\n",
            "INFO:tensorflow:d_loss = 4.6900105, g_loss = 0.07219649, per_example_loss = [0.0443938  1.0266321  0.00118519 0.00056465 0.02301341 0.04615394] (7.505 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133247\n",
            "INFO:tensorflow:examples/sec: 8.52778\n",
            "INFO:tensorflow:d_loss = 4.7469544, g_loss = 0.07750709, per_example_loss = [0.38377225 0.36616448 0.00138699 0.10218774] (7.491 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.133503\n",
            "INFO:tensorflow:examples/sec: 8.54418\n",
            "INFO:tensorflow:d_loss = 6.0208845, g_loss = 0.07132588, per_example_loss = [4.3077707  2.6489508  0.01194668 0.00096108 1.5668707  0.00420721\n",
            " 4.27213    0.00083078 3.347721   0.13630125 1.4590894 ] (7.551 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132416\n",
            "INFO:tensorflow:examples/sec: 8.4746\n",
            "INFO:tensorflow:d_loss = 5.2597876, g_loss = 0.072579354, per_example_loss = [0.00099633 1.504691   1.1204946  0.81656694] (7.617 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.131294\n",
            "INFO:tensorflow:examples/sec: 8.40284\n",
            "INFO:tensorflow:d_loss = 5.787159, g_loss = 0.06878622, per_example_loss = [0.00104575 4.1009464  1.2110534  1.2831445  0.00103706 1.4912536\n",
            " 2.0079637  0.04990632] (7.789 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.128378\n",
            "INFO:tensorflow:examples/sec: 8.21616\n",
            "INFO:tensorflow:d_loss = 4.379634, g_loss = 0.072712906, per_example_loss = [0.00065389 0.06501254 0.00208268 0.0028416 ] (7.638 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.130914\n",
            "INFO:tensorflow:examples/sec: 8.37852\n",
            "INFO:tensorflow:d_loss = 4.727124, g_loss = 0.07279303, per_example_loss = [0.6897797  0.73733664 0.03421467 0.00145996 0.00212574] (7.341 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.136219\n",
            "INFO:tensorflow:examples/sec: 8.71802\n",
            "INFO:tensorflow:d_loss = 4.892158, g_loss = 0.079119414, per_example_loss = [1.5900807  0.00133854 0.04140281 0.39087614 0.00198394] (7.556 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.132346\n",
            "INFO:tensorflow:examples/sec: 8.47013\n",
            "INFO:tensorflow:d_loss = 4.7363095, g_loss = 0.07147532, per_example_loss = [0.00105206 0.03347943 0.11736502 0.47321972 0.46712857 1.1481522 ] (7.658 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.130585\n",
            "INFO:tensorflow:examples/sec: 8.35741\n",
            "INFO:tensorflow:d_loss = 4.815037, g_loss = 0.07786875, per_example_loss = [0.00228692 0.00130818 0.0011503  0.24858858 0.18890102 0.20566097\n",
            " 0.03058842 1.674802  ] (7.260 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.137733\n",
            "INFO:tensorflow:examples/sec: 8.81492\n",
            "INFO:tensorflow:Saving checkpoints for 276 into ganbert_output_model/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 4.8929057.\n",
            "INFO:tensorflow:training_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZAEnTbQSS6_"
      },
      "source": [
        "##Eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D-7-paYRDc2",
        "outputId": "657d5f9a-1cfe-4e8a-c1e3-96cb6634c6d2"
      },
      "source": [
        "if FLAGS.do_eval:\n",
        "  eval_examples = processor.get_test_examples(FLAGS.data_dir)\n",
        "  evaluate(estimator=estimator, label_rate=label_rate, eval_examples=eval_examples,\n",
        "            task_name=task_name, label_list=label_list, tokenizer=tokenizer)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: test-NUM:dist How far is it from Denver to Aspen ?\n",
            "INFO:tensorflow:tokens: [CLS] How far is it from Denver to Aspen ? [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1731 1677 1110 1122 1121 7068 1106 23180 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: NUM_dist (id = 41)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: test-LOC:city \"What county is Modesto \n",
            "INFO:tensorflow:tokens: [CLS] \" What county is Mode ##sto [SEP]\n",
            "INFO:tensorflow:input_ids: 101 107 1327 2514 1110 18390 12223 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: LOC_city (id = 33)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: test-HUM:desc Who was Galileo ?\n",
            "INFO:tensorflow:tokens: [CLS] Who was Galileo ? [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2627 1108 23926 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: HUM_desc (id = 29)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: test-DESC:def What is an atom ?\n",
            "INFO:tensorflow:tokens: [CLS] What is an atom ? [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1327 1110 1126 18858 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: DESC_def (id = 3)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: test-NUM:date When did Hawaii become a state ?\n",
            "INFO:tensorflow:tokens: [CLS] When did Hawaii become a state ? [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1332 1225 6826 1561 170 1352 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: NUM_date (id = 40)\n",
            "INFO:tensorflow:***** Running evaluation *****\n",
            "INFO:tensorflow:  Num examples = 500 (500 actual, 0 padding)\n",
            "INFO:tensorflow:  Batch size = 8\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Running eval on CPU\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = is_real_example, shape = (?,)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = label_mask, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (28996, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = Discriminator/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = Discriminator/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = Discriminator/dense_1/kernel:0, shape = (768, 52)\n",
            "INFO:tensorflow:  name = Discriminator/dense_1/bias:0, shape = (52,)\n",
            "INFO:tensorflow:  name = Generator/dense/kernel:0, shape = (100, 768)\n",
            "INFO:tensorflow:  name = Generator/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = Generator/dense_1/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = Generator/dense_1/bias:0, shape = (768,)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_metrics/__init__.py:152: The name tf.diag_part is deprecated. Please use tf.linalg.tensor_diag_part instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_metrics/__init__.py:140: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-04-03T21:05:11Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ganbert_output_model/model.ckpt-276\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2021-04-03-21:05:34\n",
            "INFO:tensorflow:Saving dict for global step 276: eval_accuracy = 0.518, eval_f1_macro = 0.14020877, eval_f1_micro = 0.518, eval_loss = 2.2321153, eval_precision = 0.518, eval_recall = 0.518, global_step = 276, loss = 6.5903225\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 276: ganbert_output_model/model.ckpt-276\n",
            "INFO:tensorflow:evaluation_loop marked as finished\n",
            "INFO:tensorflow:***** Eval results *****\n",
            "INFO:tensorflow:  eval_accuracy = 0.518\n",
            "INFO:tensorflow:  eval_f1_macro = 0.14020877\n",
            "INFO:tensorflow:  eval_f1_micro = 0.518\n",
            "INFO:tensorflow:  eval_loss = 2.2321153\n",
            "INFO:tensorflow:  eval_precision = 0.518\n",
            "INFO:tensorflow:  eval_recall = 0.518\n",
            "INFO:tensorflow:  global_step = 276\n",
            "INFO:tensorflow:  loss = 6.5903225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liB7dQzJSV9J"
      },
      "source": [
        "##Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuIMD2trRBFC",
        "outputId": "f18a8521-18cb-41ce-8e51-f6de6eafa3b4"
      },
      "source": [
        "if True:\n",
        "#if FLAGS.do_predict:\n",
        "  predict_examples = processor.get_test_examples(FLAGS.data_dir)\n",
        "  num_actual_predict_examples = len(predict_examples)\n",
        "  if FLAGS.use_tpu:\n",
        "    # TPU requires a fixed batch size for all batches, therefore the number\n",
        "    # of examples must be a multiple of the batch size, or else examples\n",
        "    # will get dropped. So we pad with fake examples which are ignored\n",
        "    # later on.\n",
        "    while len(predict_examples) % FLAGS.predict_batch_size != 0:\n",
        "      predict_examples.append(PaddingInputExample())\n",
        "\n",
        "  predict_file = os.path.join(FLAGS.output_dir, \"predict.tf_record\")\n",
        "  file_based_convert_examples_to_features(predict_examples, None, label_list,\n",
        "                                          FLAGS.max_seq_length, tokenizer,\n",
        "                                          predict_file, label_mask_rate=label_rate, is_testing=True)\n",
        "\n",
        "  tf.logging.info(\"***** Running prediction*****\")\n",
        "  tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                  len(predict_examples), num_actual_predict_examples,\n",
        "                  len(predict_examples) - num_actual_predict_examples)\n",
        "  tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\n",
        "\n",
        "  predict_drop_remainder = True if FLAGS.use_tpu else False\n",
        "  predict_input_fn = file_based_input_fn_builder(\n",
        "      input_file=predict_file,\n",
        "      seq_length=FLAGS.max_seq_length,\n",
        "      is_training=False,\n",
        "      drop_remainder=predict_drop_remainder)\n",
        "\n",
        "  result = estimator.predict(input_fn=predict_input_fn)\n",
        "\n",
        "\n",
        "  output_predict_file = os.path.join(FLAGS.output_dir, \"test_results.tsv\")\n",
        "  with tf.gfile.GFile(output_predict_file, \"w\") as writer:\n",
        "    num_written_lines = 0\n",
        "    tf.logging.info(\"***** Predict results *****\")\n",
        "    for (i, prediction) in enumerate(result):\n",
        "      probabilities = prediction[\"probabilities\"]\n",
        "      if i >= num_actual_predict_examples:\n",
        "        break\n",
        "      output_line = \"\\t\".join(\n",
        "          str(class_probability)\n",
        "          for class_probability in probabilities) + \"\\n\"\n",
        "      writer.write(output_line)\n",
        "      num_written_lines += 1\n",
        "  assert num_written_lines == num_actual_predict_examples"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: test-NUM:dist How far is it from Denver to Aspen ?\n",
            "INFO:tensorflow:tokens: [CLS] How far is it from Denver to Aspen ? [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1731 1677 1110 1122 1121 7068 1106 23180 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: NUM_dist (id = 41)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: test-LOC:city \"What county is Modesto \n",
            "INFO:tensorflow:tokens: [CLS] \" What county is Mode ##sto [SEP]\n",
            "INFO:tensorflow:input_ids: 101 107 1327 2514 1110 18390 12223 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: LOC_city (id = 33)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: test-HUM:desc Who was Galileo ?\n",
            "INFO:tensorflow:tokens: [CLS] Who was Galileo ? [SEP]\n",
            "INFO:tensorflow:input_ids: 101 2627 1108 23926 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: HUM_desc (id = 29)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: test-DESC:def What is an atom ?\n",
            "INFO:tensorflow:tokens: [CLS] What is an atom ? [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1327 1110 1126 18858 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: DESC_def (id = 3)\n",
            "INFO:tensorflow:*** Example ***\n",
            "INFO:tensorflow:guid: test-NUM:date When did Hawaii become a state ?\n",
            "INFO:tensorflow:tokens: [CLS] When did Hawaii become a state ? [SEP]\n",
            "INFO:tensorflow:input_ids: 101 1332 1225 6826 1561 170 1352 136 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:tensorflow:label: NUM_date (id = 40)\n",
            "INFO:tensorflow:***** Running prediction*****\n",
            "INFO:tensorflow:  Num examples = 500 (500 actual, 0 padding)\n",
            "INFO:tensorflow:  Batch size = 8\n",
            "INFO:tensorflow:***** Predict results *****\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Running infer on CPU\n",
            "INFO:tensorflow:*** Features ***\n",
            "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
            "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
            "INFO:tensorflow:  name = is_real_example, shape = (?,)\n",
            "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
            "INFO:tensorflow:  name = label_mask, shape = (?,)\n",
            "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
            "INFO:tensorflow:**** Trainable Variables ****\n",
            "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (28996, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
            "INFO:tensorflow:  name = Discriminator/dense/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = Discriminator/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = Discriminator/dense_1/kernel:0, shape = (768, 52)\n",
            "INFO:tensorflow:  name = Discriminator/dense_1/bias:0, shape = (52,)\n",
            "INFO:tensorflow:  name = Generator/dense/kernel:0, shape = (100, 768)\n",
            "INFO:tensorflow:  name = Generator/dense/bias:0, shape = (768,)\n",
            "INFO:tensorflow:  name = Generator/dense_1/kernel:0, shape = (768, 768)\n",
            "INFO:tensorflow:  name = Generator/dense_1/bias:0, shape = (768,)\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ganbert_output_model/model.ckpt-276\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}